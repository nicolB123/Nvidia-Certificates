{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e",
   "metadata": {
    "id": "a1c98c44-0505-43b2-957c-86aa4d0e621e"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qk4Uw_iSr3Mc",
   "metadata": {
    "id": "Qk4Uw_iSr3Mc"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 7:** Retrieval-Augmented Generation with Vector Stores</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the previous notebook, we learned about embedding models and exercised some of their capabilities. We discussed their intended use cases of longer-form document comparison and found ways to use it as a backbone for more custom semantic comparisons. This notebook will progress these ideas toward the retrieval model's intended use case and explore how to build chatbot systems that rely on *vector stores* to automatically save and retrieve information.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Understand how semantic-similarity-backed systems can facilitate easy-to-use retrieval formulations.\n",
    "\n",
    "- Learn how to incorporate retrieval modules into your chat model systems for a retrieval-augmented generation (RAG) pipeline, which can be applied to tasks like document retrieval and conversation memory buffers.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- This notebook does not attempt to incorporate hierarchical reasoning or non-naive RAG (such as planning agents). Consider what modifications would be necessary to make these components work in an LCEL chain.\n",
    "\n",
    "- Consider when it would be best to move your vector store solution into a scalable service and when a GPU will become necessary for optimization.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5XmeiiOWtuxC",
   "metadata": {
    "id": "5XmeiiOWtuxC"
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu\n",
    "\n",
    "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# ChatNVIDIA.get_available_models()\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37fe234-2bdb-4107-8483-efda9aa5e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## Part 1: Summary of RAG Workflows\n",
    "\n",
    "This notebook will explore several paradigms and derive reference code to help you approach some of the most common retrieval-augmented workflows. Specifically, the following sections will be covered (with the differences highlighted):\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***Vector Store Workflow for Conversational Exchanges:***\n",
    "- Generate semantic embedding for each new conversation.\n",
    "- Add the message body to a vector store for retrieval.\n",
    "- Query the vector store for relevant messages to fill in the LLM context.\n",
    "\n",
    "<br>\n",
    "\n",
    "> ***Modified Workflow for an Arbitrary Document:***\n",
    "- **Divide the document into chunks and process them into useful messages.**\n",
    "- Generate semantic embedding for each **new document chunk**.\n",
    "- Add the **chunk bodies** to a vector store for retrieval.\n",
    "- Query the vector store for relevant **chunks** to fill in the LLM context.\n",
    "    - ***Optional:* Modify/synthesize results for better LLM results.**\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Extended Workflow for a Directory of Arbitrary Documents:**\n",
    "- Divide **each document** into chunks and process them into useful messages.\n",
    "- Generate semantic embedding for each new document chunk.\n",
    "- Add the chunk bodies to **a scalable vector database for fast retrieval**.\n",
    "    - ***Optional*: Exploit hierarchical or metadata structures for larger systems.**\n",
    "- Query the **vector database** for relevant chunks to fill in the LLM context.\n",
    "    - *Optional:* Modify/synthesize results for better LLM results.\n",
    "\n",
    "<br>\n",
    "\n",
    "Some of the most important terminology surrounding RAG is covered in detail on the [**LlamaIndex Concepts page**](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html), which itself is a great starting point for progressing towards the LlamaIndex loading and retrieving strategy. We highly recommend using it as a reference as you continue with this notebook and advise you to try out LlamaIndex after the course to consider the pros and cons firsthand!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437",
   "metadata": {
    "id": "baa1911b-a6a2-47c5-bc66-1b61e6516437"
   },
   "source": [
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1cFbKbVvLLnFPs3yWCKIuzXkhBWh6nLQY\" width=1200px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/data_connection_langchain.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Retrieval | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/v0.1/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XaZ20XoeSTD-",
   "metadata": {
    "id": "XaZ20XoeSTD-"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** RAG for Conversation History\n",
    "\n",
    "In our previous explorations, we delved into the capabilities of document embedding models and used them to embed, store, and compare semantic vector representations of text. Though we could motivate how to efficiently extend this into vector store land manually, the true beauty of working with a standard API is its strong incorporation with other frameworks that can already do the heavy lifting for us!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LRx0XUf_Sdxw",
   "metadata": {
    "id": "LRx0XUf_Sdxw"
   },
   "source": [
    "### **Step 1**: Getting A Conversation\n",
    "\n",
    "Consider a conversation crafted using Llama-13B between a chat agent and a blue bear named Beras. This dialogue, dense with details and potential diversions, provides a rich dataset for our study:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "IUfCuMkoShWI",
   "metadata": {
    "id": "IUfCuMkoShWI"
   },
   "outputs": [],
   "source": [
    "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
    "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
    "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
    "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
    "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
    "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
    "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
    "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
    "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tDL2tAo2Skh2",
   "metadata": {
    "id": "tDL2tAo2Skh2"
   },
   "source": [
    "Using the manual embedding strategy from the previous notebook is still very viable, but we can also rest easy and let a **vector store** do all that work for us!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hIp943mSqGZ",
   "metadata": {
    "id": "5hIp943mSqGZ"
   },
   "source": [
    "### **Step 2:** Constructing Our Vector Store Retriever\n",
    "\n",
    "To streamline similarity queries on our conversation, we can employ a vector store to help keep track of passages for us! **Vector Stores**, or vector storage systems, abstract away most of the low-level details of the embedding/comparison strategies and provide a simple interface to load and compare vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pnaOBgexS-kp",
   "metadata": {
    "id": "pnaOBgexS-kp"
   },
   "source": [
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1ZjwYbSZzsXK6ZP8O1-cY3BeRffV4oqzb\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/vector_stores.jpeg\" width=1200px/>\n",
    ">\n",
    "> From [**Vector Stores | LangChain**ü¶úÔ∏èüîó](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DwZUh6kgS5Ki",
   "metadata": {
    "id": "DwZUh6kgS5Ki"
   },
   "source": [
    "<br>\n",
    "\n",
    "In addition to simplifying the process from an API perspective, vector stores also implement connectors, integrations, and optimizations under the hood. In our case, we will start with the [**FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss), which integrates a LangChain-compatable Embedding model with the [**FAISS (Facebook AI Similarity Search)**](https://github.com/facebookresearch/faiss) library to make the process fast and scalable on our local machine!\n",
    "\n",
    "**Specifically:**\n",
    "\n",
    "1. We can feed our conversation into [**a FAISS vector store**](https://python.langchain.com/docs/integrations/vectorstores/faiss) via the `from_texts` constructor. This will take our conversational data and the embedding model to create a searchable index over our discussion.\n",
    "2. This vector store can then be \"interpreted\" as a retriever, supporting the LangChain runnable API and returning documents retrieved via an input query.\n",
    "\n",
    "The following shows how you can construct a FAISS vector store and reinterpret it as a retriever using the LangChain `vectorstore` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1kE2-ejoTKKU",
   "metadata": {
    "id": "1kE2-ejoTKKU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 243 ms, sys: 365 ms, total: 608 ms\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "## Streamlined from_texts FAISS vectorstore construction from text list\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "retriever = convstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muN66v5PW5dW",
   "metadata": {
    "id": "muN66v5PW5dW"
   },
   "source": [
    "The retriever can now be used like any other LangChain runnable to query the vector store for some relevant documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "kNZJTnlEWVYh",
   "metadata": {
    "id": "kNZJTnlEWVYh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'6eeb0d13-e80b-4cd6-9bd1-6d00514d34eb'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'aef342a2-16a4-4da5-bb7c-1bb38d6098f9'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'f4286431-b896-4283-8260-43f7398bd56d'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">you![Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'fe3c3a4a-8f62-447a-8f92-f9ea5365c365'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'6eeb0d13-e80b-4cd6-9bd1-6d00514d34eb'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'aef342a2-16a4-4da5-bb7c-1bb38d6098f9'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'f4286431-b896-4283-8260-43f7398bd56d'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I hope you get to visit them someday, Beras! It would be a great adventure for \u001b[0m\n",
       "\u001b[32myou!\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Thank you for the suggestion! Ill definitely keep it in mind for the future.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'fe3c3a4a-8f62-447a-8f92-f9ea5365c365'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "SE1eDZTEWScC",
   "metadata": {
    "id": "SE1eDZTEWScC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'78cee2ff-1f6e-4dc8-bdf7-102d732ef3d4'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'fe3c3a4a-8f62-447a-8f92-f9ea5365c365'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'6eeb0d13-e80b-4cd6-9bd1-6d00514d34eb'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">id</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'aef342a2-16a4-4da5-bb7c-1bb38d6098f9'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'78cee2ff-1f6e-4dc8-bdf7-102d732ef3d4'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
       "\u001b[32macross North America'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'fe3c3a4a-8f62-447a-8f92-f9ea5365c365'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'6eeb0d13-e80b-4cd6-9bd1-6d00514d34eb'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mid\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'aef342a2-16a4-4da5-bb7c-1bb38d6098f9'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mmetadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtNCEXLYTVf4",
   "metadata": {
    "id": "mtNCEXLYTVf4"
   },
   "source": [
    "As we can see, our retriever found a handful of semantically relevant documents from our query. You may notice that not all of the documents are useful or clear on their own. For example, a retrieval of *\"Beras\"* for *\"your name\"* may be problematic for the chatbot if provided out of context. Anticipating the potential problems and creating synergies between your LLM components can increase the likelihood of good RAG behavior, so keep an eye out for such pitfalls and opportunities.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZEDEzpqmTYMv",
   "metadata": {
    "id": "ZEDEzpqmTYMv"
   },
   "source": [
    "### **Step 3:** Incorporating Conversation Retrieval Into Our Chain\n",
    "\n",
    "Now that we have our loaded retriever component as a chain, we can incorporate it into our existing chat system as before. Specifically, we can start with an ***always-on RAG formulation*** where:\n",
    "- **A retriever is always retrieving context by default**.\n",
    "- **A generator is acting on the retrieved context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64abe478-9bcb-4802-a26e-dc5a1756e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Utility Runnables/Methods\n",
    "def RPrint(preface=\"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        if preface: print(preface, end=\"\")\n",
    "        pprint(x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "uue5UY3_TcvF",
   "metadata": {
    "id": "uue5UY3_TcvF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Based on the conversation, Beras mentioned that they live in the Arctic.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m Based on the conversation, Beras mentioned that they live in the Arctic.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {question}\"\n",
    "    \"\\nAnswer the user conversationally. User is not aware of context.\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    # | RPrint()\n",
    "    | instruct_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(chain.invoke(\"Where does Beras live?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FSIqTMuuTjIh",
   "metadata": {
    "id": "FSIqTMuuTjIh"
   },
   "source": [
    "Take a second to try out some more invocations and see how the new setup performs. Regardless of your model choice, the following questions should serve as interesting starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4jDJwrYpTmpd",
   "metadata": {
    "id": "4jDJwrYpTmpd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Hello! The Rocky Mountains stretch across North America. They are a beautiful and majestic range of mountains that</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">span across this region.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m Hello! The Rocky Mountains stretch across North America. They are a beautiful and majestic range of mountains that\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspan across this region.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "-artagLfTpBy",
   "metadata": {
    "id": "-artagLfTpBy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Hello there! The Rocky Mountains are indeed a magnificent range of mountains that stretch across North America. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">They're not immediately close to California, though. They actually span from British Columbia in Canada, through </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the United States, and down to New Mexico. So, while they're not right next to California, you might be able to see</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">them if you travel far enough north or east from the state.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m Hello there! The Rocky Mountains are indeed a magnificent range of mountains that stretch across North America. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mThey're not immediately close to California, though. They actually span from British Columbia in Canada, through \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe United States, and down to New Mexico. So, while they're not right next to California, you might be able to see\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthem if you travel far enough north or east from the state.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "GDgjdfdpTrV5",
   "metadata": {
    "id": "GDgjdfdpTrV5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Although I can't provide specific details about the exact location of Beras, I can tell you that the Rocky </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mountains are a large mountain range in North America. However, without knowing Beras's precise location in the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Arctic, it's not possible for me to determine the distance between Beras and the Rocky Mountains.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m Although I can't provide specific details about the exact location of Beras, I can tell you that the Rocky \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMountains are a large mountain range in North America. However, without knowing Beras's precise location in the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mArctic, it's not possible for me to determine the distance between Beras and the Rocky Mountains.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"How far away is Beras from the Rocky Mountains?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wp9-8CbT0L9",
   "metadata": {
    "id": "8wp9-8CbT0L9"
   },
   "source": [
    "<br>\n",
    "\n",
    "You might notice some decent performance with this always-on retrieval node in the loop since the actual context being fed into the LLM remains relatively small. It's important to experiment with factors like embedding sizes, context limits, and model options to see what kinds of behavior you can expect and which efforts are worth taking to improve performance.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OnpOybOhUCTf",
   "metadata": {
    "id": "OnpOybOhUCTf"
   },
   "source": [
    "### **Step 4:** Automatic Conversation Storage\n",
    "\n",
    "Now that we see how our vector store memory unit should function, we can perform one last integration to allow our conversation to add new entries to our conversation: a runnable that calls the `add_texts` method for us to update the store state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "FsK6-AtRVdcZ",
   "metadata": {
    "id": "FsK6-AtRVdcZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> While I'm sure the Rocky Mountains are an excellent place to enjoy some ice cream, our conversation was mainly </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">about the mountains themselves! The Rocky Mountains are known for their stunning beauty and geological </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">significance. They stretch across North America, providing breathtaking views and unique ecological systems. If you</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">have any questions or would like to know more about them, I'd be happy to share!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m While I'm sure the Rocky Mountains are an excellent place to enjoy some ice cream, our conversation was mainly \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mabout the mountains themselves! The Rocky Mountains are known for their stunning beauty and geological \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msignificance. They stretch across North America, providing breathtaking views and unique ecological systems. If you\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhave any questions or would like to know more about them, I'd be happy to share!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Based on our conversation about the Rocky Mountains, it seems like you have a fondness for ice cream! Although </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">it's not directly related to the mountains, it certainly sounds like a sweet treat to enjoy while taking in the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">views. So, I'd guess that ice cream might just be one of your favorite foods!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m Based on our conversation about the Rocky Mountains, it seems like you have a fondness for ice cream! Although \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mit's not directly related to the mountains, it certainly sounds like a sweet treat to enjoy while taking in the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mviews. So, I'd guess that ice cream might just be one of your favorite foods!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Oh, I see! Well, it seems like my guess was a bit off. It's fascinating how we all have our unique preferences, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">isn't it? Your fondness for honey certainly adds a sweet touch to your personality, Beras. Next time, I'll do my </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">best to make a more accurate guess about your favorite food. The Rocky Mountains are still a fantastic topic, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">though. They offer a variety of sights and activities, and I'm sure they could pair well with your favorite </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">honey-based treats. How do you usually enjoy your honey, by the way?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m Oh, I see! Well, it seems like my guess was a bit off. It's fascinating how we all have our unique preferences, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0misn't it? Your fondness for honey certainly adds a sweet touch to your personality, Beras. Next time, I'll do my \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbest to make a more accurate guess about your favorite food. The Rocky Mountains are still a fantastic topic, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthough. They offer a variety of sights and activities, and I'm sure they could pair well with your favorite \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhoney-based treats. How do you usually enjoy your honey, by the way?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> Well, I remember our conversation quite well! At first, I did think that ice cream was your favorite food, but </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">then you kindly corrected me and revealed that honey is your top pick. So, yes, I do know your favorite food now - </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">honey! Could you tell me more about how you usually enjoy your honey? I'm curious!</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m Well, I remember our conversation quite well! At first, I did think that ice cream was your favorite food, but \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthen you kindly corrected me and revealed that honey is your top pick. So, yes, I do know your favorite food now - \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhoney! Could you tell me more about how you usually enjoy your honey? I'm curious!\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "########################################################################\n",
    "## Reset knowledge base and define what it means to add more messages.\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
    "    return d.get('output')\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question using only the context\"\n",
    "    \"\\n\\nRetrieved Context: {context}\"\n",
    "    \"\\n\\nUser Question: {input}\"\n",
    "    \"\\nAnswer the user conversationally. Make sure the conversation flows naturally.\\n\"\n",
    "    \"[Agent]\"\n",
    ")\n",
    "\n",
    "\n",
    "conv_chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'input': (lambda x:x)\n",
    "    }\n",
    "    | RunnableAssign({'output' : chat_prompt | instruct_llm | StrOutputParser()})\n",
    "    | partial(save_memory_and_get_output, vstore=convstore)\n",
    ")\n",
    "\n",
    "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"Actually, my favorite is honey! Not sure where you got that idea?\"))\n",
    "print()\n",
    "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KRMW6G7NVSWF",
   "metadata": {
    "id": "KRMW6G7NVSWF"
   },
   "source": [
    "Unlike the more automatic full-text or rule-based approaches to injecting context into the LLM, this approach ensures some amount of consolidation which can keep the context length from getting out of hand. It's still not a full-proof strategy on its own, but it's a stark improvement for unstructured conversations (and doesn't even require a strong instruction-tuned model to perform slot-filling)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9TPkh3SaLbqh",
   "metadata": {
    "id": "9TPkh3SaLbqh"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3 [Exercise]:** RAG For Document Chunk Retrieval\n",
    "\n",
    "Given our prior exploration of document loading, the idea that data chunks can be embedded and searched through probably isn't surprising. With that said, it is definitely worth going over since applying RAG with documents is a double-edged sword; it may **seem** to work well out of the box but requires some extra care when optimizing it for truly reliable performance. It also provides an excellent opportunity to review some fundamental LCEL skills, so let's see what we can do!\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Exercise:**\n",
    "\n",
    "In the previous example, you may recall that we pulled in some relatively small papers with the help of [`ArxivLoader`](https://python.langchain.com/docs/integrations/document_loaders/arxiv) using the following syntax:\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "\n",
    "docs = [\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct\n",
    "]\n",
    "```\n",
    "\n",
    "Given all that you've learned so far, choose a selection of papers that you would like to use and develop a chatbot that can talk about them!\n",
    "\n",
    "<br>\n",
    "\n",
    "Though this is a pretty big task, a walkthrough of ***most*** of the process will be provided below. By the end of the walkthrough, many of the necessary puzzle pieces will be provided, and your real task will be to integrate them together for the final `retrieval_chain`. When you're done, get ready to re-integrate the chain (or a flavor of your choice) in the last notebook as part of the evaluation exercise!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSjfCtiQnj9e",
   "metadata": {
    "id": "jSjfCtiQnj9e"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 1**: Loading And Chunking Your Documents\n",
    "\n",
    "The following code block gives you some default papers to load in for your RAG chain. Feel free to select more papers as desired, but note that longer documents will take longer to process. A few simplifying assumptions and additional processing steps are included to help you improve your naive RAG performance:\n",
    "\n",
    "- Documents are cut off prior to the \"References\" section if one exists. This will keep our system from considering the citations and appendix sections, which tend to be long and distracting.\n",
    "\n",
    "- A chunk that lists the available documents is inserted to provide a high-level view of all available documents in a single chunk. If your pipeline does not provide metadata on each retrieval, this is a useful component and can even be listed among a list of higher-priority pieces if appropriate.\n",
    "\n",
    "- Additionally, the metadata entries are also inserted to provide general information. Ideally, there would also be some synthetic chunks that merge the metadata into interesting cross-document chunks.\n",
    "\n",
    "**NOTE:** ***For the sake of the assessment, please include at least one paper that is less than one month old!***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "S-3FBdT_lhVT",
   "metadata": {
    "id": "S-3FBdT_lhVT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Documents\n",
      "Chunking Documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - FFPDG: Fast, Fair and Private Data Generation</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Toward Co-creative Dungeon Generation via Transfer Learning</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - TaikoNation: Patterning-focused Chart Generation for Rhythm Action Games </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Mistral 7B\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - FFPDG: Fast, Fair and Private Data Generation\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Toward Co-creative Dungeon Generation via Transfer Learning\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - TaikoNation: Patterning-focused Chart Generation for Rhythm Action Games \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      " - # Chunks: 2\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Entry ID'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'http://arxiv.org/abs/1706.03762v7'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.date</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Kaiser, Illia Polosukhin'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Entry ID'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'http://arxiv.org/abs/1706.03762v7'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[1;35mdatetime.date\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Attention Is All You Need'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz \u001b[0m\n",
       "\u001b[32mKaiser, Illia Polosukhin'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1\n",
      " - # Chunks: 2\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Entry ID'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'http://arxiv.org/abs/1810.04805v2'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.date</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2019</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Entry ID'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'http://arxiv.org/abs/1810.04805v2'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[1;35mdatetime.date\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2019\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 2\n",
      " - # Chunks: 2\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Entry ID'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'http://arxiv.org/abs/2005.11401v4'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.date</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2021</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Entry ID'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'http://arxiv.org/abs/2005.11401v4'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[1;35mdatetime.date\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2021\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001b[0m\n",
       "\u001b[32mHeinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 3\n",
      " - # Chunks: 1\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Entry ID'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'http://arxiv.org/abs/2310.06825v1'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.date</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Entry ID'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'http://arxiv.org/abs/2310.06825v1'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[1;35mdatetime.date\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Mistral 7B'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
       "\u001b[32mDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, \u001b[0m\n",
       "\u001b[32mMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 4\n",
      " - # Chunks: 2\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Entry ID'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'http://arxiv.org/abs/2306.05685v4'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.date</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Entry ID'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'http://arxiv.org/abs/2306.05685v4'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[1;35mdatetime.date\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, \u001b[0m\n",
       "\u001b[32mZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 5\n",
      " - # Chunks: 2\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Entry ID'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'http://arxiv.org/abs/2504.20734v3'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.date</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2026</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, Sung Ju Hwang'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Entry ID'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'http://arxiv.org/abs/2504.20734v3'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[1;35mdatetime.date\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2026\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, Sung Ju Hwang'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 6\n",
      " - # Chunks: 43\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-06-30'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'FFPDG: Fast, Fair and Private Data Generation'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Weijie Xu, Jinjin Zhao, Francis Iannacci, Bo Wang'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Generative modeling has been used frequently in synthetic data generation. Fairness and privacy are</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">two big concerns for synthetic data. Although Recent GAN [\\\\cite{goodfellow2014generative}] based methods show good</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results in preserving privacy, the generated data may be more biased. At the same time, these methods require high </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">computation resources. In this work, we design a fast, fair, flexible and private data generation method. We show </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the effectiveness of our method theoretically and empirically. We show that models trained on data generated by the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">proposed method can perform well (in inference stage) on real application scenarios.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-06-30'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'FFPDG: Fast, Fair and Private Data Generation'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Weijie Xu, Jinjin Zhao, Francis Iannacci, Bo Wang'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Generative modeling has been used frequently in synthetic data generation. Fairness and privacy are\u001b[0m\n",
       "\u001b[32mtwo big concerns for synthetic data. Although Recent GAN \u001b[0m\u001b[32m[\u001b[0m\u001b[32m\\\\cite\u001b[0m\u001b[32m{\u001b[0m\u001b[32mgoodfellow2014generative\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m based methods show good\u001b[0m\n",
       "\u001b[32mresults in preserving privacy, the generated data may be more biased. At the same time, these methods require high \u001b[0m\n",
       "\u001b[32mcomputation resources. In this work, we design a fast, fair, flexible and private data generation method. We show \u001b[0m\n",
       "\u001b[32mthe effectiveness of our method theoretically and empirically. We show that models trained on data generated by the\u001b[0m\n",
       "\u001b[32mproposed method can perform well \u001b[0m\u001b[32m(\u001b[0m\u001b[32min inference stage\u001b[0m\u001b[32m)\u001b[0m\u001b[32m on real application scenarios.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 7\n",
      " - # Chunks: 58\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-07-27'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Toward Co-creative Dungeon Generation via Transfer Learning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Zisen Zhou, Matthew Guzdial'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Co-creative Procedural Content Generation via Machine Learning (PCGML) refers to systems where a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">PCGML agent and a human work together to produce output content. One of the limitations of co-creative PCGML is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">that it requires co-creative training data for a PCGML agent to learn to interact with humans. However, acquiring </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">this data is a difficult and time-consuming process. In this work, we propose approximating human-AI interaction </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">data and employing transfer learning to adapt learned co-creative knowledge from one game to a different game. We </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">explore this approach for co-creative Zelda dungeon room generation.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-07-27'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Toward Co-creative Dungeon Generation via Transfer Learning'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Zisen Zhou, Matthew Guzdial'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Co-creative Procedural Content Generation via Machine Learning \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPCGML\u001b[0m\u001b[32m)\u001b[0m\u001b[32m refers to systems where a \u001b[0m\n",
       "\u001b[32mPCGML agent and a human work together to produce output content. One of the limitations of co-creative PCGML is \u001b[0m\n",
       "\u001b[32mthat it requires co-creative training data for a PCGML agent to learn to interact with humans. However, acquiring \u001b[0m\n",
       "\u001b[32mthis data is a difficult and time-consuming process. In this work, we propose approximating human-AI interaction \u001b[0m\n",
       "\u001b[32mdata and employing transfer learning to adapt learned co-creative knowledge from one game to a different game. We \u001b[0m\n",
       "\u001b[32mexplore this approach for co-creative Zelda dungeon room generation.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 8\n",
      " - # Chunks: 59\n",
      " - Metadata: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-07-26'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'TaikoNation: Patterning-focused Chart Generation for Rhythm Action Games'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Emily Halina, Matthew Guzdial'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Generating rhythm game charts from songs via machine learning has been a problem of increasing </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">interest in recent years. However, all existing systems struggle to replicate human-like patterning: the placement </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of game objects in relation to each other to form congruent patterns based on events in the song. Patterning is a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">key identifier of high quality rhythm game content, seen as a necessary component in human rankings. We establish a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">new approach for chart generation that produces charts with more congruent, human-like patterning than seen in </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">prior work.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-07-26'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'TaikoNation: Patterning-focused Chart Generation for Rhythm Action Games'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Emily Halina, Matthew Guzdial'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Generating rhythm game charts from songs via machine learning has been a problem of increasing \u001b[0m\n",
       "\u001b[32minterest in recent years. However, all existing systems struggle to replicate human-like patterning: the placement \u001b[0m\n",
       "\u001b[32mof game objects in relation to each other to form congruent patterns based on events in the song. Patterning is a \u001b[0m\n",
       "\u001b[32mkey identifier of high quality rhythm game content, seen as a necessary component in human rankings. We establish a\u001b[0m\n",
       "\u001b[32mnew approach for chart generation that produces charts with more congruent, human-like patterning than seen in \u001b[0m\n",
       "\u001b[32mprior work.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "from langchain.retrievers import ArxivRetriever\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
    ")    \n",
    "\n",
    "## TODO: Please pick some papers and add them to the list as you'd like\n",
    "## NOTE: To re-use for the final assessment, make sure at least one paper is < 1 month old\n",
    "print(\"Loading Documents\")\n",
    "docs = [doc[0] for doc in ArxivRetriever().batch([\n",
    "    \"1706.03762\",     ## Attention Is All You Need Paper\n",
    "    \"1810.04805\",     ## BERT Paper\n",
    "    \"2005.11401\",     ## RAG Paper\n",
    "    \"2310.06825\",     ## Mistral Paper\n",
    "    \"2306.05685\",     ## LLM-as-a-Judge\n",
    "    \"2504.20734\"      ## Universal RAG\n",
    "    ## Some longer papers\n",
    "    # \"2210.03629\",   ## ReAct Paper\n",
    "    # \"2112.10752\",   ## Latent Stable Diffusion Paper\n",
    "    # \"2103.00020\",   ## CLIP Paper\n",
    "    ## TODO: Feel free to add more\n",
    "])] + ArxivLoader(query=\"Retrieval-Augmented Generation\", load_max_docs=2).load()\n",
    "\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = json.dumps(doc.page_content)\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"References\")]\n",
    "\n",
    "## Split the documents and also filter out stubs (overly short chunks)\n",
    "print(\"Chunking Documents\")\n",
    "docs_chunks = [text_splitter.split_documents([doc]) for doc in docs]\n",
    "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
    "\n",
    "## Make some custom Chunks to give big-picture details\n",
    "doc_string = \"Available Documents:\"\n",
    "doc_metadata = []\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "\n",
    "## Printing out some summary information for reference\n",
    "pprint(doc_string, '\\n')\n",
    "for i, chunks in enumerate(docs_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print(f\" - Metadata: \")\n",
    "    pprint(chunks[0].metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4pWU_OOnnrsT",
   "metadata": {
    "id": "4pWU_OOnnrsT"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2**: Construct Your Document Vector Stores\n",
    "\n",
    "Now that we have all of the components, we can go ahead and create indices surrounding them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lwwmr3aptwCg",
   "metadata": {
    "id": "lwwmr3aptwCg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Vector Stores\n",
      "CPU times: user 423 ms, sys: 18.5 ms, total: 441 ms\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Constructing Vector Stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j39JwCKubto0",
   "metadata": {
    "id": "j39JwCKubto0"
   },
   "source": [
    "<br>\n",
    "\n",
    "From there, we can combine our indices into a single one using the following utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Q7us66iPVc70",
   "metadata": {
    "id": "Q7us66iPVc70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 181 chunks\n"
     ]
    }
   ],
   "source": [
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VU_VEx2mqJUK",
   "metadata": {
    "id": "VU_VEx2mqJUK"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 3: [Exercise]** Implement Your RAG Chain\n",
    "\n",
    "Finally, all the puzzle pieces are in place to implement the RAG pipeline! As a review, we now have:\n",
    "\n",
    "- A way to construct a from-scratch vector store for conversational memory (and a way to initialize an empty one with `default_FAISS()`)\n",
    "\n",
    "- A vector store pre-loaded with useful document information from our `ArxivLoader` utility (stored in `docstore`).\n",
    "\n",
    "With the help of a couple more utilities, you're finally ready to integrate your chain! A few additional convenience utilities are provided (`doc2str` and the now-common `RPrint`) but are optional to use. Additionally, some starter prompts and structures are also defined.\n",
    "\n",
    "> **Given all of this:** Please implement the `retrieval_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "-RXSrb1GcNff",
   "metadata": {
    "id": "-RXSrb1GcNff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conversation History Retrieval:\\n\\n\\n Document Retrieval:\\n[Quote from Retrieval-Augmented Generation for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge-Intensive NLP Tasks] . We introduce RAG models where the parametric memory is a pre-trained seq2seq model</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baseline.\\n[Quote from UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Granularities] . Specifically, motivated by the observation that forcing all modalities into a unified </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">representation space derived from a single aggregated corpus causes a modality gap, where the retrieval tends to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">favor items from the same modality as the query, we propose modality-aware routing, which dynamically identifies </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the most appropriate modality-specific corpus and performs targeted retrieval within it, and further justify its </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">effectiveness with a theoretical analysis. Moreover, beyond modality, we organize each modality into multiple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">UniversalRAG on 10 benchmarks of multiple modalities, showing its superiority over various modality-specific and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">unified baselines.\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] Large pre-trained</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parametric and non-parametric memory for language generation\\n[Quote from UniversalRAG: Retrieval-Augmented </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Generation over Corpora of Diverse Modalities and Granularities] Retrieval-Augmented Generation (RAG) has shown </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">queries. However, most existing approaches are limited to a text-only corpus, and while recent efforts have </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge source cannot address. To address this, we introduce UniversalRAG, designed to retrieve and integrate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge from heterogeneous sources with diverse modalities and granularities\\n\\n\\n (Answer only from retrieval. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Only cite sources that are used. Make your response conversational.)'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={})</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
       "\u001b[32mjust asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  \u001b[0m\n",
       "\u001b[32mConversation History Retrieval:\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for \u001b[0m\n",
       "\u001b[32mKnowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . We introduce RAG models where the parametric memory is a pre-trained seq2seq model\u001b[0m\n",
       "\u001b[32mand the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. \u001b[0m\n",
       "\u001b[32mWe compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated \u001b[0m\n",
       "\u001b[32msequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of \u001b[0m\n",
       "\u001b[32mknowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric \u001b[0m\n",
       "\u001b[32mseq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that \u001b[0m\n",
       "\u001b[32mRAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq \u001b[0m\n",
       "\u001b[32mbaseline.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and \u001b[0m\n",
       "\u001b[32mGranularities\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Specifically, motivated by the observation that forcing all modalities into a unified \u001b[0m\n",
       "\u001b[32mrepresentation space derived from a single aggregated corpus causes a modality gap, where the retrieval tends to \u001b[0m\n",
       "\u001b[32mfavor items from the same modality as the query, we propose modality-aware routing, which dynamically identifies \u001b[0m\n",
       "\u001b[32mthe most appropriate modality-specific corpus and performs targeted retrieval within it, and further justify its \u001b[0m\n",
       "\u001b[32meffectiveness with a theoretical analysis. Moreover, beyond modality, we organize each modality into multiple \u001b[0m\n",
       "\u001b[32mgranularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate \u001b[0m\n",
       "\u001b[32mUniversalRAG on 10 benchmarks of multiple modalities, showing its superiority over various modality-specific and \u001b[0m\n",
       "\u001b[32munified baselines.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Large pre-trained\u001b[0m\n",
       "\u001b[32mlanguage models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art \u001b[0m\n",
       "\u001b[32mresults when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate \u001b[0m\n",
       "\u001b[32mknowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific \u001b[0m\n",
       "\u001b[32marchitectures. Additionally, providing provenance for their decisions and updating their world knowledge remain \u001b[0m\n",
       "\u001b[32mopen research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory\u001b[0m\n",
       "\u001b[32mcan overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a \u001b[0m\n",
       "\u001b[32mgeneral-purpose fine-tuning recipe for retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -- models which combine pre-trained \u001b[0m\n",
       "\u001b[32mparametric and non-parametric memory for language generation\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from UniversalRAG: Retrieval-Augmented \u001b[0m\n",
       "\u001b[32mGeneration over Corpora of Diverse Modalities and Granularities\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Retrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m has shown \u001b[0m\n",
       "\u001b[32msubstantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to \u001b[0m\n",
       "\u001b[32mqueries. However, most existing approaches are limited to a text-only corpus, and while recent efforts have \u001b[0m\n",
       "\u001b[32mextended RAG to other modalities such as images and videos, they typically operate over a single modality-specific \u001b[0m\n",
       "\u001b[32mcorpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of \u001b[0m\n",
       "\u001b[32mknowledge source cannot address. To address this, we introduce UniversalRAG, designed to retrieve and integrate \u001b[0m\n",
       "\u001b[32mknowledge from heterogeneous sources with diverse modalities and granularities\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from retrieval. \u001b[0m\n",
       "\u001b[32mOnly cite sources that are used. Make your response conversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Tell me about RAG!'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RAG, or Retrieval-Augmented Generation, is a concept in the field of natural language processing (NLP) that involves using both parametric and non-parametric memory to improve the performance of language generation models on knowledge-intensive tasks.\n",
      "\n",
      "Parametric memory refers to the knowledge that a model learns during training and stores in its parameters. Non-parametric memory, on the other hand, is external knowledge that a model can retrieve as needed. In the case of RAG, this non-parametric memory is accessed through a dense vector index of a large corpus, such as Wikipedia, using a pre-trained neural retriever.\n",
      "\n",
      "There are different formulations of RAG, such as one where the same retrieved passages are used across the entire generated sequence, and another where different passages can be used for each token. RAG models have been shown to outperform parametric-only seq2seq models and task-specific retrieve-and-extract architectures on a range of knowledge-intensive NLP tasks.\n",
      "\n",
      "In addition to the original RAG model, there is also a variant called UniversalRAG, which is designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. This is in contrast to most existing approaches, which are limited to a text-only corpus. UniversalRAG includes a modality-aware routing mechanism, which dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. It has been shown to outperform various modality-specific and unified baselines on 10 benchmarks of multiple modalities.\n",
      "\n",
      "Overall, RAG and UniversalRAG offer a promising approach to improving the factual accuracy and performance of language generation models on knowledge-intensive tasks by allowing them to access and precisely manipulate external knowledge as needed."
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "# NVIDIAEmbeddings.get_available_models()\n",
    "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "# ChatNVIDIA.get_available_models()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
    "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
    "\n",
    "################################################################################################\n",
    "## BEGIN TODO: Implement the retrieval chain to make your system work!\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"input\": (lambda x: x)}\n",
    "    ## Retrieve both:\n",
    "    ##  - history from convstore (conversation memory)\n",
    "    ##  - context from docstore (paper chunks)\n",
    "    | RunnableAssign({\n",
    "        \"history\": (\n",
    "            itemgetter(\"input\")\n",
    "            | convstore.as_retriever()\n",
    "            | long_reorder\n",
    "            | partial(docs2str, title=\"Conversation\")\n",
    "        )\n",
    "    })\n",
    "    | RunnableAssign({\n",
    "        \"context\": (\n",
    "            itemgetter(\"input\")\n",
    "            | docstore.as_retriever()\n",
    "            | long_reorder\n",
    "            | partial(docs2str, title=\"Document\")\n",
    "        )\n",
    "    })\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "################################################################################################\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9W7sC5Z6BfqM",
   "metadata": {
    "id": "9W7sC5Z6BfqM"
   },
   "source": [
    "### **Task 4:** Interact With Your Gradio Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fMP3l7QL2JWT",
   "metadata": {
    "id": "fMP3l7QL2JWT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://094eaf58be01da97d1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://094eaf58be01da97d1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: What Modalities does Universal RAG make use of?\\n\\n From this, we have retrieved the following </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">potentially-useful info:  Conversation History Retrieval:\\n[Quote from Conversation] User previously responded with</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Tell me about RAG!\\n[Quote from Conversation] Agent previously responded with  RAG, or Retrieval-Augmented </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Generation, is a concept in the field of natural language processing (NLP) that involves using both parametric and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">non-parametric memory to improve the performance of language generation models on knowledge-intensive </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">tasks.\\n\\nParametric memory refers to the knowledge that a model learns during training and stores in its </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parameters. Non-parametric memory, on the other hand, is external knowledge that a model can retrieve as needed. In</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the case of RAG, this non-parametric memory is accessed through a dense vector index of a large corpus, such as </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Wikipedia, using a pre-trained neural retriever.\\n\\nThere are different formulations of RAG, such as one where the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">same retrieved passages are used across the entire generated sequence, and another where different passages can be </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">used for each token. RAG models have been shown to outperform parametric-only seq2seq models and task-specific </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve-and-extract architectures on a range of knowledge-intensive NLP tasks.\\n\\nIn addition to the original RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">model, there is also a variant called UniversalRAG, which is designed to retrieve and integrate knowledge from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">heterogeneous sources with diverse modalities and granularities. This is in contrast to most existing approaches, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">which are limited to a text-only corpus. UniversalRAG includes a modality-aware routing mechanism, which </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. It </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">has been shown to outperform various modality-specific and unified baselines on 10 benchmarks of multiple </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modalities.\\n\\nOverall, RAG and UniversalRAG offer a promising approach to improving the factual accuracy and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performance of language generation models on knowledge-intensive tasks by allowing them to access and precisely </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">manipulate external knowledge as needed.\\n\\n\\n Document Retrieval:\\n[Quote from UniversalRAG: Retrieval-Augmented </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Generation over Corpora of Diverse Modalities and Granularities] . Specifically, motivated by the observation that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">forcing all modalities into a unified representation space derived from a single aggregated corpus causes a </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modality gap, where the retrieval tends to favor items from the same modality as the query, we propose </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modality-aware routing, which dynamically identifies the most appropriate modality-specific corpus and performs </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">targeted retrieval within it, and further justify its effectiveness with a theoretical analysis. Moreover, beyond </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">complexity and scope of the query. We validate UniversalRAG on 10 benchmarks of multiple modalities, showing its </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">superiority over various modality-specific and unified baselines.\\n[Quote from Retrieval-Augmented Generation for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge-Intensive NLP Tasks] . We introduce RAG models where the parametric memory is a pre-trained seq2seq model</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">baseline.\\n[Quote from Document] {'Entry ID': 'http://arxiv.org/abs/2504.20734v3', 'Published': datetime.date(2026,</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">1, 6), 'Title': 'UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Granularities', 'Authors': 'Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, Sung Ju Hwang'}\\n[Quote from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">responses with external knowledge relevant to queries. However, most existing approaches are limited to a text-only</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">UniversalRAG, designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">granularities\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. Make your response </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">conversational.)\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'What Modalities does Universal RAG make use of?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"You\u001b[0m\u001b[32m are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
       "\u001b[32mjust asked: What Modalities does Universal RAG make use of?\\n\\n From this, we have retrieved the following \u001b[0m\n",
       "\u001b[32mpotentially-useful info:  Conversation History Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Conversation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m User previously responded with\u001b[0m\n",
       "\u001b[32mTell me about RAG!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Conversation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Agent previously responded with  RAG, or Retrieval-Augmented \u001b[0m\n",
       "\u001b[32mGeneration, is a concept in the field of natural language processing \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNLP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that involves using both parametric and \u001b[0m\n",
       "\u001b[32mnon-parametric memory to improve the performance of language generation models on knowledge-intensive \u001b[0m\n",
       "\u001b[32mtasks.\\n\\nParametric memory refers to the knowledge that a model learns during training and stores in its \u001b[0m\n",
       "\u001b[32mparameters. Non-parametric memory, on the other hand, is external knowledge that a model can retrieve as needed. In\u001b[0m\n",
       "\u001b[32mthe case of RAG, this non-parametric memory is accessed through a dense vector index of a large corpus, such as \u001b[0m\n",
       "\u001b[32mWikipedia, using a pre-trained neural retriever.\\n\\nThere are different formulations of RAG, such as one where the \u001b[0m\n",
       "\u001b[32msame retrieved passages are used across the entire generated sequence, and another where different passages can be \u001b[0m\n",
       "\u001b[32mused for each token. RAG models have been shown to outperform parametric-only seq2seq models and task-specific \u001b[0m\n",
       "\u001b[32mretrieve-and-extract architectures on a range of knowledge-intensive NLP tasks.\\n\\nIn addition to the original RAG \u001b[0m\n",
       "\u001b[32mmodel, there is also a variant called UniversalRAG, which is designed to retrieve and integrate knowledge from \u001b[0m\n",
       "\u001b[32mheterogeneous sources with diverse modalities and granularities. This is in contrast to most existing approaches, \u001b[0m\n",
       "\u001b[32mwhich are limited to a text-only corpus. UniversalRAG includes a modality-aware routing mechanism, which \u001b[0m\n",
       "\u001b[32mdynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. It \u001b[0m\n",
       "\u001b[32mhas been shown to outperform various modality-specific and unified baselines on 10 benchmarks of multiple \u001b[0m\n",
       "\u001b[32mmodalities.\\n\\nOverall, RAG and UniversalRAG offer a promising approach to improving the factual accuracy and \u001b[0m\n",
       "\u001b[32mperformance of language generation models on knowledge-intensive tasks by allowing them to access and precisely \u001b[0m\n",
       "\u001b[32mmanipulate external knowledge as needed.\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from UniversalRAG: Retrieval-Augmented \u001b[0m\n",
       "\u001b[32mGeneration over Corpora of Diverse Modalities and Granularities\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Specifically, motivated by the observation that \u001b[0m\n",
       "\u001b[32mforcing all modalities into a unified representation space derived from a single aggregated corpus causes a \u001b[0m\n",
       "\u001b[32mmodality gap, where the retrieval tends to favor items from the same modality as the query, we propose \u001b[0m\n",
       "\u001b[32mmodality-aware routing, which dynamically identifies the most appropriate modality-specific corpus and performs \u001b[0m\n",
       "\u001b[32mtargeted retrieval within it, and further justify its effectiveness with a theoretical analysis. Moreover, beyond \u001b[0m\n",
       "\u001b[32mmodality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the\u001b[0m\n",
       "\u001b[32mcomplexity and scope of the query. We validate UniversalRAG on 10 benchmarks of multiple modalities, showing its \u001b[0m\n",
       "\u001b[32msuperiority over various modality-specific and unified baselines.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for \u001b[0m\n",
       "\u001b[32mKnowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . We introduce RAG models where the parametric memory is a pre-trained seq2seq model\u001b[0m\n",
       "\u001b[32mand the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. \u001b[0m\n",
       "\u001b[32mWe compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated \u001b[0m\n",
       "\u001b[32msequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of \u001b[0m\n",
       "\u001b[32mknowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric \u001b[0m\n",
       "\u001b[32mseq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that \u001b[0m\n",
       "\u001b[32mRAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq \u001b[0m\n",
       "\u001b[32mbaseline.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32m'Entry ID': 'http://arxiv.org/abs/2504.20734v3', 'Published': datetime.date\u001b[0m\u001b[32m(\u001b[0m\u001b[32m2026,\u001b[0m\n",
       "\u001b[32m1, 6\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, 'Title': 'UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and \u001b[0m\n",
       "\u001b[32mGranularities', 'Authors': 'Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, Sung Ju Hwang'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from \u001b[0m\n",
       "\u001b[32mUniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mRetrieval-Augmented Generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m has shown substantial promise in improving factual accuracy by grounding model\u001b[0m\n",
       "\u001b[32mresponses with external knowledge relevant to queries. However, most existing approaches are limited to a text-only\u001b[0m\n",
       "\u001b[32mcorpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically \u001b[0m\n",
       "\u001b[32moperate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of \u001b[0m\n",
       "\u001b[32mknowledge they require, which a single type of knowledge source cannot address. To address this, we introduce \u001b[0m\n",
       "\u001b[32mUniversalRAG, designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and \u001b[0m\n",
       "\u001b[32mgranularities\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from retrieval. Only cite sources that are used. Make your response \u001b[0m\n",
       "\u001b[32mconversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'What Modalities does Universal RAG make use of?'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Where do Ice Bears live?\\n\\n From this, we have retrieved the following potentially-useful info:  </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Conversation History Retrieval:\\n[Quote from Conversation] Agent previously responded with  Universal RAG is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">This means that it can work with various types of data, not just text. According to the document, Universal RAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">uses modality-aware routing, which dynamically identifies the most appropriate modality-specific corpus and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">performs targeted retrieval within it.\\n\\nSources:\\n\\n* UniversalRAG: Retrieval-Augmented Generation over Corpora </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of Diverse Modalities and Granularities (arxiv.org/abs/2504.20734v3)\\n* Retrieval-Augmented Generation for </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge-Intensive NLP Tasks (arxiv.org/abs/2005.10439)\\n[Quote from Conversation] User previously responded with </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">What Modalities does Universal RAG make use of?\\n[Quote from Conversation] User previously responded with Tell me </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">about RAG!\\n[Quote from Conversation] Agent previously responded with  RAG, or Retrieval-Augmented Generation, is a</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">concept in the field of natural language processing (NLP) that involves using both parametric and non-parametric </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory to improve the performance of language generation models on knowledge-intensive tasks.\\n\\nParametric memory </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">refers to the knowledge that a model learns during training and stores in its parameters. Non-parametric memory, on</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">the other hand, is external knowledge that a model can retrieve as needed. In the case of RAG, this non-parametric </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">memory is accessed through a dense vector index of a large corpus, such as Wikipedia, using a pre-trained neural </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">retriever.\\n\\nThere are different formulations of RAG, such as one where the same retrieved passages are used </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across the entire generated sequence, and another where different passages can be used for each token. RAG models </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">have been shown to outperform parametric-only seq2seq models and task-specific retrieve-and-extract architectures </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">on a range of knowledge-intensive NLP tasks.\\n\\nIn addition to the original RAG model, there is also a variant </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">called UniversalRAG, which is designed to retrieve and integrate knowledge from heterogeneous sources with diverse </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modalities and granularities. This is in contrast to most existing approaches, which are limited to a text-only </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">corpus. UniversalRAG includes a modality-aware routing mechanism, which dynamically identifies the most appropriate</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modality-specific corpus and performs targeted retrieval within it. It has been shown to outperform various </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">modality-specific and unified baselines on 10 benchmarks of multiple modalities.\\n\\nOverall, RAG and UniversalRAG </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">offer a promising approach to improving the factual accuracy and performance of language generation models on </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive tasks by allowing them to access and precisely manipulate external knowledge as needed.\\n\\n\\n </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Document Retrieval:\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] . Hence, LLM-as-a-judge is</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\\n[Quote from Attention Is </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">All You Need] .5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">parsing both with large and limited training data.\\n[Quote from FFPDG: Fast, Fair and Private Data Generation] </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Karthikeyan Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Monin-\\nder Singh, Kush R. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Varshney, and Yunfeng Zhang. Ai fairness 360: An extensible toolkit for\\ndetecting, understanding, and mitigating </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">unwanted algorithmic bias, 2018.\\nTolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Man is to\\ncomputer programmer as woman is to homemaker? debiasing word embeddings, 2016.\\nL Breiman. Random </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">forests. Machine Learning 45, 2001.\\nFlavio P. Calmon, Dennis Wei, Karthikeyan Natesan Ramamurthy, and Kush R. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Varshney. Opti-\\nmized data pre-processing for discrimination prevention, 2017.\\nL. Elisa Celis, Vijay Keswani, and</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Nisheeth Vishnoi. Data preprocessing to mitigate bias: A maxi-\\nmum entropy based approach. In Hal Daum¬¥e III and </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Aarti Singh (eds.), Proceedings of the 37th\\nInternational Conference on Machine Learning, volume 119 of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Proceedings of Machine Learn-\\ning Research, pp. 1349‚Äì1359. PMLR, 13‚Äì18 Jul 2020a. URL </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">http://proceedings.mlr.\\n[Quote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena] Evaluating large </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform.</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">well, achieving over 80% agreement, the same level of agreement between humans\\n\\n\\n (Answer only from retrieval. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Only cite sources that are used. Make your response conversational.)'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Where do Ice Bears live?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={})</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
       "\u001b[32mjust asked: Where do Ice Bears live?\\n\\n From this, we have retrieved the following potentially-useful info:  \u001b[0m\n",
       "\u001b[32mConversation History Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Conversation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Agent previously responded with  Universal RAG is \u001b[0m\n",
       "\u001b[32mdesigned to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. \u001b[0m\n",
       "\u001b[32mThis means that it can work with various types of data, not just text. According to the document, Universal RAG \u001b[0m\n",
       "\u001b[32muses modality-aware routing, which dynamically identifies the most appropriate modality-specific corpus and \u001b[0m\n",
       "\u001b[32mperforms targeted retrieval within it.\\n\\nSources:\\n\\n* UniversalRAG: Retrieval-Augmented Generation over Corpora \u001b[0m\n",
       "\u001b[32mof Diverse Modalities and Granularities \u001b[0m\u001b[32m(\u001b[0m\u001b[32marxiv.org/abs/2504.20734v3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n* Retrieval-Augmented Generation for \u001b[0m\n",
       "\u001b[32mKnowledge-Intensive NLP Tasks \u001b[0m\u001b[32m(\u001b[0m\u001b[32marxiv.org/abs/2005.10439\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Conversation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m User previously responded with \u001b[0m\n",
       "\u001b[32mWhat Modalities does Universal RAG make use of?\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Conversation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m User previously responded with Tell me \u001b[0m\n",
       "\u001b[32mabout RAG!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Conversation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Agent previously responded with  RAG, or Retrieval-Augmented Generation, is a\u001b[0m\n",
       "\u001b[32mconcept in the field of natural language processing \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNLP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that involves using both parametric and non-parametric \u001b[0m\n",
       "\u001b[32mmemory to improve the performance of language generation models on knowledge-intensive tasks.\\n\\nParametric memory \u001b[0m\n",
       "\u001b[32mrefers to the knowledge that a model learns during training and stores in its parameters. Non-parametric memory, on\u001b[0m\n",
       "\u001b[32mthe other hand, is external knowledge that a model can retrieve as needed. In the case of RAG, this non-parametric \u001b[0m\n",
       "\u001b[32mmemory is accessed through a dense vector index of a large corpus, such as Wikipedia, using a pre-trained neural \u001b[0m\n",
       "\u001b[32mretriever.\\n\\nThere are different formulations of RAG, such as one where the same retrieved passages are used \u001b[0m\n",
       "\u001b[32macross the entire generated sequence, and another where different passages can be used for each token. RAG models \u001b[0m\n",
       "\u001b[32mhave been shown to outperform parametric-only seq2seq models and task-specific retrieve-and-extract architectures \u001b[0m\n",
       "\u001b[32mon a range of knowledge-intensive NLP tasks.\\n\\nIn addition to the original RAG model, there is also a variant \u001b[0m\n",
       "\u001b[32mcalled UniversalRAG, which is designed to retrieve and integrate knowledge from heterogeneous sources with diverse \u001b[0m\n",
       "\u001b[32mmodalities and granularities. This is in contrast to most existing approaches, which are limited to a text-only \u001b[0m\n",
       "\u001b[32mcorpus. UniversalRAG includes a modality-aware routing mechanism, which dynamically identifies the most appropriate\u001b[0m\n",
       "\u001b[32mmodality-specific corpus and performs targeted retrieval within it. It has been shown to outperform various \u001b[0m\n",
       "\u001b[32mmodality-specific and unified baselines on 10 benchmarks of multiple modalities.\\n\\nOverall, RAG and UniversalRAG \u001b[0m\n",
       "\u001b[32moffer a promising approach to improving the factual accuracy and performance of language generation models on \u001b[0m\n",
       "\u001b[32mknowledge-intensive tasks by allowing them to access and precisely manipulate external knowledge as needed.\\n\\n\\n \u001b[0m\n",
       "\u001b[32mDocument Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Hence, LLM-as-a-judge is\u001b[0m\n",
       "\u001b[32ma scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. \u001b[0m\n",
       "\u001b[32mAdditionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants\u001b[0m\n",
       "\u001b[32mof LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are \u001b[0m\n",
       "\u001b[32mpublicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Attention Is \u001b[0m\n",
       "\u001b[32mAll You Need\u001b[0m\u001b[32m]\u001b[0m\u001b[32m .5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\u001b[0m\n",
       "\u001b[32mWe show that the Transformer generalizes well to other tasks by applying it successfully to English constituency \u001b[0m\n",
       "\u001b[32mparsing both with large and limited training data.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from FFPDG: Fast, Fair and Private Data Generation\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
       "\u001b[32mKarthikeyan Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Monin-\\nder Singh, Kush R. \u001b[0m\n",
       "\u001b[32mVarshney, and Yunfeng Zhang. Ai fairness 360: An extensible toolkit for\\ndetecting, understanding, and mitigating \u001b[0m\n",
       "\u001b[32munwanted algorithmic bias, 2018.\\nTolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. \u001b[0m\n",
       "\u001b[32mMan is to\\ncomputer programmer as woman is to homemaker? debiasing word embeddings, 2016.\\nL Breiman. Random \u001b[0m\n",
       "\u001b[32mforests. Machine Learning 45, 2001.\\nFlavio P. Calmon, Dennis Wei, Karthikeyan Natesan Ramamurthy, and Kush R. \u001b[0m\n",
       "\u001b[32mVarshney. Opti-\\nmized data pre-processing for discrimination prevention, 2017.\\nL. Elisa Celis, Vijay Keswani, and\u001b[0m\n",
       "\u001b[32mNisheeth Vishnoi. Data preprocessing to mitigate bias: A maxi-\\nmum entropy based approach. In Hal Daum¬¥e III and \u001b[0m\n",
       "\u001b[32mAarti Singh \u001b[0m\u001b[32m(\u001b[0m\u001b[32meds.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, Proceedings of the 37th\\nInternational Conference on Machine Learning, volume 119 of \u001b[0m\n",
       "\u001b[32mProceedings of Machine Learn-\\ning Research, pp. 1349‚Äì1359. PMLR, 13‚Äì18 Jul 2020a. URL \u001b[0m\n",
       "\u001b[32mhttp://proceedings.mlr.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Evaluating large \u001b[0m\n",
       "\u001b[32mlanguage model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m based chat assistants is challenging due to their broad capabilities and the inadequacy of \u001b[0m\n",
       "\u001b[32mexisting benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to \u001b[0m\n",
       "\u001b[32mevaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, \u001b[0m\n",
       "\u001b[32mincluding position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose \u001b[0m\n",
       "\u001b[32msolutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by \u001b[0m\n",
       "\u001b[32mintroducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform.\u001b[0m\n",
       "\u001b[32mOur results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences \u001b[0m\n",
       "\u001b[32mwell, achieving over 80% agreement, the same level of agreement between humans\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from retrieval. \u001b[0m\n",
       "\u001b[32mOnly cite sources that are used. Make your response conversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Where do Ice Bears live?'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://094eaf58be01da97d1.gradio.live\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yCb3RVVfbmQ0",
   "metadata": {
    "id": "yCb3RVVfbmQ0"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4:** Saving Your Index For Evaluation\n",
    "\n",
    "After you've implemented your RAG chain, please save your accumulated vector store as shown [in the official documentation](https://python.langchain.com/docs/integrations/vectorstores/faiss#saving-and-loading). You'll have a chance to use it again for your final assessment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y4se5wQ4Afda",
   "metadata": {
    "id": "Y4se5wQ4Afda"
   },
   "outputs": [],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"docstore_index\")\n",
    "!tar czvf docstore_index.tgz docstore_index\n",
    "\n",
    "!rm -rf docstore_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LsI7NivbIgFw",
   "metadata": {
    "id": "LsI7NivbIgFw"
   },
   "source": [
    "If everything was properly saved, the following line can be invoked to pull the index from the compressed `tgz` file (assuming the pip requirements are installed). After you have confirmed that the cell can pull in your index, download `docstore_index.tgz` for use in the last notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qs8820ucIu1t",
   "metadata": {
    "id": "Qs8820ucIu1t"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
    "!tar xzvf docstore_index.tgz\n",
    "new_db = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
    "docs = new_db.similarity_search(\"Testing the index\")\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "as_3vWJGKB2F",
   "metadata": {
    "id": "as_3vWJGKB2F"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5:** Wrap-Up\n",
    "\n",
    "Congratulations! Assuming your RAG chain is all good, you're now ready to move on to the **RAG Evaluation [Assessment]** section!\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4",
   "metadata": {
    "id": "8098de2f-32b3-428e-8f3b-f54141ec40b4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
