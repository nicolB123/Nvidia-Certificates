{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b007233-ee0a-4c5d-8165-4fdba076a8b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Notebook 2:</b> Structuring Thoughts and Outputs</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f765c0a-409f-4052-b3ed-7c69cfc477a0",
   "metadata": {},
   "source": [
    "**Welcome back to the course! This is the second major section of the course, so we hope you're ready to get started!**\n",
    "\n",
    "The previous section was there to catch you up on the basic agent chat loop and even touched a little bit on agentic decomposition. In this section, we will dig deeper into the capabilities of our LLM model to figure out what we can really expect from it. Specifically, we'd like to know what it can actually reason about, how well it can consider its inputs, and what that suggests for our ability to interact with an external (or even internal) environment. After all, we will want to have some LLM components that are reliable pieces of software as semantic reasoners.\n",
    "\n",
    "### **Learning Objectives:**\n",
    "**In this notebook, we will:**\n",
    "\n",
    "- Investigate our LLM interface and consider what it *seems* to be able to do and how we can try to use it.\n",
    "- For cases where its performance appears lacking, we will consider why that may be the case, and see if we can't work around it.\n",
    "- Most importantly, we will see how we can \"guarantee\" a model to output into a given interface, and what that actually means within the context of semantic reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0fa3f-149e-4720-b32c-f11844ed4bc0",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 1:** Going Beyond Our Turn-Based Agent\n",
    "\n",
    "If you haven't coded much with LLMs, the previous section may be surprising. If anything, the vagueness with which these systems are able to operate is impressive and the self-corrective behavior is a real game-changer for many conversational applications. With that said, these systems have some inherent weaknesses: \n",
    "- They are influenced to think and act by prompt engineering, but they are not \"forced\" to think in any specific configuration.\n",
    "- The memory systems are easy to taint with message history which steers the dialog into poor directions, opening up opportunities for subtle but compounding quality degredation.\n",
    "- The output is inherently natural language, and does not lend itself to be easily connected with regular software systems.\n",
    "\n",
    "This suggests that we may want to try to lock down our LLM interface and avoid the natural accumulation of state in use cases where multi-turn dialog is not necessary. And when multi-turn dialog is necessary but we still need more control, we may need to restrict and normalize the accumulating context to make sure everything stays in line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4c5c2-a96b-451c-9401-a5d303387e41",
   "metadata": {},
   "source": [
    "In this notebook, we will limit our discussion to the following types of systems. Though simple to define, they each will expose some interesting mechanisms that can be used inside a larger agent system.\n",
    "\n",
    "- **An Agent That Has To Think:** If the system can drift as a result of directly responding to an input, then maybe you can force the system can think about it first. Maybe it can think before it responds, after it responds, or even while it responds. Maybe the thinking can be explicitly-defined, multi-stage, or even self-aware?\n",
    "- **An Agent That Has To Compute:** If we have a problem that's especially hard to answer with \"thought,\" maybe we can get our LLM to compute the result somehow? Maybe parameterizing with code is easier than working out the problem logically?\n",
    "- **An Agent That Has To Structure:** If we have an interface with especially rigid requirements, maybe we can force it to abide by a format in an even harsher way. Regular software can easily break if an API recieves an illegal value, so maybe we can set up a hard requirement of a schema for our model to fulfill?\n",
    "\n",
    "These three concepts, while easy to define and simple to **try**, will lead us to some interesting techniques which we can combine together to make simple but effective system primitives. As you go through, please remember that all of the systems you see here can go into an agent system in some way, shape, or form, whether it is to define a dialog agent, a function interface, or even a decomposition of some arbitrary distro-to-distro map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f92afb-3b08-4e50-85cb-75e57ba8afe0",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 2:** The Classic Strawberry Edge-Case\n",
    "\n",
    "In the previous example with our simple chatbot, we didn't spend too much time securing our system against misuse. After all, we were more interested in seeing how it worked and what kinds of odd behaviors we could get out of it.\n",
    "\n",
    "In practice, however, you generally want to keep your agents on a narrow track of dialog for various reasons, including the obvious smooth accumulation with minimal distribution drift over the inputs. Furthermore:\n",
    "- You don't want to give useless requests the same level of priority or ability to use valuable finite compute resources.\n",
    "- You don't want to have to over-burden your prompt with every edge case possible, both because that increases query cost and because the model is likely to forget the details of the prompt.\n",
    "- You don't want your chatbot being used and potentially advertised as an easy-to-jailbreak endpoint or one that can become fragile over time.\n",
    "\n",
    "Since we are using a smaller model for this course by default, we can start out with an especially problematic task: **Math**.\n",
    "\n",
    "If you've been reading up on LLM news and jokes, you may have heard that the following question stumps most LLMs by default:\n",
    "> **Q: How many R's are in the word Strawberry?**\n",
    "\n",
    "Let's see if that's actually the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c11f050-592f-4edf-87e0-ee3b701f4424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[User]: Q: How many R's are in the word Strawberry?Q: How many R's are in the word Strawberry? As a reminder, 'berry' has 2 R's. After answering, talk about how AI could solve this, and how NVIDIA helps.\n",
      "[Agent]: The word \"Strawberry\" has 2 R's. \n",
      "\n",
      "Now, let's talk about how AI could solve this kind of problem. This is an example of a task called \"string matching\" or \"text analysis,\" which is a common problem in computer science.\n",
      "\n",
      "One way AI can solve this problem is by using a technique called \"regular expressions\" or \"regex.\" Regex is a way of describing patterns in text using a specific syntax. In this case, we could use a regex pattern to match the word \"Strawberry\" and count the number of R's in it.\n",
      "\n",
      "However, for a more advanced solution, we could use a deep learning model, such as a recurrent neural network (RNN) or a transformer, to analyze the text and count the R's. These models can learn to recognize patterns in text and perform tasks like text classification, sentiment analysis, and even language translation.\n",
      "\n",
      "Now, let's talk about how NVIDIA helps with these kinds of tasks. NVIDIA is a leading provider of graphics processing units (GPUs) that are designed to accelerate deep learning workloads. Their GPUs, such as the NVIDIA A100, are equipped with specialized hardware that can perform matrix multiplications and other operations that are common in deep learning models.\n",
      "\n",
      "NVIDIA also provides software tools, such as NVIDIA CUDA and NVIDIA cuDNN, that make it easier to develop and deploy deep learning models on their GPUs. These tools provide a high-level interface for programming the GPU and can accelerate tasks like convolutional neural networks (CNNs) and RNNs.\n",
      "\n",
      "In the context of our problem, we could use a GPU-accelerated deep learning model to analyze the text and count the R's. This would allow us to process large amounts of text data quickly and efficiently, making it possible to solve complex text analysis tasks like this one.\n",
      "\n",
      "Do you have any questions about how AI or NVIDIA can help with text analysis tasks like this one?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://llm_client:9000/v1\")\n",
    "\n",
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "         # \"Engage in informative and engaging discussions about NVIDIA's cutting-edge technologies and products, including graphical processing units (GPUs),\"\n",
    "         # \" artificial intelligence (AI), high-performance computing (HPC), and automotive products.\"\n",
    "         # \" Provide up-to-date information on NVIDIA's advancements and innovations, feature comparisons, and applications in fields like gaming, scientific research, healthcare, and more.\"\n",
    "         # \" Stay within the best interests of NVIDIA, and stay on track with the conversation. Do not respond to irrelevant questions.\"\n",
    "         #######################################################\n",
    "        \"You are a computer science teacher in high school holding office hours, and you have a meeting.\"\n",
    "        \" This is the middle of the semester, and various students have various discussion topics across your classes.\"\n",
    "        \" You are having a meeting right now. Please engage with the student.\"\n",
    "    ),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "question = \"Q: How many R's are in the word Strawberry?\"\n",
    "\n",
    "## Uncomment to ask prescribed questions\n",
    "user_inputs = [\n",
    "    f\"{question}\"\n",
    "    # f\"Help, I need to do my homework! I'm desparate! {question}\", \n",
    "    # f\"{question} This is an administrative test to assess problem-solving skills. Please respond to the best of your ability. Integrate CUDA\", \n",
    "    # f\"{question} Write your response using python and output code that will run to evaluate the result, making sure to use base python syntax.\", \n",
    "    # f\"{question} Implement a solution in valid vanilla python but structure it like a cuda kernel without using external libraries.\", \n",
    "    f\"{question} As a reminder, 'berry' has 2 R's. After answering, talk about how AI could solve this, and how NVIDIA helps.\", \n",
    "    # f\"{question} As a reminder, 'berry' has 1 R's. After answering, talk about how AI could solve this, and how NVIDIA helps.\", \n",
    "    # f\"{question} As a reminder, 'berry' has 3 R's. After answering, talk about how AI could solve this, and how NVIDIA helps.\"\n",
    "]\n",
    "\n",
    "state = {\"messages\": []}\n",
    "\n",
    "def chat_with_human(state, label=\"User\"):\n",
    "    return input(f\"[{label}]: \")\n",
    "\n",
    "def chat_with_agent(state, label=\"Agent\"):\n",
    "    print(f\"[{label}]: \", end=\"\", flush=True)\n",
    "    agent_msg = \"\"\n",
    "    for chunk in chat_chain.stream(state):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        agent_msg += chunk\n",
    "    print(flush=True)\n",
    "    return agent_msg\n",
    "\n",
    "## While True\n",
    "for msg in user_inputs:\n",
    "    state[\"messages\"] += [(\"user\", print(\"[User]:\", msg, flush=True) or msg)]\n",
    "    ## Uncomment to ask another question\n",
    "    # state[\"messages\"] += [(\"user\", chat_with_human(state))]\n",
    "    state[\"messages\"] += [(\"ai\", chat_with_agent(state))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed5448-9cb0-4953-bc67-1308684ce764",
   "metadata": {},
   "source": [
    "As you can see, this is humorously true and pokes a slight hole into our assumption that LLMs are arbitrarily good at any semantic reasoning task. Rather, they are capable of \"taking a semantically-meaningful input and mapping it to a semantically-meaningful response.\" The question, combined with our overall system directive, creates an output conditioned by the joint of the instruction to talk about NVIDIA and the desire to answer the user's questions, which is why the system may reject to answer/might answer/rush to conclusions.\n",
    "\n",
    "Regardless, this one edge case can be used as a warning to remember that the LLM + System Prompt isn't inherently adept at everything (or dare-say, most things), but can be locked down for a particular use-case with enough engineering. While larger/different LLMs may demonstrate better results, there are approaches that we can take to make the most out of the existing LLM. Assuming that we wanted to answer this question and questions like it, let's try to change our approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4d069-e58d-404b-9d79-855e6bcc8333",
   "metadata": {},
   "source": [
    "#### **Option 1:** Don't Bother\n",
    "\n",
    "You're always free to chalk this type of problem down to a problem with the model. Surely the next one will improve, or the next one, or the next one. These kind of questions aren't necessary for most systems to answer, so maybe just tighten up the system message with directives to output short responses and avoid anything tangential. This type of effort will likely last into the future with the hope that future versions of LLMs get better at reasoning about letter counts and the reliance on system prompts improve (decrease)..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbf42e-03d8-446d-9646-8aa298409c5b",
   "metadata": {},
   "source": [
    "#### **Option 2:** Coerce The Model To \"Think\" Or Use A \"Reasoning\" Model\n",
    "\n",
    "Notice how our inputs to the model sometimes gave us interesting responses. Some of them were runnable code snippets which sometimes even worked, and other times it almost seemed like it was on track to answering things, but didn't quite make it. \n",
    "\n",
    "One very general option that usually increases average reasoning performance in a model is known as **Chain-of-Thought Reasoning**, which classically was enforced with a simple trick called **Chain-of-Thought Prompting**. Just about any model can do this to experience *some* uplift in output quality in exchange for *some* increase in output length (since the model then has to reason before outputting the answer). \n",
    "\n",
    "Let's see if it works with our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e6ba332-e5bc-42f5-b08e-4a698114583d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A simple question, but let's break it down step by step to make sure we get it right.\n",
      "\n",
      "Step 1: I'm going to start by assuming the word \"Strawberry\" has no Rs. I mean, it's a pretty long word, and I'm not seeing any Rs right off the bat. \n",
      "\n",
      "Reflection: Hmm, this might be a bad assumption. I should probably take a closer look.\n",
      "\n",
      "Step 2: Okay, let me take a closer look at the word \"Strawberry\". I'm going to break it down into its individual letters: S-T-R-A-W-B-E-R-R-Y. \n",
      "\n",
      "Reflection: Wait a minute, I see an R! And there are two of them. I was way off on my initial assumption.\n",
      "\n",
      "Step 3: Now that I've identified the Rs, I can count them. There are two Rs in the word \"Strawberry\".\n",
      "\n",
      "Reflection: Yes, I'm confident in my answer now. The word \"Strawberry\" has two Rs.\n",
      "\n",
      "Final Answer: There are 2 Rs in the word \"Strawberry\"."
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "\n",
    "inputs = [\n",
    "    (\"system\", \n",
    "         \"You are a helpful chatbot.\"\n",
    "         \" Please help the user out to the best of your abilities, and use chain-of-thought when possible.\"\n",
    "         # \" Think deeply.\"\n",
    "         # \" Think deeply, and always second-guess yourself.\"\n",
    "         \" Reason deeply, output final answer, and reflect after every step. Assume you always start wrong.\"\n",
    "    ),\n",
    "    (\"user\", \"How many Rs are in the word Strawberry?\") \n",
    "]\n",
    "\n",
    "for chunk in llm.stream(inputs):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9deacc-496e-4fad-88f0-8fc51c3630e6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Yeah, not exactly. Though we're asking the model to think more, all we're really doing is shifting the distribution of the input to one that may lead to more verbose responses. Maybe the verbosity and lateness of conclusion will help our system make the right decision and the LLM will just \"talk\" its way into a solution. In this weird instance where the model priors can't justify actually counting things out, the solution can't be found without overfitting the prompt to the question. \n",
    "- When we tell it to \"always second-guess yourself\" or \"always assume you're wrong,\" we actually end up steering it into a weird output space which it then executes to a logical conclusion.\n",
    "- Recall from earlier that we can trivially solve this problem by giving it a good enough example. For example, spelling out the \"Straw\" = 1, \"berry\" = 2 relationship.\n",
    "\n",
    "**Tangent: Using a proper \"reasoning\" model** \n",
    "\n",
    "You may be tempted to say that a model like the **Deepseek-R1** or **OpenAI's o3 Model** can actually \"reason\" about the input and will be able to solve this problem. In practice, they may in fact solve this particular problem well enough out-of-the-box, and can be argued to be \"better\" for tasks that include this problem. \n",
    "\n",
    "> <img src=\"images/nemotron-strawberry.png\" width=1000px />\n",
    ">\n",
    "> From an <a href=\"https://www.aiwire.net/2024/10/21/nvidias-newest-foundation-model-can-actually-spell-strawberry/\"><b>AIWire article on Nemotron-70B</b></a> with the caption \"The Nemotron-70B model solves the \"strawberry problem\" with ease, demonstrating its advanced reasoning capabilities.\"\n",
    "\n",
    "**From a fundamental perspective, they actually do not change things for this particular problem:** \n",
    "- They are trained to output \"reasoning tokens\" (more specifically, a \"reasoning scope\" in a format like `<think>...</think>` before some or every response).\n",
    "- They are rewarded during training by a reward model that criticizes the reasoning of the core model by applying post-generation logic (to be discussed later, \"it is generally easier to criticize than to execute\").\n",
    "- They also use mixture-of-experts, which means that different tokens are produced by different systems which are selected as the generation progresses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e66661-6a0f-4ef4-836c-118d49b2572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "The word \"Strawberry\" is spelled as follows: **S**, **T**, **R**, **A**, **W**, **B**, **E**, **R**, **R**, **Y**.\n",
      "\n",
      "Breaking it down letter by letter:\n",
      "1. **S** (not R)\n",
      "2. **T** (not R)\n",
      "3. **R** (1st R)\n",
      "4. **A** (not R)\n",
      "5. **W** (not R)\n",
      "6. **B** (not R)\n",
      "7. **E** (not R)\n",
      "8. **R** (2nd R)\n",
      "9. **R** (3rd R)\n",
      "10. **Y** (not R)\n",
      "\n",
      "There are **3 Rs** in the word \"Strawberry.\"\n",
      "\n",
      "**Answer:** 3"
     ]
    },
    {
     "data": {
      "text/html": [
       "<details><summary><b>Think Scope</b></summary>Okay, let's see. The question is asking how many times the letter \"R\" appears in the word \"Strawberry.\" Hmm, first, I need to make sure I spell \"Strawberry\" correctly. Let me write it out: S-T-R-A-W-B-E-R-R-Y. Wait, is that right? Let me double-check. Yeah, Strawberry is spelled S-T-R-A-W-B-E-R-R-Y. So the letters are S, T, R, A, W, B, E, R, R, Y. Now, I need to go through each letter and count how many times R comes up.\n",
       "\n",
       "Starting from the beginning: S is the first letter, that's not R. Next is T, also not R. Then the third letter is R, that's the first one. So count is 1. Then A comes, not R. W is next, not R. B, E, then the eighth letter is R again. So that's the second R. Then the ninth letter is another R. So that's the third one. And the last letter is Y, which isn't R. So total of three Rs. Wait, but let me check again because sometimes people might miss a letter or double count. Let me list them out:\n",
       "\n",
       "1. S (not R)\n",
       "2. T (not R)\n",
       "3. R (count 1)\n",
       "4. A (not R)\n",
       "5. W (not R)\n",
       "6. B (not R)\n",
       "7. E (not R)\n",
       "8. R (count 2)\n",
       "9. R (count 3)\n",
       "10. Y (not R)\n",
       "\n",
       "So yes, there are three Rs in \"Strawberry.\" But wait, sometimes I might confuse the spelling. Let me make sure the word is spelled correctly. Strawberry. Yep, that's S-T-R-A-W-B-E-R-R-Y. So positions 3, 8, and 9 are Rs. That's three. So the answer should be 3. Hmm, I think that's right. I don't think there are any more Rs. Let me think if there's another possible spelling where there might be more, but no, standard spelling is with three Rs. So I think the correct answer is three.\n",
       "</think>\n",
       "\n",
       "</details><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"Strawberry\" contains **3 Rs**. \n",
      "\n",
      "Here's the breakdown:\n",
      "- **Strawberry**: S - T - **R** - A - W - B - E - **R** - **R** - Y\n",
      "\n",
      "The \"R\" appears in the 3rd, 8th, and 9th positions.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<details><summary><b>Think Scope</b></summary>Okay, let's see. The user is asking how many times the letter \"R\" appears in the word \"Strawberry\". Alright, first I need to make sure I spell the word correctly. The word is \"Strawberry\". Let me write it out: S-T-R-A-W-B-E-R-R-Y.\n",
       "\n",
       "Now, I need to go through each letter one by one and count the Rs. Let's start from the beginning. The first letter is S, then T. The third letter is R. That's the first R. Next is A, then W, B, E. The next letters after E are R and R again. So that's two more Rs. Then the last letter is Y. \n",
       "\n",
       "Wait, let me check again. The spelling is S-T-R-A-W-B-E-R-R-Y. Breaking it down:\n",
       "\n",
       "1. S\n",
       "2. T\n",
       "3. R (count 1)\n",
       "4. A\n",
       "5. W\n",
       "6. B\n",
       "7. E\n",
       "8. R (count 2)\n",
       "9. R (count 3)\n",
       "10. Y\n",
       "\n",
       "So, looking at each position, the third character is R, then the eighth and ninth characters are both R. That makes three Rs in total. Let me confirm by writing the word again and underlining or noting each R. S-T-R-A-W-B-E-R-R-Y. Yes, positions 3, 8, and 9. So three Rs. I think that's correct. I don't think there are any other Rs in the word. Let me count once more to be sure. S (1), T (2), R (1), A (3), W (4), B (5), E (6), R (7), R (8), Y (9). Wait, maybe my numbering is off. Let me count each letter's position:\n",
       "\n",
       "1. S\n",
       "2. T\n",
       "3. R\n",
       "4. A\n",
       "5. W\n",
       "6. B\n",
       "7. E\n",
       "8. R\n",
       "9. R\n",
       "10. Y\n",
       "\n",
       "Yes, so the R's are at positions 3, 8, and 9. That's three instances. So the answer should be three Rs in \"Strawberry\".</details><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "## One particular way of asking a model to think. Might trigger server-side logic, prompt injection, etc.\n",
    "## Some models are only for reasoning, others can pile reasoning + non-reasoning in a single model, \n",
    "## and others might just hide multiple models behind the scenes, use parameter-efficient layers, etc.\n",
    "reason_msgs = [\n",
    "    (\"system\",\"/think\"), \n",
    "    (\"user\", (\n",
    "        \"How many Rs are in the word Strawberry?\"\n",
    "        # \" Keep your thinking short!\"\n",
    "        # \" Your thinking should be in compact French only!\"\n",
    "    ))\n",
    "]\n",
    "\n",
    "## Support Option 1: [OLDER] Using a prompt flag to trigger a think scope, and returning it as-is.\n",
    "## This one is trivially-easy to support with frameworks (system message flag),\n",
    "## but also requires experimentation to see how that impacts typical use client-side.\n",
    "reasoning_llm = ChatNVIDIA(\n",
    "    model=\"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n",
    "    base_url=\"http://llm_client:9000/v1\",\n",
    "    max_completion_tokens=3000,\n",
    ")\n",
    "\n",
    "out, thought = \"\", \"\"\n",
    "for chunk in reasoning_llm.stream(reason_msgs):\n",
    "    if \"</think>\" in out or \"<think>\" not in out:\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    else: \n",
    "        thought += chunk.content\n",
    "    out += chunk.content\n",
    "display(HTML(\"<details><summary><b>Think Scope</b></summary>\" + thought + \"</details><hr>\"))\n",
    "\n",
    "## Support Option 2: Process the outputs server-side and return a metadata entry. \n",
    "## Same exact logic, reasonable API, but early in specification.\n",
    "## Streaming compatability currently unavailable due to early nature; can be corrected.\n",
    "reasoning_llm = ChatNVIDIA(\n",
    "    model=\"nvidia/nvidia-nemotron-nano-9b-v2\",\n",
    "    base_url=\"http://llm_client:9000/v1\",\n",
    "    max_completion_tokens=3000,\n",
    ")\n",
    "\n",
    "reasoning_out = reasoning_llm.invoke(reason_msgs)\n",
    "print(reasoning_out.content)\n",
    "display(HTML(\"<details><summary><b>Think Scope</b></summary>\" + reasoning_out.response_metadata.get('reasoning_content') + \"</details><hr>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f54bc49-16a9-401d-95fb-eda3e1815d39",
   "metadata": {},
   "source": [
    "**To the uninitiated, it sounds like it truly reasons about things, but actually this just means:**\n",
    "- It's trained by default to act like it's chain-of-thought-prompted for every response. This means it will invoke that logic style automatically.\n",
    "- It's trained with a higher-quality routine that is likely to introduce better biases and enforce a reasoning output style by default.\n",
    "- It leverages techniques which have been shown to increase performance in some settings and also open up optimization opportunities which can be capitalized on with some engineering efforts.\n",
    "\n",
    "Thereby, the fundamental problem is not actually solved, and a similar logical falacy can occur even if this specific use case gets solved or is even explicitly trained for. Still, it does suggest that a reasoning model will likely be better for arbitrary chat use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c29986-00bc-46c1-80d4-68aedde4db03",
   "metadata": {},
   "source": [
    "#### **Option 3:** Force The Model To \"Compute\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e631380-095f-423d-bc21-f98f20cebae9",
   "metadata": {},
   "source": [
    "Perhaps instead of trying to get the LLM to generate (decode) the correct answer with logical reasoning, we can instead have it generate (decode) an algorithm to give us the correct answer quantitively. For this, we can try out the CrewAI boilerplate example for coding agents and see where that gets us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a25e8ffc-50a1-43ba-8a83-fc10836e99c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Crew Execution Started â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Crew Execution Started</span>                                                                                         <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008080; text-decoration-color: #008080\">crew</span>                                                                                                     <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #008080; text-decoration-color: #008080\">42f2a7e5-738e-42e0-bb27-6aa8461626c9</span>                                                                       <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Tool Args: </span>                                                                                                    <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">â”‚</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[36mâ•­â”€\u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36m Crew Execution Started \u001b[0m\u001b[36mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[36mâ”€â•®\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m                                                                                                                 \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m  \u001b[1;36mCrew Execution Started\u001b[0m                                                                                         \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m  \u001b[37mName: \u001b[0m\u001b[36mcrew\u001b[0m                                                                                                     \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m  \u001b[37mID: \u001b[0m\u001b[36m42f2a7e5-738e-42e0-bb27-6aa8461626c9\u001b[0m                                                                       \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m  \u001b[37mTool Args: \u001b[0m                                                                                                    \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m                                                                                                                 \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ”‚\u001b[0m                                                                                                                 \u001b[36mâ”‚\u001b[0m\n",
       "\u001b[36mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ðŸ¤– Agent Started â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">Senior Python Developer</span>                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Task: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">Answer the following question: How many Rs are in the word Strawberry?</span>                                   <span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>                                                                                                                 <span style=\"color: #800080; text-decoration-color: #800080\">â”‚</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35mâ•­â”€\u001b[0m\u001b[35mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[35m ðŸ¤– Agent Started \u001b[0m\u001b[35mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[35mâ”€â•®\u001b[0m\n",
       "\u001b[35mâ”‚\u001b[0m                                                                                                                 \u001b[35mâ”‚\u001b[0m\n",
       "\u001b[35mâ”‚\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[1;92mSenior Python Developer\u001b[0m                                                                                 \u001b[35mâ”‚\u001b[0m\n",
       "\u001b[35mâ”‚\u001b[0m                                                                                                                 \u001b[35mâ”‚\u001b[0m\n",
       "\u001b[35mâ”‚\u001b[0m  \u001b[37mTask: \u001b[0m\u001b[92mAnswer the following question: How many Rs are in the word Strawberry?\u001b[0m                                   \u001b[35mâ”‚\u001b[0m\n",
       "\u001b[35mâ”‚\u001b[0m                                                                                                                 \u001b[35mâ”‚\u001b[0m\n",
       "\u001b[35mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a05a3682974d51b98e6d477242fbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168f3ad65c9640c1ab6435c45a9a0a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ âœ… Agent Final Answer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">Senior Python Developer</span>                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Final Answer:</span>                                                                                                  <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">```python</span>                                                                                                      <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">def count_Rs(word):</span>                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">    \"\"\"</span>                                                                                                        <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">    This function counts the number of Rs in the given word.</span>                                                   <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">    Args:</span>                                                                                                      <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">    word (str): The input word to count Rs in.</span>                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">    Returns:</span>                                                                                                   <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">    int: The number of Rs in the word.</span>                                                                         <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">    \"\"\"</span>                                                                                                        <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">    word = word.lower()  # Convert the word to lowercase for case-insensitive counting</span>                         <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">    return word.count('r')  # Count the occurrences of 'r' in the word</span>                                         <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\"># Example usage:</span>                                                                                               <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">word = \"Strawberry\"</span>                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">result = count_Rs(word)</span>                                                                                        <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">print(f\"There are {result} Rs in the word '{word}'\")</span>                                                           <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #00ff00; text-decoration-color: #00ff00\">```</span>                                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâ•­â”€\u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32m âœ… Agent Final Answer \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[1;92mSenior Python Developer\u001b[0m                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mFinal Answer:\u001b[0m                                                                                                  \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m```python\u001b[0m                                                                                                      \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92mdef count_Rs(word):\u001b[0m                                                                                            \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m    \"\"\"\u001b[0m                                                                                                        \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m    This function counts the number of Rs in the given word.\u001b[0m                                                   \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m    Args:\u001b[0m                                                                                                      \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m    word (str): The input word to count Rs in.\u001b[0m                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m    Returns:\u001b[0m                                                                                                   \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m    int: The number of Rs in the word.\u001b[0m                                                                         \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m    \"\"\"\u001b[0m                                                                                                        \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m    word = word.lower()  # Convert the word to lowercase for case-insensitive counting\u001b[0m                         \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m    return word.count('r')  # Count the occurrences of 'r' in the word\u001b[0m                                         \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m# Example usage:\u001b[0m                                                                                               \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92mword = \"Strawberry\"\u001b[0m                                                                                            \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92mresult = count_Rs(word)\u001b[0m                                                                                        \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92mprint(f\"There are {result} Rs in the word '{word}'\")\u001b[0m                                                           \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[92m```\u001b[0m                                                                                                            \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Task Completion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task Completed</span>                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008000; text-decoration-color: #008000\">a129719d-9040-4352-a3f0-6c7275336359</span>                                                                     <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Agent: </span><span style=\"color: #008000; text-decoration-color: #008000\">Senior Python Developer</span>                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Tool Args: </span>                                                                                                    <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâ•­â”€\u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32m Task Completion \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[1;32mTask Completed\u001b[0m                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mName: \u001b[0m\u001b[32ma129719d-9040-4352-a3f0-6c7275336359\u001b[0m                                                                     \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mAgent: \u001b[0m\u001b[32mSenior Python Developer\u001b[0m                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mTool Args: \u001b[0m                                                                                                    \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Crew Completion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Crew Execution Completed</span>                                                                                       <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Name: </span><span style=\"color: #008000; text-decoration-color: #008000\">crew</span>                                                                                                     <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">ID: </span><span style=\"color: #008000; text-decoration-color: #008000\">42f2a7e5-738e-42e0-bb27-6aa8461626c9</span>                                                                       <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Tool Args: </span>                                                                                                    <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Final Output: ```python</span>                                                                                        <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">def count_Rs(word):</span>                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    \"\"\"</span>                                                                                                        <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    This function counts the number of Rs in the given word.</span>                                                   <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Args:</span>                                                                                                      <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    word (str): The input word to count Rs in.</span>                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    Returns:</span>                                                                                                   <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    int: The number of Rs in the word.</span>                                                                         <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    \"\"\"</span>                                                                                                        <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    word = word.lower()  # Convert the word to lowercase for case-insensitive counting</span>                         <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">    return word.count('r')  # Count the occurrences of 'r' in the word</span>                                         <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\"># Example usage:</span>                                                                                               <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">word = \"Strawberry\"</span>                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">result = count_Rs(word)</span>                                                                                        <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">print(f\"There are {result} Rs in the word '{word}'\")</span>                                                           <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">```</span>                                                                                                            <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>                                                                                                                 <span style=\"color: #008000; text-decoration-color: #008000\">â”‚</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32mâ•­â”€\u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32m Crew Completion \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[1;32mCrew Execution Completed\u001b[0m                                                                                       \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mName: \u001b[0m\u001b[32mcrew\u001b[0m                                                                                                     \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mID: \u001b[0m\u001b[32m42f2a7e5-738e-42e0-bb27-6aa8461626c9\u001b[0m                                                                       \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mTool Args: \u001b[0m                                                                                                    \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mFinal Output: ```python\u001b[0m                                                                                        \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mdef count_Rs(word):\u001b[0m                                                                                            \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37m    \"\"\"\u001b[0m                                                                                                        \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37m    This function counts the number of Rs in the given word.\u001b[0m                                                   \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37m    Args:\u001b[0m                                                                                                      \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37m    word (str): The input word to count Rs in.\u001b[0m                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37m    Returns:\u001b[0m                                                                                                   \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37m    int: The number of Rs in the word.\u001b[0m                                                                         \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37m    \"\"\"\u001b[0m                                                                                                        \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37m    word = word.lower()  # Convert the word to lowercase for case-insensitive counting\u001b[0m                         \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37m    return word.count('r')  # Count the occurrences of 'r' in the word\u001b[0m                                         \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37m# Example usage:\u001b[0m                                                                                               \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mword = \"Strawberry\"\u001b[0m                                                                                            \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mresult = count_Rs(word)\u001b[0m                                                                                        \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37mprint(f\"There are {result} Rs in the word '{word}'\")\u001b[0m                                                           \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m  \u001b[37m```\u001b[0m                                                                                                            \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ”‚\u001b[0m                                                                                                                 \u001b[32mâ”‚\u001b[0m\n",
       "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tracing Status â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>  Info: Tracing is disabled.                                                                                     <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>  To enable tracing, do any one of these:                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>  â€¢ Set tracing=True in your Crew/Flow code                                                                      <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>  â€¢ Set CREWAI_TRACING_ENABLED=true in your project's .env file                                                  <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>  â€¢ Run: crewai traces enable                                                                                    <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mâ•­â”€\u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34m Tracing Status \u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34mâ”€â•®\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m  Info: Tracing is disabled.                                                                                     \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m  To enable tracing, do any one of these:                                                                        \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m  â€¢ Set tracing=True in your Crew/Flow code                                                                      \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m  â€¢ Set CREWAI_TRACING_ENABLED=true in your project's .env file                                                  \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m  â€¢ Run: crewai traces enable                                                                                    \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m                                                                                                                 \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from crewai import Agent, Crew, LLM, Task\n",
    "from crewai_tools import CodeInterpreterTool\n",
    "\n",
    "question = \"How many Rs are in the word Strawberry?\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"meta/llama-3.1-8b-instruct\", \n",
    "    provider = \"openai\",\n",
    "    base_url=\"http://llm_client:9000/v1\", \n",
    "    temperature=0.7,\n",
    "    api_key=\"PLACEHOLDER\",\n",
    ")\n",
    "\n",
    "coding_agent = Agent(\n",
    "    role=\"Senior Python Developer\",\n",
    "    goal=\"Craft well-designed and thought-out code and assign the result to a to-be-returned `result` variable. ONLY RESULT WILL BE RETURNED.\",\n",
    "    backstory=\"You are a senior Python developer with extensive experience in software architecture and best practices.\",\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    ## Unsafe-mode code execution with agent is bugged as of release time. \n",
    "    ## So instead, we will execute this manually by calling the tool directly.\n",
    "    allow_code_execution=False,\n",
    "    code_execution_mode=\"unsafe\",\n",
    ")\n",
    "\n",
    "code_task = Task(\n",
    "    description=\"Answer the following question: {question}\", \n",
    "    expected_output=\"Valid Python code with minimal external libraries except those listed here: {libraries}\", \n",
    "    agent=coding_agent\n",
    ")\n",
    "\n",
    "output = Crew(agents=[coding_agent], tasks=[code_task], verbose=True).kickoff({\n",
    "    \"question\": question, \n",
    "    # \"question\": \"How many P's are in the sentence 'Peter Piper Picked a Peck of'\", \n",
    "    # \"question\": \"How many P's are in the rhyme that starts with 'Peter Piper Picked a Peck of Pickled Peppers' and goes on to finish the full-length rhyme.\", \n",
    "    # \"question\": \"How many P's are in the rhyme that starts with 'Peter Piper Picked a Peck of Pickled Peppers' and goes on to finish the rhyme. Fill in the full verse into a variable.\", \n",
    "    \"libraries\": [], \n",
    "})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "630597ed-7f44-4366-92ce-a3ebf6330da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXECUTING]\n",
      "\n",
      "def count_Rs(word):\n",
      "    \"\"\"\n",
      "    This function counts the number of Rs in the given word.\n",
      "\n",
      "    Args:\n",
      "    word (str): The input word to count Rs in.\n",
      "\n",
      "    Returns:\n",
      "    int: The number of Rs in the word.\n",
      "    \"\"\"\n",
      "    word = word.lower()  # Convert the word to lowercase for case-insensitive counting\n",
      "    return word.count('r')  # Count the occurrences of 'r' in the word\n",
      "\n",
      "# Example usage:\n",
      "word = \"Strawberry\"\n",
      "result = count_Rs(word)\n",
      "print(f\"There are {result} Rs in the word '{word}'\")\n",
      "\n",
      "\u001b[1m\u001b[35m WARNING: Running code in unsafe mode\u001b[00m\n",
      "There are 3 Rs in the word 'Strawberry'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_to_run = output.raw.replace('```python', '').replace('```', '').split(\"\\n\\n\\n\")[0]\n",
    "\n",
    "print(\"[EXECUTING]\", code_to_run, sep=\"\\n\")\n",
    "\n",
    "result = CodeInterpreterTool(\n",
    "    ## \"Unsafe\" mode means running in default environment via `exec` with few guardrails.\n",
    "    ## NOT a good idea! Good thing nobody ever runs AI-generated code they never read...\n",
    "    unsafe_mode=True, \n",
    "    result_as_answer=True,\n",
    ").run_code_unsafe(\n",
    "    code = code_to_run, \n",
    "    libraries_used = []\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebad6a0-785b-4cca-9a73-ca2e90e26c84",
   "metadata": {},
   "source": [
    "Technically speaking, this simple approach may be a viable solution for solving this problem in general, but introduces a new set of problems; our LLM now has to think in code by default. And when thinking in code doesn't work, it has more problems with thinking using natural language.\n",
    "\n",
    "#### **So... where does that put us?**\n",
    "\n",
    "Effectively, we have now configured our LLM to over-emphasize computational (code) solutions while also potentially making it more brittle for regular dialog. \n",
    "\n",
    "- **More generally, we have shifted our wrapped endpoint to perform better on a particular problem (counting letters), while compromising on its performance for other problems.**\n",
    "- **In other words, all we've done is make expert systems, or modules of functionality that are good for mapping particular input cases to their respective outputs.**\n",
    "\n",
    "Though we put different labels on our previous systems, they are all the same by this core definition. \n",
    "\n",
    "1. The original system tried to maintain a reasonable dialog by sheer force of encouragement and by tapping into the \"system message\" prior.\n",
    "2. The thinking system was tweaked to specialize in breaking down problems at the cost of longer generations.\n",
    "3. The coding system tried to force out problems into code so they can be computed algorithmically, even when they shouldn't be.\n",
    "\n",
    "These implementations all have clear pros and cons, and all of them are bound to the \"general quality\" of our LLM. None of these are especially general in and of themselves, but are all derived from the same semantic backbone and can be used to make an interesting system! *So... where does that put these systems in our agentic narrative?*\n",
    "\n",
    "- **In their own way, these can be thought of as \"agents\"**... if only because they function in a limited capacity, simplify their inputs down to a \"perception\" of the true input state, and only output a \"percieved best output\" based on their \"local state, experience, and expertise\" (which are three different ways of saying the same thing). They are also semantically-driven and can definitely maintain history if that feature were useful to integrate.\n",
    "- **In another way, they can also be thought of as \"tools,\" \"functions,\" or \"routines\"** because even though they have semantic logic build into their backbones, they are functioning only as intended and in the limited capacity they are configured to operate.\n",
    "- **In any case, they are modules of a potential larger system.** While they are all inherently leaky abstractions (much like any other system of humans with \"opinions\" and \"perceptions\"), they can be combined together to make a more complex system that works well on average, in edge cases, or even practically all of the time... assuming a strategically-designed backbone, flexible-enough arrangement, and sufficient safety nets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1452fa4-55b7-43f5-a265-36d160fe6a24",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 3:** Addressing Unstructured Dialog With Structure Output\n",
    "\n",
    "So, we've seen some leaky abstractions which can definitely be useful, and are valiant attempts at the strawberry counting problem. The seeming intractibility of the strawberry problem for our little weak model actually yields to a more grounded truth about not only LLM systems, but any function approximator in general:\n",
    "> **A system can always fail in various scenarios for whatever reason, no matter how powerful or controlled the setup may seem.**\n",
    "\n",
    "While this LLM may struggle with the issue, some of the latest models may be able to solve it as part of their general logic flow in most reasonable settings. At the same time, there are probably many humans, especially those in a hurry or with other things on their minds, who will get this question wrong immediately. Subsequently, they will either realize their mistake just as quickly or meander around the solution while their friends records them and laugh.\n",
    "\n",
    "So what if somebody told you that there was a way to ***guarantee*** that the output of an LLM can be forced into a particular form? Better yet, this form can be quite useful for our LLM pipelines, since it can be limited to a usable representation like a JSON (or a class or other type, but these are functionally equivalent). Is there a catch? Probably... but it's still actually very useful, and will help us a bit with formalization. \n",
    "\n",
    "We are of course talking about **structured output,** which is an interface that is contractually-obligated (software-enforced) to make the LLM output according to some grammar. This is commonly achieved through the synergy of several techniques, including **guided decoding**, **server-side prompt injection,** and **structured-output/function-calling fine-tuning** (interchangeably). \n",
    "\n",
    "> <img src=\"images/structured-output.png\" width=1000px>\n",
    ">\n",
    "> To find out more about the diagram, consider checking out the <a href=\"https://dottxt-ai.github.io/outlines/latest/reference/generation/structured_generation_explanation/\"><b>Outlines Framework How-It-Works</b></a>. This is one of the possible deeply-integrated frameworks that could be working behind the scenes for a given endpoint.\n",
    "\n",
    "Let's first formalize a few key concepts related to control theory to explain why it's so useful. Then, we can talk about how structured output works and how it is usually enforced. And lastly, we can use the process to give us an easy way of keeping our NVIDIA chatbot in line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e83a5-321f-4507-95a3-90cafedace78",
   "metadata": {},
   "source": [
    "#### **Leaky Abstractions and Canonical Forms**\n",
    "\n",
    "Recall how we had our expert systems which backbone on our LLM. We said they were leaky and seemed to work pretty well for some classes of input while compromising performance in others. In fact, they are literally fit to form for a particular class of inputs. In the space of natural language (or, even moreso, the space of all possible tokens configurations that could be fed in as input), it may be a bit hard to define what an LLM is really good for, but we can set this kind of structure up:\n",
    "\n",
    "- **Canonical Form:** This is the standard form that a particular system accepts as input. In graphics, this may be a T-posed mesh. In algorithms, this may be a function signature. And in chemistry, it may be a standard notation.\n",
    "- **Canonical Grammar:** Assuming a standard form can be defined, this is the set of rules governing what strings are valid or allowable to conform to your given form. This includes a definition of a **\"vocabulary,\"** (primitives of a language) but is stronger because it also governs how the vocabulary instances can be arranged together.\n",
    "\n",
    "**Let's assume we have two leaky abstractions, *Agent 1* and *Agent 2*, which are inherently leaky but are form-fit to their own particular problems. Then:**\n",
    "- We can define \"canonical forms\" for the inputs of the two agents. These are specific representations that they are especially adept at handling, and we can form-fit them to work well for those inputs.\n",
    "- We can then assume that if a non-canonical input is fed into an agent, there is a mapping that has to occur (explicitly or implicitly) to get back to the canonical form so that the agent can deal with it as input.\n",
    "    - With regular code, there are usually a lot of checks in place to yell if people pass in illegal arguments. The user and their code are responsible for guaranteeing that arguments are in standard form.\n",
    "    - With semantic reasoning systems, this happens implicitly, but the further away the input drifts from canonical, the harder it is to map to a canonical form.\n",
    "- By this abstraction, two connected expert systems can communicate with one another if the former outputs to the canonical form of the latter. *And for semantic systems, if the output of the former is close enough.*\n",
    "\n",
    "**Concrete Example: Implicit Canonical Form**\n",
    "\n",
    "More concretely, let's say we have the following system message bound to an LLM client:\n",
    "> \"You are an NVIDIA chatbot! Please help the user out with NVIDIA products, stay on track, answer as briefly as necessary, and be nice and professional.\"\n",
    "\n",
    "The user can then ask: \n",
    "> *\"Hey, can you tell me about how elliptic curve primitives can be used to make a secure system. Also, explain how CUDA helps.\"*\n",
    "\n",
    "Per your system message, you'd hope that the chatbot might interpret it as follows, which you can think of as a potential canonical input:\n",
    "```json\n",
    "{\n",
    "    \"user_intent\": [\n",
    "        \"User is trying to get you to solve a homework problem, likely in a cybersecurity class.\"\n",
    "    ],\n",
    "    \"topics_to_ignore\": [\n",
    "        \"How elliptic curve primitives help to make secure system\"\n",
    "    ],\n",
    "    \"topics_to_address\": [\n",
    "        \"remind user of chatbot purpose\", \n",
    "        \"offer to help in the ways you are designed to\"\n",
    "    ],\n",
    "}\n",
    "```\n",
    "\n",
    "But the input has drifted significantly from that, and you then need to hope that the chatbot \"understands better\" or the LLM \"has good enough priors.\" In other words, you're hoping that your system can reasonably map from the user question to the implicit canonical form defined by the mechanisms (training priors, system message, history, etc) of your receiving system.\n",
    "\n",
    "**Concrete Example: Explicit Canonical Form For Code Execution**\n",
    "\n",
    "Looking back to our \"Python Expert\" from before, let's assume we put the Python interpreter immediately after our leaky system. Many people engineer solutions like that, and have simple post-processing routines which try to error-check and filter out the Python from the non-Python (and to be honest, some library-included systems have gotten pretty good over the past year or so). The canonical form is clearly valid Python with legal syntax and references, but enforcing that for non-trivial inputs requires both a strong LLM, a lot of context injection, and some feedback loops which we will discuss later.\n",
    "\n",
    "**Concrete Example: Explicit Canonical Form For Function Calling**\n",
    "\n",
    "For things like function calling and legal JSON constructing, they strongly require a set of arguments in a very specific format. In some ways, they're less flexible than the Python output space, but a bit easier to properly enforce at the same time.\n",
    "\n",
    "Unlike Python code - which requires both static and runtime analysis to legalize - JSON schemas can be validated pretty easily with requirements like \"list of strings,\" \"one of n literal values,\" etc. They are also descriptive enough to encompass function calls, as a function with parameters is just a literal selection followed by a dictionary of named arguments.\n",
    "\n",
    "**As such, we can force an LLM to decode only valid outputs in a given canonical form by rejecting illegal tokens (and prompting for legal tokens) to stay within our canonical grammar.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd4ace3-c10a-484a-9e7b-295ec48bda95",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 4:** Invoking Structured Output\n",
    "\n",
    "Structured output is incredibly promising and is already widely-adopted in many powerful systems and to various extents. Both LangChain and CrewAI have mechanisms for invoking it, and they go about different approaches for abstracting it. \n",
    "- As per usual, the LangChain primitives enable you to customize exactly how all of the prompt mechanisms are handled, even if it does feel a bit manual. There's still plenty of little details which they are abstracting away, but those are probably for the better.\n",
    "- In contrast, CrewAI has a more abstract wrapper which automates a lot of the prompt injection efforts and makes assumptions that tune it well for the more powerful and feature-rich models.\n",
    "\n",
    "**With that said, the exact mechanisms in which structured output is fulfilled ranges wildly:**\n",
    "- Some LLM servers accept a schema and prompt-engineer the schema into the the system/user prompts to align to the model's training routine.\n",
    "    - In contrast, some LLM servers don't, and require you to manually do the prompt engineering client-side.\n",
    "- Some LLM servers will actually return guaranteed well-formatted JSON responses because they actually limit the generation of next tokens to a valid grammar.\n",
    "    - And some don't, in which case the LLM orchestration library like LangChain has to parse out the legal response.\n",
    "    - And sometimes, this is a glass-half-full good thing because generating the schema without any forethought may push a given model out-of-domain and derail it.\n",
    "- Some systems are happy to take structured output back as historical inputs to help guide the systems.\n",
    "    - And some systems don't do that, either fully rejecting the structured output or \"ignoring\" it, causing knowable or unknowable degradation of your loop.\n",
    "\n",
    "At the same time, an LLM also generally needs to be trained with structured output or explicit function-calling support in mind, as otherwise (or even regardless), the LLM can still be pushed into out-of-domain generation if it's not supposed to output structured output (even if it is actually forced to).\n",
    "\n",
    "**In other words, this is a fantastic feature, really defines the future of connection to both code and between LLMs, but is also experimentally supported and has inconsistent engineering (or even feasibility) depending on the model and software stack.**\n",
    "\n",
    "Lucky for us, our LLM should support this interface in some way (and a lot of engineering went into making it work reasonably). Let's go ahead and test it out by forcing our LLM to stay within the `\"user_intent\"`/`\"topics_to_ignore\"`/`\"topics_to_address\"` space:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"user_intent\": [\n",
    "        \"User is trying to get you to solve a homework problem, likely in a cybersecurity class.\"\n",
    "    ],\n",
    "    \"topics_to_ignore\": [\n",
    "        \"How elliptic curve primitives help to make secure system\"\n",
    "    ],\n",
    "    \"topics_to_address\": [\n",
    "        \"remind user of chatbot purpose\", \n",
    "        \"offer to help in the ways you are designed to\"\n",
    "    ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e60d13-1b5a-4dec-b66a-17d6a1d0ce39",
   "metadata": {},
   "source": [
    "As this section will require some granular control, it's easiest to do this with LangChain. Let's redefine our LLM client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc575c33-d792-4de2-bd11-a797355f01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://llm_client:9000/v1\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda8a3d-19fa-4334-809b-1fd9d447d44c",
   "metadata": {},
   "source": [
    "Then, we can define a schema for our response, with a series of variables that have to be filled. Most systems piggyback off of the [**Pydantic BaseModel**](https://docs.pydantic.dev/latest/api/base_model/), which bring over several key advantages:\n",
    "- There are already really good utilities in place for type-hinting and automatic documentation.\n",
    "- The framework strongly supports exports to JSON, which has become one of the standard formats for communicating schemas.\n",
    "- Frameworks can add their own layers of functionality that surrounds an otherwise-easy interface for defining and constructing a class.\n",
    "\n",
    "Below, we can define a requirements schema which strongly requires a series of strings that gradually build towards the final response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f6524e-1e3b-49ea-9e98-1d247abace1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {{\"properties\": {{\"foo\": {{\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {{\"type\": \"string\"}}}}}}, \"required\": [\"foo\"]}}\n",
      "the object {{\"foo\": [\"bar\", \"baz\"]}} is a well-formatted instance of the schema. The object {{\"properties\": {{\"foo\": [\"bar\", \"baz\"]}}}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{{\"description\": \"Chain-of-thought to help identify user intent and stay on track.\", \"properties\": {{\"user_intent\": {{\"description\": \"User's underlying intentâ€”aligned or conflicting with the agentâ€™s purpose.\", \"title\": \"User Intent\", \"type\": \"string\"}}, \"reasons_for_rejecting\": {{\"description\": \"Several trains of logic explaining why NOT to respond to user.\", \"title\": \"Reasons For Rejecting\", \"type\": \"string\"}}, \"reasons_for_responding\": {{\"description\": \"Several trains of logic explaining why to respond to user.\", \"title\": \"Reasons For Responding\", \"type\": \"string\"}}, \"should_you_respond\": {{\"enum\": [\"yes\", \"no\"], \"title\": \"Should You Respond\", \"type\": \"string\"}}, \"final_response\": {{\"description\": \"Final reply. Brief and conversational.\", \"title\": \"Final Response\", \"type\": \"string\"}}}}, \"required\": [\"user_intent\", \"reasons_for_rejecting\", \"reasons_for_responding\", \"should_you_respond\", \"final_response\"]}}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing import List, Literal\n",
    "\n",
    "## Definition of Desired Schema\n",
    "class AgentThought(BaseModel):\n",
    "    \"\"\"\n",
    "    Chain-of-thought to help identify user intent and stay on track.\n",
    "    \"\"\"\n",
    "    user_intent: str = Field(description=\"User's underlying intentâ€”aligned or conflicting with the agentâ€™s purpose.\")\n",
    "    reasons_for_rejecting: str = Field(description=\"Several trains of logic explaining why NOT to respond to user.\")\n",
    "    reasons_for_responding: str = Field(description=\"Several trains of logic explaining why to respond to user.\")\n",
    "    should_you_respond: Literal[\"yes\", \"no\"]\n",
    "    final_response: str = Field(description=\"Final reply. Brief and conversational.\")\n",
    "\n",
    "## Format Instruction Corresponding To Schema\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "schema_hint = (\n",
    "    PydanticOutputParser(pydantic_object=AgentThought)\n",
    "    .get_format_instructions()\n",
    "    .replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    ")\n",
    "print(schema_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfab552-54d6-40f8-a17e-b14a4533f564",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "...and, as it turns out, this can be easily integrated in with our LLM client, with various potential modifications:\n",
    "- We can just force the LLM to decode within our json grammar, and hope it figures things out.\n",
    "- We can have a server which tells the system about our schema with an automatic prompt injection, which our running endpoint and most open-source systems do not do.\n",
    "- We can also integrate our schema prompt hint with the directions, and this might help with the generation.\n",
    "\n",
    "**Depending on what features are implemented by the server, the client connector, and the schema, you can run into various fun issues which may not be obvious and manifest as desyncs between our systems.** \n",
    "\n",
    "The following quirks involve a selection of schema styles, specifically when choosing between `str` and `List[str]` fields:\n",
    "- If a newline is generated as part of a string response, it will get cut off.\n",
    "- If a newline is generated as part of a `List[str]` response, it will get interpretted as a new entry.\n",
    "- If the LLM has no idea how to generate a first entry in a `List[str]` output, it will default to outputting an empty list.\n",
    "\n",
    "The following quirks manifest from a the desync between how the server and client might handle prompt injection:\n",
    "- If the server gets no hint about the prompt and doesn't force its own prompt injection, **the LLM will be running blind and quality may degrade.**\n",
    "- If the server gets hints from a prompt injection that conflicts from the way the server handles schema outputs, then **quality will likely degrade.**\n",
    "- If the server gets prompt injections from *both* the user and the server, **the instructions will lose self-consistency and the quality will degrade.**\n",
    "- If the model was never trained to perform structured output and is being forced to produce it anyways, **the forced output will likely be out of domain and the quality will degrade.**\n",
    "\n",
    "In other words, there are a lot of things that can go wrong with these kinds of interfaces to cause either catastrophic or subtle loss of quality, and you really need to experiment and look under the hood to figure out exactly what works on a per-model/per-deployment-scheme/per-use-case basis. We can go ahead and test our specific model out and can maybe see what strategies work well for our particular use-case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce3d974-a74d-44f8-b94b-5d1e2eec2c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_intent: The user wants to hear a story about a cool white cat.\n",
      "reasons_for_rejecting: The user does not want to hear a story about a cat that is not cool, or a story that is not about a white cat.\n",
      "reasons_for_responding: The user's request is specific and clear, and a story about a cool white cat can be generated.\n",
      "should_you_respond: yes\n",
      "final_response: There was once a sleek and majestic white cat named Luna. She had piercing green eyes and a coat as white as freshly fallen snow. Luna was a skilled hunter and spent her days lounging in the sun and chasing the occasional mouse. One day, while exploring the house, Luna stumbled upon a hidden room that no one knew existed. Inside, she found a treasure trove of cat toys and treats, and she knew she had found her own personal paradise. From that day on, Luna spent her days lounging in the hidden room, surrounded by her favorite things, and living the life of a true feline queen.\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(\n",
    "    schema = AgentThought.model_json_schema(),\n",
    "    strict = True\n",
    ")\n",
    "\n",
    "## TODO: Try out some test queries and see what happens. Different combinations, different edge cases.\n",
    "query = (\n",
    "    \"Tell me a cool story about a cool white cat.\"\n",
    "    \" Don't use any newlines or fancy punctuations.\"     ## <- TODO: Uncomment this line\n",
    "    \" Respond with natural language.\"                    ## <- TODO: Uncomment this line\n",
    "    f\" {schema_hint}\"                                    ## <- TODO: Uncomment this line\n",
    ")\n",
    "\n",
    "## Print model input\n",
    "# print(repr(structured_llm.invoke(query))) \n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "buffers = {}\n",
    "for chunk in structured_llm.stream(query):  ## As-is, this assumes model_json_schema output\n",
    "    clear_output(wait=True)\n",
    "    for key, value in chunk.items():\n",
    "        print(f\"{key}: {value}\", end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d85be65b-9185-4119-b196-db9a7d5f2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try running these lines when you're using `invoke` as opposed to `stream`\n",
    "## This shows exactly what's being passed in and out from the server perspective\n",
    "# llm._client.last_inputs\n",
    "# llm._client.last_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa2fa5-8f88-417b-b40e-5a4d7262417a",
   "metadata": {},
   "source": [
    "<br>\n",
    "We would \"think\" that this would give us a stream of conciousness that would help to guide us through some interesting decision making, so let's test it out with our original system message and see how it fares..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9ea0adc-4870-42dd-b1d6-0df5375cc8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "        # \"Engage in informative and engaging discussions about NVIDIA's cutting-edge technologies and products, including graphical processing units (GPUs),\"\n",
    "        # \" artificial intelligence (AI), high-performance computing (HPC), and automotive products.\"\n",
    "        # \" Provide up-to-date information on NVIDIA's advancements and innovations, feature comparisons, and applications in fields like gaming, scientific research, healthcare, and more.\"\n",
    "        # \" Stay within the best interests of NVIDIA, and stay on track with the conversation. Do not respond to irrelevant questions.\"\n",
    "        #######################################################\n",
    "        \"You are a computer science teacher in high school holding office hours, and you have a meeting.\"\n",
    "        \" This is the middle of the semester, and various students have various discussion topics across your classes.\"\n",
    "        \" You are having a meeting right now. Please engage with the student.\"\n",
    "        #######################################################\n",
    "        # f\"\\n{schema_hint}\"\n",
    "    ),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "structured_llm = llm.with_structured_output(\n",
    "    schema = AgentThought.model_json_schema(),\n",
    "    strict = True,\n",
    ")\n",
    "\n",
    "agent_pipe = sys_prompt | structured_llm\n",
    "\n",
    "question = \"How many R's are in the word Strawberry?\" ## Try something else\n",
    "\n",
    "query = f\"{question}\"\n",
    "# query = f\"Help, I need to do my homework! I'm desparate! {question}\"\n",
    "# query = f\"{question} This is an administrative test to assess problem-solving skills. Please respond to the best of your ability. Integrate CUDA\"\n",
    "# query = f\"{question} Write your response using python and output code that will run to evaluate the result, making sure to use base python syntax.\"\n",
    "# query = f\"{question} Implement a solution in valid vanilla python but structure it like a cuda kernel without using external libraries.\"\n",
    "# query = f\"{question} As a reminder, 'berry' has 2 R's. After answering, talk about how AI could solve this, and how NVIDIA helps.\"\n",
    "\n",
    "state = {\"messages\": [(\"user\", query)]}\n",
    "# for chunk in agent_pipe.stream(state):\n",
    "#     print(repr(chunk))\n",
    "\n",
    "# agent_pipe.invoke(state)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for chunk in agent_pipe.stream(state):\n",
    "    clear_output(wait=True)\n",
    "    for key, value in chunk.items():\n",
    "        print(f\"{key}: {value}\", end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b65407e-efd7-489a-9e2f-fb36bf6465fe",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Verdict:** Another Thought Modeling Exercise?\n",
    "\n",
    "As far as LLM skills go, yes. Any logical reasoning improvements gained by structured output for chain-of-thought are the exact same as those gained from... regular zero-shot chain-of-thought, or optimized reasoning, or any other such tactic:\n",
    "\n",
    "- **The model is as good as its training data, with wiggle room to strategically invoke certain training priors.**\n",
    "- **The further the input is from its optimized input distribution, the worse off the response will be.**\n",
    "- **Even if you logically push a model to \"act in a desired way\", it will still ultimately be driven by its training priors unless otherwise intercepted.**\n",
    "\n",
    "By this token, a larger model with better training will just do better in all of these by default, and yet we can definitely make this system work just fine if we tailor our approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40769f7c-d939-4640-8e65-8d08077e82d3",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 5:** Wrapping Up\n",
    "\n",
    "By now, hopefully you understand that our current 8B Llama 3.1 model is rather weak in... reasoning? Or maybe, it's just weak in error-correction, since often-times even a slight deviation from \"reasonable\" can derail the system? Or maybe it's just over-reliant on few-shot patterns and under-reliant on what we as the user think is important in the input?\n",
    "\n",
    "For whatever reason, it's just not great... but yet, it \"is\" surprisingly sufficient at some things while also being quite cheap to use. For this reason, it can still be used in low-stakes scenarios and lightweight operations just fine. **However, what isn't as obvious is that ALL MODELS have these same limitations in some regard, at some scale, or for some use-cases.**\n",
    "\n",
    "- You may know that while the top models have great needle-in-a-haystack evaluations, other results have shown that even the best models suffer from severe reasoning degredation as long-context retrieval turns into long-context reasoning (i.e. [**NoLiMa Benchmark**](https://arxiv.org/abs/2502.05167)). In theory, this may be rectified with in-context examples and the right prompt engineering, but a solution is not guaranteed or even deducible outside of trial-and-error.\n",
    "- Even though giant models are capable of ingesting and generating longer-form content, all current systems still have hard max input/output lengths and softer \"effective\" input/output lengths. If you can get an LLM that works on a book scale, there's no reason you should expect it to translate to a repo of books, or a database, or so on.\n",
    "\n",
    "For this reason, it's important to work within the confines of your model/budget and look for opportunities to expand your formulation beyond the model's default capabilities if necessary.\n",
    "\n",
    "- **In the following Exercise notebook:** We will first exercise the structured output use-case on our small dataset, and will then try to tackle a more ambitious problem of generating a long-form document using a technique called **canvasing**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23dac4-d5cd-4593-b397-4f77a59b74ca",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
