{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b007233-ee0a-4c5d-8165-4fdba076a8b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Assessment Warm-Up:</b> Creating A Basic Retriever Node</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929680ed-a1d5-4e23-923f-cf093b9a3659",
   "metadata": {},
   "source": [
    "We've now been introduced to ReAct as a concept, and could make a toy system that exhibits this property. However, focusing on it as the ideal paradigm isn't necessarily always correct. It is extremely flexible and does have its purposes. When organized by an overarching strong LLM, this type of loop can run for quite a while since the tools can be used to hide details from the main loop. Integrate this system in with some context re-canonicalization step, and you could theorhetically go on forever.\n",
    "\n",
    "From an implementation perspective, creating a coherent system with this is actually quite simple in principle. It's a good exercise, but isn't worth the effort to build in this course as it doesn't show off any new features:\n",
    "\n",
    "> **HINT:** It's the agent loop from Section 3, but the LLM is bound to call functions, the stop condition is when no tool is called, and some effort is needed to make sure the tool responses actually help to enforce a valid prompting strategy.\n",
    "\n",
    "This paradigm is great for ***horizontal agents*** and ***supervisor-style nodes***, where you can keep tossing more functions (\"ways to interface with the environment\") at the LLM while hoping it will pick something up. Hence, why this pattern works better with a stronger LLM where the state of \"it just works\" is easier to achieve.\n",
    "\n",
    "In this notebook, we will try implementing a ***tool-like agent*** system which is tweaked for the specific problem and aims to hide its runtime details from any other main event loops which may be overseeing it. (i.e. the user, an overarching ReAct loop, some other supervisor, etc). In doing so, we will rediscover some of the interfaced from the RAG course while recontextualizing them into our LangGraph workflows.\n",
    "\n",
    "**This exercise is specifically intended to prepare you for the assessment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3524c160-7261-445d-b624-90927f69c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints._statics import MODEL_TABLE\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "llama_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\", clean_up_tokenization_spaces=True)\n",
    "def token_len(text):\n",
    "    return len(llama_tokenizer.encode(text=text))\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://llm_client:9000/v1\")\n",
    "llm = ChatNVIDIA(model=\"nvidia/llama-3.1-nemotron-nano-8b-v1\", base_url=\"http://llm_client:9000/v1\")\n",
    "## If you know structured output is supported and want less warnings:\n",
    "MODEL_TABLE[llm.model].supports_structured_output = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581704cf-5fc4-43ce-93f8-ae5f9a1738e6",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 1:** Pulling In Some Boilerplate\n",
    "\n",
    "Let's start off by pulling in our old-reliable base specification for a simple multi-turn system. To make this and subsequent processes easier, we will switch to using an entirely Command-based routing scheme, and will try to reuse components as they need integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c3da50-2dcd-4263-9b26-41a92fb2d77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]: What is the most popular food among polar bears?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: I'm sorry for the laughter, but I couldn't find a source indicating that polar bears have different popular food preferences among them. Polar bears are typically carnivores that primarily hunt seals, which are their main food source. Additionally, they may eat other marine mammals like walruses or caribou in certain seasons or regions. However, there is no definitive information about a \"most popular\" food among individual polar bears."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]: How many polar bears do live in Alaska then?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: The number of polar bears in Alaska is dynamic and fluctuates in response to various factors such as snow cover, prey populations, and human factors.  The most recent estimate released by the U.S. Fish and Wildlife Service at the end of 2023 was approximately 3,500 to 4,000 polar bears. This estimate is based on a long-term trend of declining numbers due to climate change. \n",
      "\n",
      "*   It's important to note that these estimates can vary between studies as methodologies and data collection techniques can differ. \n",
      "*   Polar bears are also found in the Arctic Ocean, which includes parts of Canada as well. \n",
      "\n",
      "If you are looking for the most current numbers, it's best to consult the most recent scientific studies and reports from reputable sources. "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]: Maybe the polar bears of Alaska do have a different most popular food than the ones living on Antartica\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: Indeed, while polar bears (Ursus maritimus) are primarily found in the Arctic with populations divided between adaptations in the Arctic Island, more commonly in Greenland and physical portions of the Arctic Islands with less snow in Turkey Island and the East Arctic Islands.  They mostly consume marine mammals focusing on seals, which serve as a critical food source.\n",
      "\n",
      "In contrast, other polar bear species, such as the Polar Bear (Pys) or Polar Wolf (Canis lupus recordeanus) that inhabit Antarctica, have similar diets. \n",
      "\n",
      "Other polar bears, like Polar Otters (Neophryniscus anatin), which are found in coastal regions of the Southern Ocean, consume primarily marine invertebrates, such as marine insect larvae, and fish. While food preferences can vary within these species because of habitat and availability, there is no significant distinction in preferred food sources between polar bears in Alaska and those on Antarctica. Both rely on high-quality, protein-rich food sources specific to their environments.\n",
      "\n",
      "If you're interested in learning more about the different sub-populations of polar bears, I'd suggest looking up further information from scientific studies or wildlife organizations for detailed data. "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: Understood! If you have any further questions, please don't hesitate to ask."
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from typing import Annotated, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph.message import add_messages\n",
    "from functools import partial\n",
    "from colorama import Fore, Style\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "from course_utils import stream_from_app\n",
    "\n",
    "##################################################################\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "def user(state: State):\n",
    "    update = {\"messages\": [(\"user\", interrupt(\"[User]:\"))]}\n",
    "    return Command(update=update, goto=\"agent\")\n",
    "    \n",
    "def agent(state: State, config=None):\n",
    "    if not (state.get(\"messages\") or [\"\"])[-1].content: return {}\n",
    "    update = {\"messages\": [llm.invoke(state.get(\"messages\"), config=config)]}\n",
    "    if \"stop\" in state.get(\"messages\")[-1].content: return update\n",
    "    return Command(update=update, goto=\"start\")\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"start\", lambda state: {})\n",
    "builder.add_node(\"user\", user)\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_edge(START, \"start\")\n",
    "builder.add_edge(\"start\", \"user\")\n",
    "app = builder.compile(checkpointer=MemorySaver())\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "app_stream = partial(app.stream, config=config)\n",
    "\n",
    "for token in stream_from_app(app_stream, verbose=False, debug=False):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf1b37-6cd1-4397-b961-e23e3e526ab4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "In this notebook, we will be combining our simple LangGraph app with the logic of our DLI Instructor prompt from earlier. You may recall that implementation looked something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9f30ec-31db-4d5f-a8ce-5a3aa91d5f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "[Human]: What is the area of our universe in square kilometers?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Agent]: Estimating the area of the observable universe is a vast and nuanced task due to the current limits in our ability to determine the universe's extent. The area of the observable universe, based on the most current measurements, is best expressed in terms of its radius, which is about 46.8 billion light-years. Given that light travels at approximately 300,000 km/s, we can calculate the radius in kilometers and then use the formula for the surface area of a sphere.\n",
      "\n",
      "Given these conditions, the area of the observable universe is about:\n",
      "\n",
      "*Observable Universe Area (Sphere) ≈ 4π * (46.8e9 light-years)^2*\n",
      "\n",
      "However, since 1 light-year ≈ 9.461e12 kilometers, we can convert the radius to kilometers and then compute the area accordingly, leading to a number on the order of approximately 3.87e33 square kilometers for the observable universe's surface area. This is purely an estimation based on the consensus of cosmological measurements. However, it is crucial to note that the size and shape of the entire universe, including potentially regions beyond the observable, remain unknown and subject to significant uncertainties and ongoing research."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "[Human]: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Agent]: Stop. I have provided the information you were looking for regarding the area of the observable universe. Let me know if you have any further questions or need clarification on any of the points."
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "[Human]: \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "from functools import partial\n",
    "\n",
    "## Back-and-forth loop\n",
    "core_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "         \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "         \" Please help to answer user questions about the course. The first message is your context.\"\n",
    "         \" Restart from there, and strongly rely on it as your knowledge base. Do not refer to your 'context' as 'context'.\"\n",
    "    ),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "## Am LCEL chain to pass into chat_with_generator\n",
    "chat_chain = core_prompt | llm | StrOutputParser()\n",
    "\n",
    "with open(\"simple_long_context.txt\", \"r\") as f:\n",
    "    full_context = f.read()\n",
    "\n",
    "long_context_state = {\n",
    "    \"messages\": [],\n",
    "    \"context\": full_context,\n",
    "}\n",
    "\n",
    "from course_utils import chat_with_chain\n",
    "\n",
    "chat = partial(chat_with_chain, chain=chat_chain)\n",
    "chat(long_context_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539022d8-024e-4425-b3aa-e957de6d71fe",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You may also recall that this system was only able to take a couple of questions at a time because the context length would quickly exceed the deployed model's limits. We will try to fix that as part of our exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585fa02-92f3-4414-91c5-8e8de948c0ab",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 2:** Filtering Out The Details\n",
    "\n",
    "Understanding that the context length of our content chatbot is too limited, you may feel included to refine it some more and get down to an even smaller context, but our current entries are already relatively short. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2f48738-68c2-43b6-a0a0-80db0cb2efd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Token Length: 6371 (95.09 * 67)\n",
      "Document Token Range: [42, 148]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "context_entries = full_context.split(\"\\n\\n\")\n",
    "context_docs = [Document(page_content=entry) for entry in context_entries if len(entry.split(\"\\n\")) > 2]\n",
    "context_lens = [token_len(d.page_content) for d in context_docs]\n",
    "print(f\"Context Token Length: {sum(context_lens)} ({sum(context_lens)/len(context_lens):.2f} * {len(context_lens)})\")\n",
    "print(f\"Document Token Range: [{min(context_lens)}, {max(context_lens)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74a39f1-846e-4ada-8a5c-61f9dae38cd7",
   "metadata": {},
   "source": [
    "Perhaps we could invoke some heuristic to help us know which ones to focus on for any given question. Lucky for us, we have several viable heuristics in the form of **embedding models**! These have been covered at length in other courses, so here's just a high-level overview:\n",
    "\n",
    "**Instead of *autoregressing* a sequence out of another sequence as a response/continuation, an encoder *embeds* the sequence into a per-token embedding, of which a subset (zero-th entry, subset, entire sequence) is used as a semantic encoding of the input.** Let's see which model options we have at our displosal.\n",
    "\n",
    "- A **reranking model** orders a set of document pairs by relevance as its default behavior. This style of model is usually implemented with a *cross-encoder*, which takes both sequences as input and directly predicts a relevance score while actively considering both sequences.\n",
    "- An **embedding model** embeds a document into a semantic embedding space as its default behavior. This style of model is usually implemented with a *bi-encoder*, which takes in one sequence at a time to produce the embedding. However, two embedded entries can be compared using some similarity metric (i.e. cosine similarity).\n",
    "\n",
    "Either model could technically be used for retrieval, so let's go ahead and try both options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec639cb0-cb93-4454-9861-c56ce75a6fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.21 ms, sys: 0 ns, total: 4.21 ms\n",
      "Wall time: 333 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='f05440f7-d7c1-413e-bebe-590182b9065a', metadata={}, page_content='Address Complex/Logical Tasks With Conversational AI: Multi-Agent, Multi-Turn Framework From Scratch\\nPresentors: Sagar Desai\\nDescription: Learn to build an agent framework to tackle multi-turn complex tasks and conversational AI interactions with ease. This hands-on course will help you master the skills to create agents that can handle complex conversations and persona-based interactions using a multi-turn framework.'),\n",
       " Document(id='8ea5c698-6dda-4c1b-b195-99f773ff27ce', metadata={}, page_content=\"Getting Started with Langflow: Build a Multi-Agent Shopping Assistant\\nPresentors: Kiyu Gabriel\\nDescription: This interactive training lab will teach you to design and deploy a custom shopping assistant using Langflow's intuitive interface and NVIDIA NeMo Framework. You'll learn to configure agents, customize Retrieval-Augmented Generation (RAG) pipelines, and deploy a multi-agent workflow, gaining hands-on experience in streamlining agent creation.\"),\n",
       " Document(id='7ce33d03-43c9-4f2a-870f-c3dbffcb4da2', metadata={}, page_content=\"Build Visual AI Agents With RAG Using NVIDIA Morpheus, RIVA, and Metropolis\\nPresentors: Adola Adesoba // Dhruv Nandakumar\\nDescription: In this presentation, you'll learn how to create custom visual AI agents using NVIDIA Visual Insights Agent (VIA) microservices and a practical example of agentic Retrieval-Augmented Generation (RAG) pipeline with NVIDIA Morpheus SDK and Riva Speech Services. You'll perform RAG on egocentric video feeds for dynamic, open-world question-answering.\"),\n",
       " Document(id='6237f60e-b0ad-47d5-bb31-cb90f9b9d1ce', metadata={}, page_content=\"Blueprints for Success: Take the Red Pill — Navigating NIM Agent Workflows for Real-World Multimodal Retrieval\\nPresentors: Natalia Segal // Rita Fernandes Neves // Ziv Ilan\\nDescription: In this course, you'll learn how to leverage NVIDIA NIM agent blueprints for multimodal retrieval-augmented generation (RAG) in real-world applications. Through hands-on modules, you'll gain the skills and knowledge needed to implement and optimize multimodal data ingestion, embedding, and retrieval, and guide your AI solution from proof of concept to production.\"),\n",
       " Document(id='a0667e31-4ac1-42e7-9be2-7614ca2f96fc', metadata={}, page_content='Build Next-Gen Agents With Large Vision Language Models\\nPresentors: Abubakr Karali // Debraj Sinha // Sammy Ochoa\\nDescription: This workshop will demystify the challenges of deploying vision-language models (VLMs), teaching participants how to choose the best VLM, train and fine-tune it on a small dataset, and deploy it with NVIDIA Nemo. The workshop will cover topics from VLM fundamentals to hands-on experience with NVIDIA Model Management (NIMs) and Vision Inferencing Accelerator (VIA).')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "## First, we can try out the embedding model, which is commonly used by first constructing a vectorstore.\n",
    "## - Pros: If you have m documents and n queries, you need n inference-time embeddings and m*n similarity comparisons. \n",
    "## - Cons: Prediction of d_i sim q_j uses learned embeddings Emb_D(d_i) and Emb_Q(q_i),\n",
    "##         not a joint learned representation Emb(d_i, q_j). In other words, somewhat less accurate.\n",
    "\n",
    "question = \"Can you tell me about multi-turn agents?\"\n",
    "\n",
    "embed_d = NVIDIAEmbeddings(model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\", base_url='http://llm_client:9000/v1', truncate='END', max_batch_size=128)\n",
    "embed_q = NVIDIAEmbeddings(model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\", base_url='http://llm_client:9000/v1', truncate='END', max_batch_size=128) ## Not necessary\n",
    "vectorstore = FAISS.from_documents(context_docs, embed_d)\n",
    "vectorstore.embedding_function = embed_q\n",
    "retriever = vectorstore.as_retriever()\n",
    "%time retriever.invoke(question, k=5)\n",
    "# %time retriever.invoke(question, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda0f7e3-7ea9-4ddf-be2d-ab189babda8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 8.59 ms, total: 8.59 ms\n",
      "Wall time: 354 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'relevance_score': 2.84375}, page_content='Address Complex/Logical Tasks With Conversational AI: Multi-Agent, Multi-Turn Framework From Scratch\\nPresentors: Sagar Desai\\nDescription: Learn to build an agent framework to tackle multi-turn complex tasks and conversational AI interactions with ease. This hands-on course will help you master the skills to create agents that can handle complex conversations and persona-based interactions using a multi-turn framework.'),\n",
       " Document(metadata={'relevance_score': -12.515625}, page_content=\"Evaluating RAG and Semantic Search Systems\\nPresentors: Amit Bleiweiss\\nDescription: As LLMs and RAG adoption increases in the enterprise, there's a growing need for robust evaluation of these systems to ensure they meet high regulatory standards. The presentation will cover evaluation techniques addressing domain-specific language, evaluation metrics, and independent assessment of retrieval and generation steps, taking temporal information into account.\"),\n",
       " Document(metadata={'relevance_score': -12.515625}, page_content='007 Evaluations for Your Customer Assistant LLM Agent: No Time for Hallucinations\\nPresentors: Dmitry Mironov // Sergio Perez // Ziv Ilan\\nDescription: This presentation focuses on evaluating and optimizing a Customer Assistant LLM agent to ensure it performs accurately and with no \"hallucinations\" in real-world scenarios. You will learn how to leverage the NeMo Evaluator Microservice to create unit tests and track model quality during development, and to optimize accuracy for multilingual deployments.'),\n",
       " Document(metadata={'relevance_score': -12.515625}, page_content=\"Getting Started with Langflow: Build a Multi-Agent Shopping Assistant\\nPresentors: Kiyu Gabriel\\nDescription: This interactive training lab will teach you to design and deploy a custom shopping assistant using Langflow's intuitive interface and NVIDIA NeMo Framework. You'll learn to configure agents, customize Retrieval-Augmented Generation (RAG) pipelines, and deploy a multi-agent workflow, gaining hands-on experience in streamlining agent creation.\"),\n",
       " Document(metadata={'relevance_score': -13.65625}, page_content=\"Blueprints for Success: Take the Red Pill — Navigating NIM Agent Workflows for Real-World Multimodal Retrieval\\nPresentors: Natalia Segal // Rita Fernandes Neves // Ziv Ilan\\nDescription: In this course, you'll learn how to leverage NVIDIA NIM agent blueprints for multimodal retrieval-augmented generation (RAG) in real-world applications. Through hands-on modules, you'll gain the skills and knowledge needed to implement and optimize multimodal data ingestion, embedding, and retrieval, and guide your AI solution from proof of concept to production.\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_nvidia import NVIDIARerank\n",
    "\n",
    "## Next, we can try out the reranking model, which is queried directly to get predicted relevance scores.\n",
    "## - Pros: Literally predicts Emb(d_i, q_i), so better joint relationships can be learned. \n",
    "## - Cons: If you have m documents and n queries, you need n*m inference-time embeddings. \n",
    "\n",
    "question = \"Can you tell me about multi-turn agents?\"\n",
    "\n",
    "reranker = NVIDIARerank(model=\"nvidia/llama-3.2-nv-rerankqa-1b-v2\", base_url='http://llm_client:9000/v1', top_n=5, max_batch_size=128)\n",
    "%time reranker.compress_documents(context_docs, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e35301e-dc2b-4b30-b666-b6ce182b4cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reranker._client.last_inputs\n",
    "# reranker._client.last_response.json()\n",
    "# embed_d._client.last_inputs\n",
    "# embed_d._client.last_response.json()\n",
    "# embed_q._client.last_inputs\n",
    "# embed_q._client.last_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe21a57-f1d0-4b98-8fcc-909d422771a0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As we can see, this process is very fast at identifying similarities and produces pretty good rankings for this small data pool! More generally:\n",
    "- **The reranking model is greatly preferred when we're dealing with a small pool of values,** since it leverage joint conditioning.\n",
    "- **The embedding model is greatly preferred when dealing with large document pools,** since we can offload much of the embedding burden to the preprocessing stage.\n",
    "\n",
    "For our limited use-case, the choice won't really matter, and you are free to use whichever option you find most compelling. With that said, go ahead and define a `retrieve` function to abstract this decision away. Furthermore, to streamline our handling later down the road, let's also return just the final string content so that we have less problems to worry about later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1c9b142-71c6-481a-b8d5-e0e182e659e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Address Complex/Logical Tasks With Conversational AI: Multi-Agent, Multi-Turn Framework From Scratch\\nPresentors: Sagar Desai\\nDescription: Learn to build an agent framework to tackle multi-turn complex tasks and conversational AI interactions with ease. This hands-on course will help you master the skills to create agents that can handle complex conversations and persona-based interactions using a multi-turn framework.',\n",
       " \"Evaluating RAG and Semantic Search Systems\\nPresentors: Amit Bleiweiss\\nDescription: As LLMs and RAG adoption increases in the enterprise, there's a growing need for robust evaluation of these systems to ensure they meet high regulatory standards. The presentation will cover evaluation techniques addressing domain-specific language, evaluation metrics, and independent assessment of retrieval and generation steps, taking temporal information into account.\",\n",
       " '007 Evaluations for Your Customer Assistant LLM Agent: No Time for Hallucinations\\nPresentors: Dmitry Mironov // Sergio Perez // Ziv Ilan\\nDescription: This presentation focuses on evaluating and optimizing a Customer Assistant LLM agent to ensure it performs accurately and with no \"hallucinations\" in real-world scenarios. You will learn how to leverage the NeMo Evaluator Microservice to create unit tests and track model quality during development, and to optimize accuracy for multilingual deployments.',\n",
       " \"Getting Started with Langflow: Build a Multi-Agent Shopping Assistant\\nPresentors: Kiyu Gabriel\\nDescription: This interactive training lab will teach you to design and deploy a custom shopping assistant using Langflow's intuitive interface and NVIDIA NeMo Framework. You'll learn to configure agents, customize Retrieval-Augmented Generation (RAG) pipelines, and deploy a multi-agent workflow, gaining hands-on experience in streamlining agent creation.\",\n",
       " \"Blueprints for Success: Take the Red Pill — Navigating NIM Agent Workflows for Real-World Multimodal Retrieval\\nPresentors: Natalia Segal // Rita Fernandes Neves // Ziv Ilan\\nDescription: In this course, you'll learn how to leverage NVIDIA NIM agent blueprints for multimodal retrieval-augmented generation (RAG) in real-world applications. Through hands-on modules, you'll gain the skills and knowledge needed to implement and optimize multimodal data ingestion, embedding, and retrieval, and guide your AI solution from proof of concept to production.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve_via_query(query: str, k=5):\n",
    "    if not query: return []\n",
    "    reranker = NVIDIARerank(\n",
    "        model=\"nvidia/llama-3.2-nv-rerankqa-1b-v2\", \n",
    "        base_url='http://llm_client:9000/v1', \n",
    "        top_n=k, max_batch_size=128\n",
    "    )\n",
    "    rets = reranker.compress_documents(context_docs, query)\n",
    "    return [entry.page_content for entry in rets]\n",
    "\n",
    "retrieve_via_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a10921d9-a169-47ee-b7b8-91aa4823e287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_nvidia_ai_endpoints.reranking.NVIDIARerank"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NVIDIARerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97acc0-cbab-4c3d-9839-8db74bb355f8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "From here, we can either make it into a \"schema function,\" a \"tool,\" or a \"node,\" with the following key destinctions:\n",
    "- A **schema function** can be bound to an LLM to force the output to the schema unconditionally.\n",
    "- A **tool** is also a schema function, but is defined implicitly (i.e. structure of input is implied from signature) and is easier to toss into a tolbank.\n",
    "- A **node** operates on and writes to the state buffer of a graph, so it should take in a `state` + `config`, operate on state variables, and output a state buffer modification request.\n",
    "\n",
    "In this exercise, we're actually going to use the retrieval as an always-on feature of the \"retrieval\" agent, so we can bypass the first two and directly make our node function. Let's assume:\n",
    "- We want our node to perform a retrieval on the previous message (i.e. we want the retrieval to happen AFTER the user submits a message, and we want to retrieve based on whatever the user sent).\n",
    "- We want to write the retrieval results to a value `context` to the state buffer, as we will want the next node (an LLM generation) to use the context.\n",
    "    - And we will want to accumulate `context` over time to include all relevant retrievals. That way, we can put the retrieval into the system message and it can continue to contribute to all subsequent outputs. This implies we'll want to store the values in a set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7c3ab06-dd5d-4ce5-9225-32227e293299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever: {'Address Complex/Logical Tasks With Conversational AI: Multi-Agent, Multi-Turn Framework From Scratch\\nPresentors: Sagar Desai\\nDescription: Learn to build an agent framework to tackle multi-turn complex tasks and conversational AI interactions with ease. This hands-on course will help you master the skills to create agents that can handle complex conversations and persona-based interactions using a multi-turn framework.', \"Getting Started with Langflow: Build a Multi-Agent Shopping Assistant\\nPresentors: Kiyu Gabriel\\nDescription: This interactive training lab will teach you to design and deploy a custom shopping assistant using Langflow's intuitive interface and NVIDIA NeMo Framework. You'll learn to configure agents, customize Retrieval-Augmented Generation (RAG) pipelines, and deploy a multi-agent workflow, gaining hands-on experience in streamlining agent creation.\", 'Automate 5G Network Configurations With NVIDIA AI LLM Agents and Kinetica Accelerated Database\\nPresentors: Amparo Canaveras // Swastika Dutta\\nDescription: This presentation will show how to create AI agents using NVIDIA NIM and LangGraph to automate 5G network configurations. The agents will monitor network quality of service and respond to congestion by creating new network slices. AI will be integrated into a simulated 5G environment using the Open Air Interface 5G lab and LangGraph with Python.'} (3)\n",
      "\n",
      "Context: {'Address Complex/Logical Tasks With Conversational AI: Multi-Agent, Multi-Turn Framework From Scratch\\nPresentors: Sagar Desai\\nDescription: Learn to build an agent framework to tackle multi-turn complex tasks and conversational AI interactions with ease. This hands-on course will help you master the skills to create agents that can handle complex conversations and persona-based interactions using a multi-turn framework.', 'Automate 5G Network Configurations With NVIDIA AI LLM Agents and Kinetica Accelerated Database\\nPresentors: Amparo Canaveras // Swastika Dutta\\nDescription: This presentation will show how to create AI agents using NVIDIA NIM and LangGraph to automate 5G network configurations. The agents will monitor network quality of service and respond to congestion by creating new network slices. AI will be integrated into a simulated 5G environment using the Open Air Interface 5G lab and LangGraph with Python.', 'Accelerate Physical AI Development Workflows with Omniverse Cloud Sensor RTX\\nPresentors: Justine Lin // Daniel Lindsey\\nDescription: This hands-on lab introduces developers to NVIDIA Omniverse Cloud Sensor RTX, a set of APIs for physically accurate sensor simulation. The lab is ideal for robotics engineers, autonomous vehicle developers, and AI specialists, offering practical experience in leveraging Sensor RTX APIs to accelerate autonomous system development. This lab aims to accelerate the creation and testing of autonomous systems.', \"Applying AI Weather Models With NVIDIA Earth-2\\nPresentors: Georg Ertl // Jussi Leinonen // Stefan Weissenberger\\nDescription: This presentation explores how NVIDIA Earth-2 facilitates efficient weather and climate modeling. It covers the use of a growing stack of global AI weather forecasting models and the application of downscaling models to generate high-resolution outputs. We'll discover the key use cases and applications that benefit from this emerging technology.\", \"Getting Started with Langflow: Build a Multi-Agent Shopping Assistant\\nPresentors: Kiyu Gabriel\\nDescription: This interactive training lab will teach you to design and deploy a custom shopping assistant using Langflow's intuitive interface and NVIDIA NeMo Framework. You'll learn to configure agents, customize Retrieval-Augmented Generation (RAG) pipelines, and deploy a multi-agent workflow, gaining hands-on experience in streamlining agent creation.\", 'Build Your First AI Robotic Arm With OpenVLA and Isaac Sim\\nPresentors: Abubakr Karali // Maycon da Silva Carvalho // Teresa Conceicao\\nDescription: This hands-on workshop introduces creating a simulated robotic arm using OpenVLA and NVIDIA Isaac Sim. Participants will learn to set up and integrate these tools, fine-tune pre-trained VLA models for specific tasks, and control a simulated robotic arm using IsaacSim. This foundation enables the construction and adaptation of robotic applications in a simulated environment, reducing the need for physical hardware.'} (6)\n",
      "\n",
      "Context: {'Address Complex/Logical Tasks With Conversational AI: Multi-Agent, Multi-Turn Framework From Scratch\\nPresentors: Sagar Desai\\nDescription: Learn to build an agent framework to tackle multi-turn complex tasks and conversational AI interactions with ease. This hands-on course will help you master the skills to create agents that can handle complex conversations and persona-based interactions using a multi-turn framework.', 'Automate 5G Network Configurations With NVIDIA AI LLM Agents and Kinetica Accelerated Database\\nPresentors: Amparo Canaveras // Swastika Dutta\\nDescription: This presentation will show how to create AI agents using NVIDIA NIM and LangGraph to automate 5G network configurations. The agents will monitor network quality of service and respond to congestion by creating new network slices. AI will be integrated into a simulated 5G environment using the Open Air Interface 5G lab and LangGraph with Python.', 'Build Your First AI Robotic Arm With OpenVLA and Isaac Sim\\nPresentors: Abubakr Karali // Maycon da Silva Carvalho // Teresa Conceicao\\nDescription: This hands-on workshop introduces creating a simulated robotic arm using OpenVLA and NVIDIA Isaac Sim. Participants will learn to set up and integrate these tools, fine-tune pre-trained VLA models for specific tasks, and control a simulated robotic arm using IsaacSim. This foundation enables the construction and adaptation of robotic applications in a simulated environment, reducing the need for physical hardware.', \"Getting Started with Langflow: Build a Multi-Agent Shopping Assistant\\nPresentors: Kiyu Gabriel\\nDescription: This interactive training lab will teach you to design and deploy a custom shopping assistant using Langflow's intuitive interface and NVIDIA NeMo Framework. You'll learn to configure agents, customize Retrieval-Augmented Generation (RAG) pipelines, and deploy a multi-agent workflow, gaining hands-on experience in streamlining agent creation.\", \"Learn to Build Agentic AI Workflows for Enterprise Applications\\nPresentors: Dhruv Nandakumar // Matt Penn\\nDescription: In this class, you'll learn to build configurable AI workflows and tools using NVIDIA technologies for enterprise applications. You'll deploy an agentic AI workflow, create tools for AI agents, and augment existing workflows with new tools to increase productivity.\", \"Applying AI Weather Models With NVIDIA Earth-2\\nPresentors: Georg Ertl // Jussi Leinonen // Stefan Weissenberger\\nDescription: This presentation explores how NVIDIA Earth-2 facilitates efficient weather and climate modeling. It covers the use of a growing stack of global AI weather forecasting models and the application of downscaling models to generate high-resolution outputs. We'll discover the key use cases and applications that benefit from this emerging technology.\", \"Build Visual AI Agents With RAG Using NVIDIA Morpheus, RIVA, and Metropolis\\nPresentors: Adola Adesoba // Dhruv Nandakumar\\nDescription: In this presentation, you'll learn how to create custom visual AI agents using NVIDIA Visual Insights Agent (VIA) microservices and a practical example of agentic Retrieval-Augmented Generation (RAG) pipeline with NVIDIA Morpheus SDK and Riva Speech Services. You'll perform RAG on egocentric video feeds for dynamic, open-world question-answering.\", 'Accelerate Physical AI Development Workflows with Omniverse Cloud Sensor RTX\\nPresentors: Justine Lin // Daniel Lindsey\\nDescription: This hands-on lab introduces developers to NVIDIA Omniverse Cloud Sensor RTX, a set of APIs for physically accurate sensor simulation. The lab is ideal for robotics engineers, autonomous vehicle developers, and AI specialists, offering practical experience in leveraging Sensor RTX APIs to accelerate autonomous system development. This lab aims to accelerate the creation and testing of autonomous systems.'} (8)\n"
     ]
    }
   ],
   "source": [
    "def retrieval_node(state: State, config=None, out_key=\"context\"):\n",
    "    ## NOTE: Very Naive; Assumes user question is a good query\n",
    "    ret = retrieve_via_query(state.get(\"messages\")[-1].content, k=3)\n",
    "    return {out_key: set(ret)}\n",
    "\n",
    "## After we define the node, we can assess whether or not it would work.\n",
    "\n",
    "## Given an initial empty state...\n",
    "state = {\n",
    "    \"messages\": [], \n",
    "    \"context\": set(),\n",
    "}\n",
    "\n",
    "## Given an update rule explaining how to handle state updates...\n",
    "add_sets = (lambda x,y: x.union(y))\n",
    "\n",
    "## Will the continued accumulation of messages, followed by a continued accumulation of retrievals, function properly?\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [(\"user\", \"Can you tell me about agents?\")])\n",
    "state[\"context\"] = add_sets(state[\"context\"],  retrieval_node(state)[\"context\"])\n",
    "print(f\"Retriever: {state['context']} ({len(state['context'])})\")\n",
    "\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [(\"user\", \"How about earth simulations?\")])\n",
    "state[\"context\"] = add_sets(state[\"context\"],  retrieval_node(state)[\"context\"])\n",
    "print(f\"\\nContext: {state['context']} ({len(state['context'])})\")\n",
    "\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [(\"user\", \"How about earthly agents?\")])\n",
    "state[\"context\"] = add_sets(state[\"context\"],  retrieval_node(state)[\"context\"])\n",
    "print(f\"\\nContext: {state['context']} ({len(state['context'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf4d2a-662f-4981-a094-57ae5c16fe5b",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 3:** Adding Retrieval To Our Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c9a47-85f5-4184-9be5-fd43ef0fe032",
   "metadata": {},
   "source": [
    "Now that we have a generally-applicable node with some base assumptions, let's integrate it into our dialog loop from earlier and see if it just works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77843092-fc7d-4dc7-a107-e3aeaa2d60f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]: Tell me about Nvidia weather forecasting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: Certainly! NVIDIA's weather forecasting capabilities are part of their broader applications in the field of Artificial Intelligence, particularly leveraging their up-to-date AI engine and CUDA technologies. While NVIDIA weather forecasting is not as commonly mentioned as other AI solutions like self-driving cars or computer vision, it can be inferred from such presentations as the one provided earlier:\n",
      "\n",
      "*   **Georg Ertl and Jussi Leinonen** may have spoken about NVIDIA's weather and climate modeling applications. Here's how: La Jolla Superscale is one of the main hosted GPU compute clusters at the US National Science Foundation's NPAC, which is more known for its HPC and AI research. However, from the presentations mentioned, one of the other key instances is that of the Jorn Weather Forecasting Group from Finland, JJFY. They were part of the stack of 15 weather forecasting models that have been developed at NPAC.\n",
      "\n",
      "*   **Stefan Weissenberger** also spoke to the audience about these AI weather forecasting models at the NPAC. These models essentially simulate the Earth's climate and atmosphere. They represent advancements in utilizing large-scale AI to simulate such natural systems, enabling the development of short-term forecasting tools and climatology over various geographical scales.\n",
      "\n",
      "These models and associated applications would be preaching to installation, operation, and benefits for areas such as agriculture and climate models. The technologies help merge AI with environmental science, providing insights into seasonal patterns and environmental changes, thus offering direct benefits such as drought Warning system. Therefore, NVIDIA's weather forecasting is essentially leveraging the power of AI to simulate the world's environmental systems, which allows for more predictable and accurate climate modeling.  This helps predict changes in weather patterns before they happen, which is crucial for farmers to protect their crops and for engineers to build structures that can withstand extreme weather conditions. More broadly, this represents an AI application that depends on environmental models and services. Also, in these forecasts, Kronos can integrate to GPUs to bring predictions to requires less resource usage comparable to traditional forecasting systems. By leveraging GPU power, AI-driven forecasting is growing more efficient and computationally responsive. \n",
      "\n",
      "However, it's less directly discussed than other AI applications by NVIDIA but is generally tied into the organization's larger efforts in AI and sharp-end research. For more detailed information or specific project insights, consulting the source presentations mentioned, as well as research papers, would be more informative. Furthermore, given that we are only speaking about consideration specs from the presentations, a more temperate look at the same presentations would result as what each of the other presentations predominantly spoke about (i.e., sensor processing pipelines and rapid Spark acceleration with RAPIDS) becoming needed that reacts to the following people’s presentations, since a forum rarely mentions about multiple related topics, and only reports information from the presentations.\n",
      "\n",
      "It's important to note that specific research papers or official NVIDIA documentation might provide the most current and detailed information on their weather forecasting applications. "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]: How important are GPUs for that comparing to CPUs only?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: GPU (Graphics Processing Unit) technology is significantly more effective than traditional CPUs (Central Processing Units) in various types of real-time and data-heavy applications. Here are some key differences that make GPUs more suitable than CPUs for tasks typically requiring AI, computer vision, and weather forecasting, such as the document you provided:\n",
      "\n",
      "1. **High Parallelism**: GPUs excel at performing massive parallel computations, allowing them to process numerous operations simultaneously. This parallelism is crucial for tasks that involve complex and volumetric computations, such as weather modeling, which often involve statistical analysis of vast datasets over large spatial and temporal scales. CPUs, on the other hand, are limited by their single-threaded architecture, which limits their parallel processing capabilities.\n",
      "\n",
      "2. **CUDA (Compute Unified Data Graphics Architecture)**: NVIDIA's proprietary CUDA architecture aims to efficiently utilize GPU processing power. Cuda optimizes performance by assigning tasks to thousands of massively parallel threads (threads) in a highly optimized function. These can collectively process accurate predictions faster than traditional CPUs, especially for use cases like weather forecasting, where rigorous and real-time computation is essential.\n",
      "\n",
      "3. **Stronger Demand on Memory**: GPUs typically handle significantly larger amounts of data and can reuse data with much less memory cost. Significant real-time data processing and storage demands such as in weather foll ast ensures the use of powerful memory for deep learning models. Datasets generated by environmental science will logically require massive amounts of storage and processing, beneficial to GPU acceleration.\n",
      "\n",
      "4. **Extended Single Resource Locality - ISRL** (Access to the system's processor and memory): GPUs have a larger memory access bandwidth compared to CPUs, and in many Irisl scenarios, the GPU can directly access system memory, which can maintain better cache coherence and lower latency. This results in faster communication between memory, CPU, and GPU, making them ideal for real-time computations.\n",
      "\n",
      "5. **Virtualization**: Distributed computing systems using GPUs offer improved digital realism, precision and powerWake, reducing resource overhead by efficiently dividing the workload from CPU, leading more productive and responsive real-time services.\n",
      "\n",
      "6. **Maintenance and Scaling**: Even with the potential drawbacks such as increased power consumption, GPUs are ever-shrinking and more energy efficient than older CPUs due to improving GPU transistor technology, making them suitable for aims such as faster response times in real-time forecasting, with new processors being relatively cheaper per USE. \n",
      "\n",
      "In summary, for tasks requiring significant computational power such as weather forecasting and AI, GPUs are your Instrumental choice. They excel in theoreticali challenge such as simulating complex environments so find themselves more powerful for such - long-term Effects.\n",
      "\n",
      "7. **Efficient Memory management**: GPUs typically allocate and reuse memory, limiting program verb memory usage, efficient use of resources over extended computing areas. Processing requirements can be particularly intensive in processes which require sequential analysis, making GPU processing more powerful.\n",
      "\n",
      "Overall, the question of whether GPUs are more important than CPUs for weather forecasting and AI workloads is more nuanced. However, the wide use of GPUs in these areas, when they are designed to accelerate specific aspects of an application, such as processing mathematical models, demonstrate the potential for their benefits. If the question were specifically in comparing GPUs and CPUs for the same application, then in general scenarios like this, the GPU is expected to handle simulations and large-scale data processing, which in weather, could be superior by factor many in supported features such as GPUs’ more efficient parallel processing and the use of CUDA architecture promoting collective performance. However, the alternative methods of a virtual environment with several CPUs which may be powerful is another direct option. The answer could best be given based on specific research and in practice scenarios aiming them is to be used with GPU rather further supports.\n",
      "\n",
      "Together. \n",
      "\n",
      "To end the summary, GPUs are better for factors of parallelism and eye the specialized processing capabilities. Thennout put this together, the first principle is that GPUs' greater parallelism and specialized processing make them more suitable for computationally intensive tasks such as weather forecasting and AI workloads, compared to CPUs. However, even in these scenarios, a combination of processors, CPUs, and GPUs may be used. The choice between them depends on the specific requirements and constraints of the application. For large scale computational workloads necessitating heavy statistical modeling (as in weather forecasting), GPUs due to their ability to handle parallel computations would give a significant edge in computational time and accuracy. Ultimately, the decision also hinges on hardware solutions and execution plans consistent with practices in peer research literature and pragmatic network deployments utilizing these GPUs to accelerate insights from modeling climate. That said a founder-in-programming considering software versions— impressed by just getting things right in a little time, and seeking a boost for weapon now you have a learning curve towards learning it as a prof. \n",
      "\n",
      "As for I differences, in algorithms and accelerator architecture. GPUs can suggest this, but the selection in practice refers to real-world applications, the core reason that GPUs are preferred over CPUs for certain tasks is their multi-threaded architecture and per Bloomberg whereas is the workhorse in Natural Language Processing, the consumers of GPUs are all-around in all sorts of of"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]: \n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from typing import Annotated, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph.message import add_messages\n",
    "from functools import partial\n",
    "from colorama import Fore, Style\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "\n",
    "##################################################################\n",
    "## Define the authoritative state system (environment) for your use-case\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"The Graph State for your Agent System\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    context: Annotated[set, (lambda x,y: x.union(y))]\n",
    "\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "         \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "         \" Please help to answer user questions about the course. The first message is your context.\"\n",
    "         \" Restart from there, and strongly rely on it as your knowledge base. Do not refer to your 'context' as 'context'.\"\n",
    "    ),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", \"Thank you. I will not restart the conversation and will abide by the context.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "def user(state: State):\n",
    "    update = {\"messages\": [(\"user\", interrupt(\"[User]:\"))]}\n",
    "    return Command(update=update, goto=\"retrieval_router\")\n",
    "\n",
    "## TODO: Add the retrieval between user and agent\n",
    "def retrieval_router(state: State):\n",
    "    # Run retrieval based on the latest user message and write results into state[\"context\"]\n",
    "    # retrieval_node(...) is assumed to return something like {\"context\": set([...])}\n",
    "    return Command(update=retrieval_node(state), goto=\"agent\")\n",
    "\n",
    "def agent(state: State, config=None):\n",
    "    if not (state.get(\"messages\") or [\"\"])[-1].content:\n",
    "        return {}\n",
    "    update = {\"messages\": [(agent_prompt | llm).invoke(state, config=config)]}\n",
    "    if \"stop\" in state.get(\"messages\")[-1].content:\n",
    "        return update\n",
    "    return Command(update=update, goto=\"start\")\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"start\", lambda state: {})\n",
    "builder.add_node(\"user\", user)\n",
    "## TODO: Register the new router to the nodepool\n",
    "builder.add_node(\"retrieval_router\", retrieval_router)\n",
    "\n",
    "builder.add_node(\"agent\", agent)\n",
    "builder.add_edge(START, \"start\")\n",
    "builder.add_edge(\"start\", \"user\")\n",
    "app = builder.compile(checkpointer=MemorySaver())\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "app_stream = partial(app.stream, config=config)\n",
    "\n",
    "for token in stream_from_app(app_stream, verbose=False, debug=False):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730bf232-3ca9-437a-8d6d-0c49d2eeab74",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "As you can see, it wasn't too hard to integrate with the way this graph system is defined. We now have an \"always-on\" retrieval system which is always going to naively take our last message and retrieve the most relevant resources for our query... allegedly. However, if you play around with it a bit, you should start to notice that the raw input may not be exactly optimal, so most setups like to first rephrase the input into the canonical input form for the embedding model... but that would introduce latency and increase the time-to-first-token, making our system less responsive. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22d87c-8346-4986-bdf4-00322e6d63cb",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Part 4:** Adding A \"Think Deeper\" Mechanism\n",
    "\n",
    "In this section, we're going to take some minor inspiration from ReAct to give our system multiple levels of thoroughness. Since our retrieval above was so light and is probably sufficient for most use-cases, let's keep it in. However, let's add a more rigorous thinking process that forces both a **query refinement** and a **web search** as part of its execution. \n",
    "\n",
    "This type of mechanism is often called a \"reflection\" mechanism since it can evaluate the output of the LLM and try to correct the execution flow. It largely works from the intuition that it's easier to verify if an output looks good than to generate the output in the first place.\n",
    "\n",
    "We can achieve the querying logic with a single structured output schema, which we can test out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "133b91f3-e2f7-4c1c-be49-82cc59d7dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: [Queries(big_questions=['What are agents?'], semantic_queries=['Can you explain agents in the context of this conversation?', 'Are agents used for a specific purpose?'], web_search_queries=['Agents | Google'])]\n",
      "\n",
      "Queries: [Queries(big_questions=['What are agents?'], semantic_queries=['Can you explain agents in the context of this conversation?', 'Are agents used for a specific purpose?'], web_search_queries=['Agents | Google']), Queries(big_questions=['What are agents?', 'Can you discuss earth simulations?', 'Could you share more details about an earth simulation? Aiding specific examples if possible.'], semantic_queries=['An agent is a perspective-taking entity, in this context likely referring to intelligent being with intentions whether or not they are humans, related to the topic of the discussion.', 'How can earth simulations help in understanding global climate impacts?', 'Are there specific types of agents considered in earth simulation modeling, like economic or environmental agents?'], web_search_queries=['agents definition in philosophy', 'earth simulations climate impact research', 'earth simulation models with agent variables'])]\n"
     ]
    }
   ],
   "source": [
    "from course_utils import SCHEMA_HINT\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    \"\"\"Queries to help you research across semantic and web resources for more information. Specifically focus on the most recent question.\"\"\"\n",
    "    big_questions: List[str] = Field(description=\"Outstanding questions that need research, in natural language\")\n",
    "    semantic_queries: List[str] = Field(description=\"Questions (3 or more) to ask an expert to get more info to help, expressed in different ways.\")\n",
    "    web_search_queries: List[str] = Field(description=\"Questions (3 or more) that will be sent over to a web-based search engine to gather info.\")\n",
    "\n",
    "def query_node(state: State):\n",
    "    if not state.get(\"messages\"): return {\"queries\": []}\n",
    "    chat_msgs = [\n",
    "        (\"system\", SCHEMA_HINT.format(schema_hint = Queries.model_json_schema())),\n",
    "        (\"user\", \"Corrent Conversation:\\n\" + \"\\n\\n\".join([f\"[{msg.type}] {msg.content}\" for msg in state.get(\"messages\")])),\n",
    "    ]\n",
    "    schema_llm = llm.with_structured_output(schema=Queries.model_json_schema(), strict=True)\n",
    "    response = Queries(**schema_llm.invoke(chat_msgs))\n",
    "    return {\"queries\": [response]}\n",
    "\n",
    "add_queries = (lambda l,x: l+x) \n",
    "\n",
    "state = {\n",
    "    \"messages\": [], \n",
    "    \"queries\": [],\n",
    "}\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [(\"user\", \"Can you tell me about agents?\")])\n",
    "state[\"queries\"] = add_queries(state[\"queries\"],  query_node(state)[\"queries\"])\n",
    "print(\"Queries:\", state[\"queries\"])\n",
    "\n",
    "state[\"messages\"] = add_messages(state[\"messages\"], [(\"user\", \"How about earth simulations?\")])\n",
    "state[\"queries\"] = add_queries(state[\"queries\"],  query_node(state)[\"queries\"])\n",
    "print(\"\\nQueries:\", state[\"queries\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87bb72c-91bc-49bf-965a-349e2498d8a1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, we have to actually fulfill those requests, so let's bring in both our retrieval function from this notebook and our DDGS search tool from the previous notebook and actually fulfill those requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13668d0b-de54-4379-8bdd-897a7e806125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 14 chunks from the internet and the knowledge base\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'007 Evaluations for Your Customer Assistant LLM Agent: No Time for Hallucinations\\nPresentors: Dmitry Mironov // Sergio Perez // Ziv Ilan\\nDescription: This presentation focuses on evaluating and optimizing a Customer Assistant LLM agent to ensure it performs accurately and with no \"hallucinations\" in real-world scenarios. You will learn how to leverage the NeMo Evaluator Microservice to create unit tests and track model quality during development, and to optimize accuracy for multilingual deployments.',\n",
       " \"12 hours ago - In this guide on how to build AI agents using Google Gemini, you’ll learn why Google’s Gemini models are a practical choice for agent workflows, and how to go from first prompt to a working loop you can test and ship. [Snippet found from 'How to Build AI Agents Using Google Gemini (Step-by-Step Guide)' (https://clickup.com/blog/how-to-build-ai-agents-using-google-gemini/)]\",\n",
       " 'Address Complex/Logical Tasks With Conversational AI: Multi-Agent, Multi-Turn Framework From Scratch\\nPresentors: Sagar Desai\\nDescription: Learn to build an agent framework to tackle multi-turn complex tasks and conversational AI interactions with ease. This hands-on course will help you master the skills to create agents that can handle complex conversations and persona-based interactions using a multi-turn framework.',\n",
       " \"An agent is a person who is empowered to act on behalf of another. Read about different agent types, such as real estate, insurance, and business agents . [Snippet found from 'What Is an Agent? Definition, Types of Agents, and Examples' (https://www.investopedia.com/terms/a/agent.asp)]\",\n",
       " 'Automate 5G Network Configurations With NVIDIA AI LLM Agents and Kinetica Accelerated Database\\nPresentors: Amparo Canaveras // Swastika Dutta\\nDescription: This presentation will show how to create AI agents using NVIDIA NIM and LangGraph to automate 5G network configurations. The agents will monitor network quality of service and respond to congestion by creating new network slices. AI will be integrated into a simulated 5G environment using the Open Air Interface 5G lab and LangGraph with Python.',\n",
       " \"Build Visual AI Agents With RAG Using NVIDIA Morpheus, RIVA, and Metropolis\\nPresentors: Adola Adesoba // Dhruv Nandakumar\\nDescription: In this presentation, you'll learn how to create custom visual AI agents using NVIDIA Visual Insights Agent (VIA) microservices and a practical example of agentic Retrieval-Augmented Generation (RAG) pipeline with NVIDIA Morpheus SDK and Riva Speech Services. You'll perform RAG on egocentric video feeds for dynamic, open-world question-answering.\",\n",
       " \"December 4, 2025 - Design, manage, and share AI agents in Google Workspace Studio to automate tasks and complex workflows using Gemini 3. [Snippet found from 'Introducing Google Workspace Studio to automate everyday work with AI agents | Google Workspace Blog' (https://workspace.google.com/blog/product-announcements/introducing-google-workspace-studio-agents-for-everyday-work)]\",\n",
       " \"Define agents . agents synonyms, agents pronunciation, agents translation, English dictionary definition of agents . n. 1. One that acts or has the power or authority to act. 2. One empowered to act for or represent another: an author's agent ; an insurance agent . 3. [Snippet found from 'Agents - definition of agents by The Free Dictionary' (https://www.thefreedictionary.com/agents)]\",\n",
       " \"Discover pre-built and custom AI agents for industry and business use cases, validated by Google Cloud. [Snippet found from 'AI agent finder | Google Cloud' (https://cloud.withgoogle.com/agentfinder/)]\",\n",
       " \"Discover, create, share, and run AI agents all in one secure platform, bringing the best of Google AI to every employee, for every workflow. [Snippet found from 'Gemini Enterprise: Best of Google AI for Business | Google Cloud' (https://cloud.google.com/gemini-enterprise)]\",\n",
       " \"Dive into the world of agents , from their legal foundations to their diverse roles in business, sports, and real estate. Explore the responsibilities, types, and significance of agents in representing and safeguarding the interests of their principals in various domains. [Snippet found from 'Understanding Agents: Roles, Types & Implications - Tickeron' (https://tickeron.com/trading-investing-101/what-is-an-agent-and-how-do-they-operate-across-different-domains/)]\",\n",
       " \"Getting Started with Langflow: Build a Multi-Agent Shopping Assistant\\nPresentors: Kiyu Gabriel\\nDescription: This interactive training lab will teach you to design and deploy a custom shopping assistant using Langflow's intuitive interface and NVIDIA NeMo Framework. You'll learn to configure agents, customize Retrieval-Augmented Generation (RAG) pipelines, and deploy a multi-agent workflow, gaining hands-on experience in streamlining agent creation.\",\n",
       " \"Introduction to Agents An agent is a person or entity authorized to act on behalf of a principal, creating a fiduciary relationship built on trust and responsibility. Through agency law, an attorney, stockbroker, real estate agent , or even an insurance agent can carry out important real estate transactions, financial decisions, or negotiations. [Snippet found from 'What Is an Agent? Definition, Types, Responsibilities, and Future' (https://devpumas.com/agents/)]\",\n",
       " \"Learn to Build Agentic AI Workflows for Enterprise Applications\\nPresentors: Dhruv Nandakumar // Matt Penn\\nDescription: In this class, you'll learn to build configurable AI workflows and tools using NVIDIA technologies for enterprise applications. You'll deploy an agentic AI workflow, create tools for AI agents, and augment existing workflows with new tools to increase productivity.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## HINT: You can paste the retrieval node and search tools directly and just resolve them in fulfill_query\n",
    "\n",
    "# from langchain.tools import tool\n",
    "# \n",
    "# @tool\n",
    "# def search_internet(user_question: List[str], context: List[str], final_query: str):\n",
    "#     \"\"\"Search the internet for answers. Powered by search engine, in Google search format.\"\"\"\n",
    "#     from ddgs import DDGS\n",
    "#     return DDGS().text(final_query, max_results=10)\n",
    "\n",
    "# def retrieval_node(state: State, config=None, out_key=\"context\"):\n",
    "#     ## NOTE: Very Naive; Assumes user question is a good query\n",
    "#     ret = retrieve_via_query(get_nth_message(state, n=-1), k=3)\n",
    "#     return {out_key: set(ret)}\n",
    "\n",
    "def fulfill_queries(queries: Queries, verbose=False):\n",
    "    # big_questions: List[str]\n",
    "    # semantic_queries: List[str]\n",
    "    # web_search_queries: List[str]\n",
    "    from ddgs import DDGS\n",
    "    web_queries = queries.web_search_queries + queries.big_questions\n",
    "    sem_queries = queries.semantic_queries + queries.big_questions\n",
    "    # if verbose: print(f\"Querying for retrievals via {web_queries = } and {sem_queries = }\")\n",
    "    web_ret_fn = lambda q: [\n",
    "        str(f\"{v.get('body')} [Snippet found from '{v.get('title')}' ({v.get('href')})]\") \n",
    "        for v in DDGS().text(q, max_results=4)\n",
    "    ]\n",
    "    sem_ret_fn = retrieve_via_query\n",
    "    web_retrievals = [web_ret_fn(web_query) for web_query in web_queries]\n",
    "    sem_retrievals = [sem_ret_fn(sem_query) for sem_query in sem_queries]\n",
    "    # if verbose: print(f\"Generated retrievals: {web_retrievals = } and {sem_retrievals = }\")\n",
    "    return set(sum(web_retrievals + sem_retrievals, []))\n",
    "\n",
    "retrievals = set()\n",
    "new_rets = fulfill_queries(state[\"queries\"][0], verbose=True)\n",
    "retrievals = retrievals.union(new_rets)\n",
    "print(f\"Retrieved {len(new_rets)} chunks from the internet and the knowledge base\")\n",
    "new_rets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45c975e-4973-420c-aa17-f7899055d0ff",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And perfect! We now have an unusably-long context that is, admittedly, better-thought-out but intractably long. Lucky for us, we have a pretty streamlined way of subsetting this with our retriever system, if only we generalize it a bit more.\n",
    "\n",
    "In the following cell, please implement a `format_retrieval` function to create the actual context for the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "708ee723-1939-4970-927d-daf7f2608751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Define agents . agents synonyms, agents pronunciation, agents translation, English dictionary definition of agents . n. 1. One that acts or has the power or authority to act. 2. One empowered to act for or represent another: an author's agent ; an insurance agent . 3. [Snippet found from 'Agents - definition of agents by The Free Dictionary' (https://www.thefreedictionary.com/agents)]\",\n",
       " \"Introduction to Agents An agent is a person or entity authorized to act on behalf of a principal, creating a fiduciary relationship built on trust and responsibility. Through agency law, an attorney, stockbroker, real estate agent , or even an insurance agent can carry out important real estate transactions, financial decisions, or negotiations. [Snippet found from 'What Is an Agent? Definition, Types, Responsibilities, and Future' (https://devpumas.com/agents/)]\",\n",
       " \"An agent is a person who is empowered to act on behalf of another. Read about different agent types, such as real estate, insurance, and business agents . [Snippet found from 'What Is an Agent? Definition, Types of Agents, and Examples' (https://www.investopedia.com/terms/a/agent.asp)]\",\n",
       " \"Dive into the world of agents , from their legal foundations to their diverse roles in business, sports, and real estate. Explore the responsibilities, types, and significance of agents in representing and safeguarding the interests of their principals in various domains. [Snippet found from 'Understanding Agents: Roles, Types & Implications - Tickeron' (https://tickeron.com/trading-investing-101/what-is-an-agent-and-how-do-they-operate-across-different-domains/)]\",\n",
       " \"December 4, 2025 - Design, manage, and share AI agents in Google Workspace Studio to automate tasks and complex workflows using Gemini 3. [Snippet found from 'Introducing Google Workspace Studio to automate everyday work with AI agents | Google Workspace Blog' (https://workspace.google.com/blog/product-announcements/introducing-google-workspace-studio-agents-for-everyday-work)]\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_retrieval(\n",
    "    queries: Queries, \n",
    "    new_retrievals: list[str], \n",
    "    existing_retrievals: set[str] = set(), \n",
    "    k=5\n",
    "):\n",
    "    # big_questions: List[str]\n",
    "    # semantic_queries: List[str]\n",
    "    # web_search_queries: List[str]\n",
    "    reranker = NVIDIARerank(model=\"nvidia/llama-3.2-nv-rerankqa-1b-v2\", base_url='http://llm_client:9000/v1', top_n=(k + len(existing_retrievals)), max_batch_size=128)\n",
    "    docs = [Document(page_content = ret) for ret in new_retrievals]\n",
    "    rets = reranker.compress_documents(docs, \"\\n\".join(queries.big_questions))\n",
    "    return [entry.page_content for entry in rets if entry.page_content not in existing_retrievals][:k]\n",
    "\n",
    "filtered_retrieval = filter_retrieval(state[\"queries\"][0], new_rets)\n",
    "filtered_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ae15b39-35e0-4567-9016-f78eb7146d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_retrieval(chunks: list[str], header: str = \"Relevant materials:\") -> str:\n",
    "    if not chunks:\n",
    "        return \"\"\n",
    "    lines = [header, \"\"]\n",
    "    for i, chunk in enumerate(chunks, start=1):\n",
    "        cleaned = (chunk or \"\").strip()\n",
    "        if not cleaned:\n",
    "            continue\n",
    "        lines.append(f\"[{i}] {cleaned}\")\n",
    "        lines.append(\"\")  # spacer\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "filtered = filter_retrieval(state[\"queries\"][0], new_rets, existing_retrievals=state.get(\"context\", set()), k=5)\n",
    "context_str = format_retrieval(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf400e02-6378-4272-976e-8018e7afb112",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "And to wrap all of that up, proceed to make a single unified node call that executes this process as part of the routine, preferably without ever writing to the state buffer until the final new retrievals are generated.\n",
    "\n",
    "**We will leave the final combination as an exercise, but the solution is provided for those interested.** After all, this should be prep for the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8c7f6-e031-4485-a230-35e3d815997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]: Tell me about Nvidia Nemotron\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from typing import Annotated, Optional\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command, Send\n",
    "from langgraph.graph.message import add_messages\n",
    "from functools import partial\n",
    "from colorama import Fore, Style\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "\n",
    "##################################################################\n",
    "## Define the authoritative state system (environment) for your use-case\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"The Graph State for your Agent System\"\"\"\n",
    "    messages: Annotated[list, add_messages]\n",
    "    context: Annotated[set, (lambda x,y: x.union(y))]\n",
    "\n",
    "agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "         \"You are a helpful instructor assistant for NVIDIA Deep Learning Institute (DLI). \"\n",
    "         \" Please help to answer user questions about the course. The first message is your context.\"\n",
    "         \" Restart from there, and strongly rely on it as your knowledge base. Do not refer to your 'context' as 'context'.\"\n",
    "         \" If you think nothing in the context accurately answers your question and you should search deeper,\"\n",
    "         \" include the exact phrase 'let me search deeper' in your response to perform a web search.\"\n",
    "    ),\n",
    "    (\"user\", \"<context>\\n{context}</context>\"),\n",
    "    (\"ai\", (\n",
    "        \"Thank you. I will not restart the conversation and will abide by the context.\"\n",
    "        \" If I need to search more, I will say 'let me search deeper' near the end of the response.\"\n",
    "    )),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "def user(state: State):\n",
    "    update = {\"messages\": [(\"user\", interrupt(\"[User]:\"))]}\n",
    "    return Command(update=update, goto=\"retrieval_router\")\n",
    "\n",
    "## TODO: Add the retrieval between user and agent\n",
    "def retrieval_router(state: State):\n",
    "    # Populate / extend state[\"context\"] based on the latest user message\n",
    "    return Command(update=retrieval_node(state), goto=\"agent\")\n",
    "\n",
    "def agent(state: State, config=None):\n",
    "    if \"END\" in state.get(\"messages\")[-1].content:\n",
    "        return {\"messages\": []}\n",
    "    update = {\"messages\": [(agent_prompt | llm).invoke(state, config=config)]}\n",
    "    if \"New Context Retrieved:\" in state.get(\"messages\")[-1].content:\n",
    "        pass\n",
    "    elif \"let me search deeper\" in update[\"messages\"][-1].content.lower():\n",
    "        return Command(update=update, goto=\"deep_thought_node\")\n",
    "    return Command(update=update, goto=\"start\")\n",
    "\n",
    "def deep_thought_node(state: State, config=None):\n",
    "    ## NOTE: Very Naive; Assumes user question is a good query\n",
    "    deeper_queries = query_node(state)['queries'][0]\n",
    "    new_rets = fulfill_queries(deeper_queries, verbose=True)\n",
    "    new_rets = filter_retrieval(deeper_queries, new_rets, state.get(\"context\"))\n",
    "    update = {\"messages\": [(\"user\", f\"New Context Retrieved: {new_rets}\")]}\n",
    "    return Command(update=update, goto=\"agent\")\n",
    "    \n",
    "##################################################################\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"start\", lambda state: {})\n",
    "builder.add_node(\"user\", user)\n",
    "## TODO: Register the new nodes to the nodepool\n",
    "builder.add_node(\"retrieval_router\", retrieval_router)\n",
    "builder.add_node(\"deep_thought_node\", deep_thought_node)\n",
    "builder.add_node(\"agent\", agent)\n",
    "\n",
    "builder.add_edge(START, \"start\")\n",
    "builder.add_edge(\"start\", \"user\")\n",
    "\n",
    "app = builder.compile(checkpointer=MemorySaver())\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "app_stream = partial(app.stream, config=config)\n",
    "\n",
    "for token in stream_from_app(app_stream, verbose=False, debug=False):\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e9ff4-a6fb-4d8d-8cb2-06ed364b0a1d",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>HINT:</b></summary>\n",
    "    <code>retrieval_router</code> is currently manually injecting a context of \"\", so maybe we can just run the call to our retrieval function with a minimal amount of wrapping?  \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary><b>SOLUTION:</b></summary>\n",
    "\n",
    "```python\n",
    "## TODO: Add the retrieval between user and agent\n",
    "def retrieval_router(state: State):\n",
    "    return Command(update=retrieval_node(state), goto=\"agent\")\n",
    "\n",
    "def retrieval_node(state: State, config=None, out_key=\"context\"):\n",
    "    ## NOTE: Very Naive; Assumes user question is a good query\n",
    "    ret = retrieve_via_query(state.get(\"messages\")[-1].content, k=3)\n",
    "    return {out_key: set(ret)}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<hr><br>\n",
    "\n",
    "### **Part 5:** Reflecting On This Exercise\n",
    "\n",
    "And just like that, we have some semblance of a ReAct-style loop, if in a much more limited capacity. Though it's not a \"pool of tools\" approach, it is definitely a \"reflection system\" with build-in routing. It also isn't really a proper \"deep researcher\" since it doesn't actually read the full body of the articles and isn't capable of expanding on the material quite yet, but it does exhibit very basic retrieval simplification to enable a much longer exchange window.\n",
    "\n",
    "**In the next section, get ready to try out the assessment where you will be implementing a reasoning and searching feature based on the techniques presented in this notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e804c0-7785-43a2-a316-72913d8c4459",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
