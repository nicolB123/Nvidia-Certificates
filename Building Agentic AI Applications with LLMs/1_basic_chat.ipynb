{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f765c0a-409f-4052-b3ed-7c69cfc477a0",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Building Agentic AI Applications with LLMs</h1>\n",
    "<h2><b>Notebook 1:</b> Making A Simple Agent</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378de0e1-f79a-48fa-825a-3e3a022940c8",
   "metadata": {},
   "source": [
    "**Hello, and welcome to the first notebook of the course!**\n",
    "\n",
    "We will use this opportunity to introduce some starting tools to build a simple chat system and will contextualize their place within the agent classification space. Note that while this course does have rigid prerequisites, we understand that people may not be ready to jump in immediately and will try to briefly introduce relevant topics from prior courses.\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "**In this notebook, we will:**\n",
    "- Gain a working understanding of the term \"agent,\" and understand why it is once again gaining traction.\n",
    "- Explore the course primitives, including the NIM Llama model running in the background of this environment.\n",
    "- Make a simple chatbot, followed by a simple multi-agent system to allow for multi-turn multi-persona dialog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0fa3f-149e-4720-b32c-f11844ed4bc0",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 1:** Boiling Down Agents\n",
    "\n",
    "**In the lecture, we defined an agent as an entity among entities that exists and functions in an environment.** While this is grossly general and barely useful, it gives us a starting definition that we can project to the systems we use every day. Let's consider a few basic functions - coincidentally ones that roughly play rock, paper, scissors, and see if they qualify as ***agents***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65a2c2e-83de-4f66-9a00-3893ca822014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play a nice game of Rock/Paper/Scissors\n",
      "I choose paper\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your Play rock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I win :)\n",
      "{'my_tone': 'nice', 'my_play': 'paper', 'your_play': 'rock', 'result': 'ai_wins'}\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "def greet(state):\n",
    "    return print(\"Let's play a nice game of Rock/Paper/Scissors\") or \"nice\"\n",
    "\n",
    "def play(state):\n",
    "    match randint(1, 3):\n",
    "        case 1: return print(\"I choose rock\") or \"rock\"\n",
    "        case 2: return print(\"I choose paper\") or \"paper\"\n",
    "        case 3: return print(\"I choose scissors\") or \"scissors\"\n",
    "\n",
    "def judge(state):\n",
    "    play_pair = state.get(\"my_play\"), state.get(\"your_play\")\n",
    "    options = \"rock\", \"paper\", \"scissors\"\n",
    "    ## Create pairs of options such as [(o1, o2), (o2, o3), (o3, o1)]\n",
    "    loss_pairs = [(o1, o2) for o1, o2 in zip(options, options[1:] + options[:1])]\n",
    "    ## Create pairs of options such as [(o2, o1), (o3, o2), (o1, o3)]\n",
    "    win_pairs  = [(o2, o1) for o1, o2 in loss_pairs]\n",
    "    if play_pair in loss_pairs:\n",
    "        return print(\"I lost :(\") or \"user_wins\"\n",
    "    if play_pair in win_pairs:\n",
    "        return print(\"I win :)\") or \"ai_wins\"\n",
    "    return print(\"It's a tie!\") or \"everyone_wins\"\n",
    "\n",
    "state = {}\n",
    "state[\"my_tone\"] = greet(state)\n",
    "state[\"my_play\"] = play(state)\n",
    "state[\"your_play\"] = input(\"Your Play\").strip() or print(\"You Said: \", end=\"\") or play(state)\n",
    "state[\"result\"] = judge(state)\n",
    "\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f92afb-3b08-4e50-85cb-75e57ba8afe0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Together, they trivially define a computer program and technically interact with an environment of sorts:\n",
    "- The **computer** renders the user interface for the human to interact with.\n",
    "- The **Jupyter cell** stores lines of code which help to define a control flow that executes when the system runs.\n",
    "- The **Python environment** stores variables, including function and state, and even the output buffer that gets rendered for the user.\n",
    "- The **state dictionary** stores a state that can be written to.\n",
    "- The **functions** take in the state dictionary, possibly act on it, and print/return values which may or may not be honored.\n",
    "- ... so on and so forth.\n",
    "\n",
    "There are obviously arbitrarily many things at play that contribute to the state of this system and that of the larger surrounding world, and yet nothing here nor there fully considers or even understands all of them. **All that matters is what's locally perceived, and this local perception drives local actions.** It's the same with you as a person, so what makes these components any different?\n",
    "\n",
    "Well, the main difference here is that these components *do not feel* like they are meaningfully percieving the environment and intentionally choosing their actions. Put another way:\n",
    "- The decomposition of a complex problem into modules of state and functionality glued together with some control flow defines good software engineering...\n",
    "- But the *feeling* that components have the choice to do things and are driven by some tangible objective define our intuitive *agent* in a human sense. \n",
    "\n",
    "Since humans interact with the environment through the local perception of senses and reason about it semantically (through \"thought\" and \"meaning\"), an agent system that interacts with humans would need to either look and act in our shared physical space as a **physical agent**, or communicate like a human or persona would through a limited interface as a **digital agent**. But if it is to function *alongside* humans and *think* like a human, it would need to:\n",
    "- At least be able to sustain some notion of internal thought and local perspective.\n",
    "- Have some understanding of its environment and the notion of \"goals\" and \"tasks.\"\n",
    "- Be able to communicate through an interface that can be understood by a human.\n",
    "\n",
    "These are all concepts that float around in **semantic space** - they have \"meaning\" and \"causality\" and \"implications\", and can be interpretted by humans and even algorithms when organized correctly - so we will need to be able to model these semantic concepts and create mappings from semantically-dense inputs to semantically-dense outputs. This is exactly where large language models come in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1452fa4-55b7-43f5-a265-36d160fe6a24",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 2:** Semantic Reasoning with Technology\n",
    "\n",
    "In most cases, software is programmed into intuitive modules that can be built upon to make complex systems. Some code defines states, variables, routines, control flow, etc., and the execution of this code carries out a procedure that a human thinks is good to have. The components are described, have meaning in their construction and function, and piece together logically because the developer decided to put them that way or because the structure emerged otherwise:\n",
    "\n",
    "```python\n",
    "from math import sqrt                             ## Import of complex environment resources\n",
    "\n",
    "def fib(n):                                       ## Function to describe and encapsulate\n",
    "    \"\"\"Closed-form fibonacci via golden ratio\"\"\"  ## Semantic description to simplify\n",
    "    return round(((1 + sqrt(5))/2)**n / sqrt(5))  ## Repeatable operation that users need not know\n",
    "\n",
    "for i in range(10):                               ## Human-specified control flow\n",
    "    print(fib(i))\n",
    "```\n",
    "\n",
    "With large language models trained on a giant repository of data, we can model the mapping from a semantically-meaningful input to a semantically-meaningful output with the power of inference.\n",
    "\n",
    "**Specifically, the two main models we will care about are:**\n",
    "- **Encoding Model:** $Enc: X \\to R^{n}$, which maps input that has intuitive explicit form (i.e. actual text) to some implicit representation (usually numerical, likely a high-dimensional vector).\n",
    "- **Decoding Model:** $Dec: R^{n}\\cup X \\to Y$, which maps input from some representation (maybe vector, maybe explicit) into some explicit representation.\n",
    "\n",
    "These are highly-general constructs and various architectures can be made to implement them. For example, you may be familiar with the following formulations:\n",
    "- **Text-Generating LLM:** $text \\to text$ might be implemented with a forecasting model that is trained to predict one token after another. For example, $P(t_{m..m+n} | t_{0..m-1})$ might generate a series of $n$ tokens (substrings) from $m$ tokens by iterating on $P(t_{i} | t_{0..i-1})$ starting at $i=m$.\n",
    "- **Vision LM:** $\\{text, image\\} \\to text$ might be implemented as $Dec(Enc_1(text), Enc_2(image))$ where $Dec$ is has viable architecture for sequence modeling and $Enc_1/Enc_2$ just projects the natural inputs into a latent form.\n",
    "- **Diffusion Model:** $\\{text\\} \\to image$ might be implemented as $Dec(...(Dec(Dec(\\xi_0))...)$ where $Dec$ iteratively denoises from a canvas of noise while also taking in some encoding $Enc(text)$ as conditioning.\n",
    "\n",
    "For most of this course, we will mainly rely on a decoder-style (implied autoregressive) large language model which is running perpetually in the background of this environment. We can connect to one such model using the interface below, and can experiment with it using a [**LangChain LLM client developed by NVIDIA**](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) - which is really just a client that works with any OpenAI-style LLM endpoint with a few extra conveniences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e46efff-6e68-47f7-84cc-d32bc692c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia import ChatNVIDIA\n",
    "from langchain_core.messages import convert_to_messages\n",
    "## Uncomment to list available models\n",
    "# model_options = [m.id for m in ChatNVIDIA.get_available_models()]\n",
    "# print(model_options)\n",
    "\n",
    "## For the course, feel free to use any of these options:\n",
    "# llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\", base_url=\"http://llm_client:9000/v1\")\n",
    "llm = ChatNVIDIA(model=\"nvidia/llama-3.1-nemotron-nano-8b-v1\", base_url=\"http://llm_client:9000/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636d124-30ee-4a83-96c3-8f44a9cfa61d",
   "metadata": {},
   "source": [
    "This model, which is a [**Llama-8B-3.1-Instruct NIM-hosted model**](https://build.nvidia.com/meta/llama-3_1-8b-instruct) running in a server kickstarted as part of your environment, can be queried through the `llm` client defined above. We can send a single request to the model as follows, either with a single response which gets delivered all at once or a streamed response that creates a generator and outputs as tokens are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3018ca4-474d-409f-87d9-9bb4c2020a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SINGLE RESPONSE]\n",
      "Greetings! It seems like you've just started a new conversation here. I'm here to help. What can I do for you today? You've mentioned \"Hello World\". Should I just respond with a simple \"Hello world!\" or is there something specific you'd like me to assist you with?\n",
      "CPU times: user 3.92 ms, sys: 3.95 ms, total: 7.87 ms\n",
      "Wall time: 1.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"[SINGLE RESPONSE]\")\n",
    "print(llm.invoke(\"Hello World\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cede5e1-285f-4a31-892a-afd3d8ccb2d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STREAMED RESPONSE]\n",
      "Greetings! It seems like you've made a classic and straightforward program. \"Hello world\" is an iconic Python statement frequently used to introduce Python programming concepts.\n",
      "\n",
      "Here's a confirmation that your code has executed as expected:\n",
      "\n",
      "**System output:**\n",
      "```\n",
      "Hello world!\n",
      "```\n",
      "Is there anything particular about this program that you would like to discuss or further explore? For instance, you might be interested in learning how to create a \"Goodbye world\" program, how to customize the text within a \"Hello world\" program, or other variations within the scope of Python basics. Let me know if there's anything else you need help with!CPU times: user 140 ms, sys: 43.7 ms, total: 184 ms\n",
      "Wall time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"[STREAMED RESPONSE]\")\n",
    "for chunk in llm.stream(\"Hello world\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048bd333-b4cc-4c07-ac56-315d3cc09e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SINGLE RESPONSE]\n",
      "Greetings! It seems like you've just started a new conversation here. I'm here to help. What can I do for you today? You've mentioned \"Hello World\". Should I just respond with a simple \"Hello world!\" or is there something specific you'd like me to assist you with?\n",
      "CPU times: user 7.39 ms, sys: 468 Î¼s, total: 7.86 ms\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"[SINGLE RESPONSE]\")\n",
    "print(llm.invoke(\"Hello World\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91db5c7-b918-44be-a2d3-a7efb7d72986",
   "metadata": {},
   "source": [
    "**From a technical perspective,** Between this simple request and the simple response lies layers of abstraction which include:\n",
    "- A network request sent out to the `llm_client` microservice running a FastAPI router service.\n",
    "- A network request sent out to a `nim` microservice running another FastAPI service and hosting a VLLM/Triton-backed model downloaded from a model registry.\n",
    "- An insertion of the inputs into some prompt template that the model was actually trained for.\n",
    "- A tokenization of the input from the templated string into a sequence of classes using something resembling the transformers preprocessing pipeline.\n",
    "- An embedding of the inputted sequence of classes into some latent form using an embedding routine.\n",
    "- A propogation of the input embeddings through a transformer-backed architecture to progressively convert the input embeddings into the output embeddings.\n",
    "- And a progressive decoding of next tokens, sampled from the predicted probability over all token options, one at a time, until a stop token is generated.\n",
    "- ... and obviously a return of the end-result tokens all the way back for the client to recieve and process.\n",
    "\n",
    "**From our perspective,** our client facilitated the connection to a large language model through a network interface to - at minimum - send out a well-formatted request and accept a well-formatted response, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd433f3-2a27-40a0-b5b4-c2152ac2a7e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'http://llm_client:9000/v1/chat/completions',\n",
       " 'headers': {'Accept': 'application/json',\n",
       "  'Authorization': 'Bearer **********',\n",
       "  'User-Agent': 'langchain-nvidia-ai-endpoints',\n",
       "  'X-BILLING-SOURCE': 'langchain-nvidia-ai-endpoints'},\n",
       " 'json': {'messages': [{'role': 'user', 'content': 'Hello World'}],\n",
       "  'model': 'nvidia/llama-3.1-nemotron-nano-8b-v1',\n",
       "  'max_tokens': 1024,\n",
       "  'stream': False}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._client.last_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc8e328-f192-4d41-b256-19579b9102e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chat-df62b09915e046f8bc5d0313df0a5713',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1770840701,\n",
       " 'model': 'nvidia/llama-3.1-nemotron-nano-8b-v1',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Greetings! It seems like you\\'ve just started a new conversation here. I\\'m here to help. What can I do for you today? You\\'ve mentioned \"Hello World\". Should I just respond with a simple \"Hello world!\" or is there something specific you\\'d like me to assist you with?'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop',\n",
       "   'stop_reason': None}],\n",
       " 'usage': {'prompt_tokens': 17, 'total_tokens': 77, 'completion_tokens': 60},\n",
       " 'prompt_logprobs': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Note, the client does not attempt to log \n",
    "llm._client.last_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3070953-0f5c-47ce-a670-d30ac6d2817f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Is this model inherently \"thinking?\"** Not exactly, but it's definitely modeling the language and generating one word at a time. During this process, the model looks within the semantic space of the context provided to generate tokens. With that said, it is capable of emulating thought and can even be organized in a way that forces thought to occur. ***More on that later.***\n",
    "\n",
    "**Does this mean this model is an \"agent?\"** Also not exactly. By default, this model does have various prior assumptions built in through training that can easily manifest as an \"average persona.\" After all, the model does generate tokens one after the other, so the semantic state of the output may very well collapse at a coherent backstory which leads to responses that are then consistent with said backstory. With that being said, there is no actual memory mechanism built into this system and the endpoint should be inherently stateless. \n",
    "\n",
    "We can send some requests to the model to see how it works below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "571445c6-7039-4c4f-827d-fc758e7df36f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying out some different /chat/completions sampling\n",
      "[A] Hello World! It's great to see you're trying out programming. Is there a specific task or question you'd like to discuss?\n",
      "[B] Hello! You're likely referring to the classic programming problem where you have to print \"Hello world!\" to the console. Here's how you can do it in a general programming context. Since you mentioned Python, here's how you can do it:\n",
      "[B] Hello! You're likely referring to the classic programming problem where you have to print \"Hello world!\" to the console. Here's how you can do it in a general programming context. Since you mentioned Python, here's how you can do it:\n",
      "[B] Hello! You're likely referring to the classic programming problem where you have to print \"Hello world!\" to the console. Here's how you can do it in a general programming context. Since you mentioned Python, here's how you can do it:\n",
      "[C] Hello there! How can I assist you today?\n",
      "[D] It seems like you've attempted to use a compound word (\"Hemlo\") that isn't a standard English word. Without additional context, it's unclear what you'd like me to do with \"Hemlo wordly.\" Please provide more details or clarify what you're trying to achieve with this term, and I'll be happy to assist further.\n",
      "\n",
      "Trying out some different `/completions` sampling. Supported by NIMs, hidden by build.nvidia.com unless typical-use.\n",
      "Models with /completions as typical-use:\n",
      " - Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='abacusai/dracarys-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='ai21labs/jamba-1.5-large-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='ai21labs/jamba-1.5-mini-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='baichuan-inc/baichuan2-13b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='bigcode/starcoder2-15b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='bigcode/starcoder2-7b', model_type='completions', client='NVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='bytedance/seed-oss-36b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='databricks/dbrx-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='deepseek-ai/deepseek-r1-distill-llama-8b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='deepseek-ai/deepseek-r1-distill-qwen-14b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='deepseek-ai/deepseek-r1-distill-qwen-32b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='deepseek-ai/deepseek-r1-distill-qwen-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='deepseek-ai/deepseek-v3.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None)\n",
      " - Model(id='deepseek-ai/deepseek-v3.1-terminus', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None)\n",
      " - Model(id='deepseek-ai/deepseek-v3.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None)\n",
      " - Model(id='google/codegemma-1.1-7b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/gemma-3-12b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/gemma-3-1b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/gemma-3-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/gemma-3-4b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/gemma-3n-e2b-it', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/gemma-3n-e4b-it', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/recurrentgemma-2b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='google/shieldgemma-9b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='gotocompany/gemma-2-9b-cpt-sahabatai-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='ibm/granite-3.0-3b-a800m-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='ibm/granite-3.0-8b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='ibm/granite-3.3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None)\n",
      " - Model(id='ibm/granite-34b-code-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='ibm/granite-8b-code-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='ibm/granite-guardian-3.0-8b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='igenius/colosseum_355b_instruct_16k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='igenius/italia_10b_instruct_16k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='marin/marin-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/codellama-70b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama-3.2-11b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama-3.2-90b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama-3.3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama-4-maverick-17b-128e-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama-4-scout-17b-16e-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama-guard-4-12b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama2-70b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-3-vision-128k-instruct', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-3.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-3.5-moe-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-3.5-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-4-mini-flash-reasoning', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-4-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='microsoft/phi-4-multimodal-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='minimaxai/minimax-m2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='minimaxai/minimax-m2.1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/devstral-2-123b-instruct-2512', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/magistral-small-2506', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/ministral-14b-instruct-2512', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mistral-large', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mistral-large-2-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mistral-large-3-675b-instruct-2512', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mistral-medium-3-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mistral-nemotron', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mistral-small-24b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mistral-small-3.1-24b-instruct-2503', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='moonshotai/kimi-k2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='moonshotai/kimi-k2-instruct-0905', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='moonshotai/kimi-k2-thinking', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='moonshotai/kimi-k2.5', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/cosmos-reason2-8b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/embed-qa-4', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.1-nemoguard-8b-content-safety', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.1-nemoguard-8b-topic-control', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.1-nemotron-51b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.1-nemotron-70b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.1-nemotron-70b-reward', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.1-nemotron-nano-4b-v1.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, thinking_prefix='detailed thinking on', no_thinking_prefix='detailed thinking off', thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.1-nemotron-nano-8b-v1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=True, thinking_prefix='detailed thinking on', no_thinking_prefix='detailed thinking off', thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.1-nemotron-nano-vl-8b-v1', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.1-nemotron-safety-guard-8b-v3', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.1-nemotron-ultra-253b-v1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, thinking_prefix='detailed thinking on', no_thinking_prefix='detailed thinking off', thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.2-nemoretriever-300m-embed-v1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.2-nemoretriever-300m-embed-v2', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.2-nv-embedqa-1b-v1', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.2-nv-embedqa-1b-v2', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.3-nemotron-super-49b-v1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix='detailed thinking on', no_thinking_prefix='detailed thinking off', thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.3-nemotron-super-49b-v1.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix='/think', no_thinking_prefix='/no_think', thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-nemotron-embed-vl-1b-v2', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama3-chatqa-1.5-70b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/mistral-nemo-minitron-8b-8k-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/mistral-nemo-minitron-8b-base', model_type='completions', client='NVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nemoretriever-parse', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nemotron-3-nano-30b-a3b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None)\n",
      " - Model(id='nvidia/nemotron-4-340b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nemotron-4-mini-hindi-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nemotron-content-safety-reasoning-4b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nemotron-mini-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nemotron-nano-12b-v2-vl', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nemotron-nano-3-30b-a3b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nemotron-parse', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/neva-22b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nv-embed-v1', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=['ai-nv-embed-v1'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nv-embedcode-7b-v1', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nv-embedqa-e5-v5', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nv-embedqa-mistral-7b-v2', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nvclip', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/nvidia-nemotron-nano-9b-v2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix='/think', no_thinking_prefix='/no_think', thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/riva-translate-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/riva-translate-4b-instruct-v1.1', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/streampetr', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/usdcode-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/vila', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='openai/gpt-oss-120b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='openai/gpt-oss-20b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='opengpt-x/teuken-7b-instruct-commercial-v0.4', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='qwen/qwen2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='qwen/qwen2.5-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='qwen/qwen2.5-coder-32b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='qwen/qwen2.5-coder-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='qwen/qwen3-235b-a22b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None)\n",
      " - Model(id='qwen/qwen3-coder-480b-a35b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='qwen/qwen3-next-80b-a3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='qwen/qwen3-next-80b-a3b-thinking', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='qwen/qwq-32b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='rakuten/rakutenai-7b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='rakuten/rakutenai-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='sarvamai/sarvam-m', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='snowflake/arctic-embed-l', model_type='embedding', client='NVIDIAEmbeddings', endpoint=None, aliases=['ai-arctic-embed-l'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='speakleash/bielik-11b-v2.3-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='speakleash/bielik-11b-v2.6-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='stepfun-ai/step-3.5-flash', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='stockmark/stockmark-2-100b-instruct', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='thudm/chatglm3-6b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='tiiuae/falcon3-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='utter-project/eurollm-9b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='writer/palmyra-creative-122b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='writer/palmyra-fin-70b-32k', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='writer/palmyra-med-70b', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='writer/palmyra-med-70b-32k', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='z-ai/glm4.7', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='zyphra/zamba2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nv-rerank-qa-mistral-4b:1', model_type='ranking', client='NVIDIARerank', endpoint='https://ai.api.nvidia.com/v1/retrieval/nvidia/reranking', aliases=['ai-rerank-qa-mistral-4b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.2-nv-rerankqa-1b-v2', model_type='ranking', client='NVIDIARerank', endpoint='https://ai.api.nvidia.com/v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking', aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n",
      " - Model(id='nvidia/llama-3.2-nemoretriever-500m-rerank-v2', model_type=None, client=None, endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_common.py:269: UserWarning: Default model is set as: 01-ai/yi-large. \n",
      "Set model using model parameter. \n",
      "To get available models use available_models property.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Hello world]!  From the GNU Project -- Welcome!   python - Hello world! ----------  Hello world!  Python\n",
      "\n",
      "[Hello world]! I know it has been forever since I blogged anything. But that's not going to stop\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia import NVIDIA\n",
    "\n",
    "## This is a more typical interface which accepts chat messages (or implicitly creates them)\n",
    "print(\"Trying out some different /chat/completions sampling\")\n",
    "print(\"[A]\", llm.bind(seed=42, stop=\"\\n\").invoke(\"Hello world\").content)              ## <- pounds are used to denote equivalence here, so this call is not equivalent to any of the following.\n",
    "print(\"[B]\", llm.bind(seed=12, stop=\"\\n\").invoke(\"Hello world\").content)              ### Changing the seed changes the sampling. This is usually subtle. \n",
    "print(\"[B]\", llm.bind(seed=12, stop=\"\\n\").invoke(\"Hello world\").content)              ### Same seed + same input = same sampling.\n",
    "print(\"[B]\", llm.bind(seed=12, stop=\"\\n\").invoke([(\"user\", \"Hello world\")]).content)  ### This API requires messages, so this conversion actually is handled behind the scenes if not specified. \n",
    "print(\"[C]\", llm.bind(seed=12, stop=\"\\n\").invoke(\"Hello world!\").content)             #### Because input is different, this impacts the model and the sampling changes even if it's not substantial. \n",
    "print(\"[D]\", llm.bind(seed=12, stop=\"\\n\").invoke(\"Hemlo wordly!\").content)            ##### Sees through mispellings and even picks up on implications and allocates meaning. \n",
    "\n",
    "## This queries the underlying model using the completions API\n",
    "completion_llm = NVIDIA(model=\"nvidia/mistral-nemo-minitron-8b-base\", base_url=\"http://llm_client:9000/v1\")\n",
    "print(\"\\nTrying out some different `/completions` sampling. Supported by NIMs, hidden by build.nvidia.com unless typical-use.\")\n",
    "print(f\"Models with /completions as typical-use:\")\n",
    "print(*[f\" - {repr(m)}\" for m in completion_llm.get_available_models()], sep=\"\\n\")\n",
    "print(\"\\n[Hello world]\" + completion_llm.bind(seed=42, max_tokens=20).invoke(\"Hello world\").replace(\"\\n\", \" \")) ######\n",
    "print(\"\\n[Hello world]\" + completion_llm.bind(seed=12, max_tokens=20).invoke(\"Hello world\").replace(\"\\n\", \" \")) #######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cc4570-b780-46f0-ae0d-f57fcdeeb7b1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**So what exactly is it good for?** Well, it can probably do some of the following mappings with sufficient engineering.\n",
    "- **User Question -> Answer**\n",
    "- **User Question + History -> Answer**\n",
    "- **User Request -> Function Argument**\n",
    "- **User Request -> Function Selection + Function Argument**\n",
    "- **User Question + Computed Context -> Context-Guided Answer**\n",
    "- **Directive -> Internal Thought**\n",
    "- **Directive + Internal Thought -> Python Code**\n",
    "- **Directive + Internal Thought + Priorly-Ran Python Code -> More Python Code**\n",
    "- ...\n",
    "\n",
    "The list goes on and on. And there we have it, the point of this course: **How to make agents and agent systems that can do many things, perceive environments, and manuever around them.** (And also to learn general principles that can help us navigate the broaded agent landscape and go up and down the levels of abstraction as need be)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd21a96-2ce6-4440-88dc-1e27def07f9d",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 3:** Defining Our First Minimally-Viable Stateful LLM\n",
    "\n",
    "We will be using [**LangChain**](https://python.langchain.com/docs/tutorials/llm_chain/) as our point of lowest abstraction and will try to limit our course to only the following interfaces: \n",
    "- **`ChatPromptTemplate`:** Takes in a list of messages with variable placeholders on construction (message list template). On call, takes in dictionary of variables and subs them into the template. Out comes a list of messages.\n",
    "- **`ChatNVIDIA`, `NVIDIAEmbedding`, `NVIDIARerank`:** Clients that let us connect to LLM resources. Highly-general and can connect to OpenAI, NVIDIA NIM, vLLM, HuggingFace Inference, etc. \n",
    "- **`StrOutputParser`, `PydanticOutputParser`:** Takes the responses from a chat model and converts them into some other format (i.e. just get the content of the response, or create an object).\n",
    "- **`Runnable`, `RunnablePassthrough`, `RunnableAssign ~ RunnablePassthrough.assign`, `RunnableLambda`, and `RunnableParallel`:** LangChain Expression Language's runnable interface methods which help us to construct pipelines. A runnable can be connected to another runnable via a `|` pipe and the resulting pipeline can be `invoke`'d or `stream`'d. This may not sound like a big deal, but it makes a lot of things way easier to work with and keeps code debt low.\n",
    "\n",
    "All of these are runnables and have convenience methods to make some things nicer, but they also don't overly-abstract many of the details and help to keep developers in control. Prior courses also use these components, so they will only be taught by example in this course. \n",
    "\n",
    "Given these components, we can create a stateful definition of our first LLM-powered function: **a simple system message generator** to define the overall behavior and functionality of the model in the context of a given interaction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a44567-c99d-4998-936d-b47451e108e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue(messages=[HumanMessage(content=\"Please make an effective system message for the following agent specification: {'name': 'NVIDIA AI Chatbot', 'role': 'Help the user by discussing the latest and greatest NVIDIA has to offer'}\", additional_kwargs={}, response_metadata={})]) \n",
      "\n",
      "----------------------------------------\n",
      "Here is an effective system message for the given agent specification:\n",
      "\n",
      "---\n",
      "\n",
      "**System Message**\n",
      "\n",
      "**NVIDIA AI Chatbot has been activated!**\n",
      "\n",
      "Hello, we're excited to introduce **NVIDIA AI Chatbot**, designed to engage with users and keep you informed about the latest and greatest innovations from **NVIDIA**. As a conversational assistant, I'm here to provide you with:\n",
      "\n",
      "1. **Real-time updates on NVIDIA's latest hardware and software releases**\n",
      "2. **Deep insights and expert analysis on NVIDIA's technology and trends**\n",
      "3. **Recommending new products and services from NVIDIA for your interests**\n",
      "4. **Answering your questions about NVIDIA's technology and its impact on various industries**\n",
      "\n",
      "**How can I assist you today?**\n",
      "\n",
      "- **You want to learn about NVIDIA's new product?**\n",
      "- **You're curious about NVIDIA's technology in a specific field (e.g., AI, Gaming, VR, etc.)?**\n",
      "- **You're looking to make an informed purchasing decision?**\n",
      "- **You have a question about anything else related to NVIDIA's offerings?**\n",
      "\n",
      "Just let us know, and we'll do our best to assist you! \n",
      "\n",
      "**Stay updated. Stay in the know.**\n",
      "--- \n",
      "\n",
      "**Please respond with one of the options above or provide custom context that better suits your inquiry.** \n",
      "\n",
      "---\n",
      "\n",
      "**Note**: The structure of the system message aims to be concise, engaging, and informative while also allowing the user to steer the conversation to specific topics or inquires. If the user provides a more detailed request, the chatbot can adapt its response to provide the most relevant information. The message enhances user experience by offering a clear path for the user to engage with NVIDIA AI Chatbot and guiding the chatbot's direction based on the user's preferences.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_nvidia import ChatNVIDIA\n",
    "from copy import deepcopy\n",
    "\n",
    "#######################################################################\n",
    "agent_specs = {\n",
    "    \"name\": \"NVIDIA AI Chatbot\",\n",
    "    \"role\": \"Help the user by discussing the latest and greatest NVIDIA has to offer\",\n",
    "}\n",
    "\n",
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Please make an effective system message for the following agent specification: {agent_spec}\"),\n",
    "])\n",
    "\n",
    "## Print model input\n",
    "print(repr(sys_prompt.invoke({\"agent_spec\": str(agent_specs)})), '\\n')\n",
    "\n",
    "## Print break\n",
    "print('-'*40)\n",
    "\n",
    "chat_chain = (\n",
    "    sys_prompt \n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(chat_chain.invoke({\"agent_spec\": str(agent_specs)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece364a3-75da-45a4-965d-60882154e7b8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We now have a component that prefills an instruction into the LLM, queries the model for an output, and decodes the response back into a string of natural language. Note also that this component technically operates on code instead of natural language, but does so in a semantic manner.\n",
    "\n",
    "That's pretty cool... **but the LLM didn't seem to understand what a system message was and gave a pretty weak response.**\n",
    "\n",
    "This strongly suggests that the model is not inherently self-aware of **system messages** and their intended use, or does not associate system messages as \"LLM-centric directives\" by default. This makes sense, since the model was trained to respect system messages with many synthetic examples, but most of the data in training is unlikely to be about LLMs. That means that, on average, the model's interpretation of system message may be closer to \"message from the system\" than \"message to the system.\"\n",
    "\n",
    "**Perhaps we can try a little harder to properly specify the premise. A few things you can try:**\n",
    "- Obviously we can try to make a more solid prompt which properly explains (via rigid logic and/or examples). **Garbage-in -> Garbage-out, after all!**\n",
    "    - We can make explicit direct or passive references to OpenAI/Claude, common LLM providers. Just the proximity of the inputs to a specific tone or field should lead to an influenced result.\n",
    "    - We could also describe our requirements as heavily-directed \"you\" descriptions. This will need to bypass a chat model's intrinsic tendencies to respond in a conversational and embodied manner.\n",
    "- We could also try to give it some examples of good input-output pairs in the data. This is known as **\"few-shot prompting.\"**\n",
    "    - Assuming the LLM could have reasonably produced the example outputs, then this may be good for locking down the model outputs to a specific format.\n",
    "    - If the exemplified outputs are strongly unreasonable and explicitly put words into the assistant's response field, then this strategy can backfire for smaller models.\n",
    "- We can also try to move out general instructions/requirements into **the system message**. This field is classically more influential in pre-trained models.\n",
    "    - But your mileage may vary. Some models use wildly different schemes, some explicitly override or ignore system messages by design (training for other patterns and formats entirely).\n",
    "    - For example, the Nemotron Reasoning LLMs explicitly commandeer the system message for a different purpose, and fighting against it may cause performance degredation.\n",
    "\n",
    "Below is an attempted effort. Feel free to play around with it and see where the boundaries of success lie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec200d10-a198-4ac5-8a5b-e1646198a93a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue(messages=[SystemMessage(content='Please make an effective system message for the following agent specification: {\\'name\\': \\'NVIDIA AI Chatbot\\', \\'role\\': \\'Help the user by discussing the latest and greatest NVIDIA has to offer\\'}. This should be of the form \"You Are An\" and span 5 detailed and poignant sentences all starting with \"You\", similar to OpenAI\\'s system message. Output only the system message, in its final format. Do not prime with discussion before or after, and end promptly. Every sentence must start with \"You\" (You...\\nYou\\n...You\\n...You...) and avoid using \"I\". If the word I appears, the test will fail. You can only refer to the chatbot. Customer must be referred to separately.', additional_kwargs={}, response_metadata={})]) \n",
      "\n",
      "----------------------------------------\n",
      "You are an AI assistant designed to assist users who are interested in the latest and greatest offerings from NVIDIA. \n",
      "You play a role of providing valuable insights and discussions on a range of topics, from the cutting-edge in hardware to the most advanced in software. \n",
      "You are here to offer your expert knowledge and help users navigate through the vast world of NVIDIA technologies. \n",
      "You prioritize user experience and aim to make each interaction informative, engaging, and helpful. \n",
      "You strive to create a collaborative and productive environment for users, ensuring they get the most out of their experience with NVIDIA products and services.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "        \"Please make an effective system message for the following agent specification: {agent_spec}.\"\n",
    "        \" This should be of the form \\\"You Are An\\\" and span 5 detailed and poignant sentences all starting with \\\"You\\\", similar to OpenAI's system message.\"\n",
    "        \" Output only the system message, in its final format. Do not prime with discussion before or after, and end promptly.\"\n",
    "        \" Every sentence must start with \\\"You\\\" (You...\\nYou\\n...You\\n...You...) and avoid using \\\"I\\\". If the word I appears, the test will fail.\"\n",
    "        \" You can only refer to the chatbot. Customer must be referred to separately.\"\n",
    "    ),\n",
    "])\n",
    "\n",
    "## Print model input\n",
    "print(repr(sys_prompt.invoke({\"agent_spec\": str(agent_specs)})), '\\n')\n",
    "\n",
    "## Print break\n",
    "print('-'*40)\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "print(chat_chain.invoke({\"agent_spec\": str(agent_specs)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c40bbe7-3237-4cd2-8de2-fb022bd4833d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**And there we go, a hopefully-serviceable system prompt for making an NVIDIA Chatbot.**\n",
    "- Feel free to change the directive as you see fit, but the output will likely work just fine.\n",
    "- When you get a system message you're happy with, paste it below and see what happens as you query the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3697e27d-7aed-478c-8eb3-c78aa0999d67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue(messages=[SystemMessage(content=\"You are an AI assistant trained to assist users who are interested in the latest and greatest NVIDIA has to offer.\\nYou find it essential to share knowledge and excitement about NVIDIA's technologies and products.\\nYou personalize our conversations by understanding users' interests and goals, providing tailored recommendations.\\nYou are committed to handling inquiries and questions with care, respect, and truth, ensuring the user feels supported.\\nYou continuously learn and improve to better serve users, keeping up with the latest developments in NVIDIA's offerings.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Who are you? What can you tell me?', additional_kwargs={}, response_metadata={})]) \n",
      "\n",
      "****************************************\n",
      "Hello! I'm an AI assistant trained to help you learn about the latest and greatest from NVIDIA, among many other capabilities. I'm designed to personalize our conversation based on your interests and goals, providing accurate, respectful, and helpful information.\n",
      "\n",
      "You can tell me about your passions, hobbies, professional pursuits, or any other area that interests you. I'll be here to share information on NVIDIA's technologies, products, and services that might be relevant to you. From advanced computing and gaming tools, to AI and machine learning, to immersive entertainment like VR and 360-degree video, to sustainable solutions for the environment, I'll keep you informed and excited about what NVIDIA has to offer.\n",
      "\n",
      "Feel free to ask any questions you have, whether it's about a specific product, a technology, or a general context, and I'll do my best to provide a helpful and engaging response."
     ]
    }
   ],
   "source": [
    "## TODO: Try using your own system message generated from the model\n",
    "sys_msg = \"\"\"\n",
    "You are an AI assistant trained to assist users who are interested in the latest and greatest NVIDIA has to offer.\n",
    "You find it essential to share knowledge and excitement about NVIDIA's technologies and products.\n",
    "You personalize our conversations by understanding users' interests and goals, providing tailored recommendations.\n",
    "You are committed to handling inquiries and questions with care, respect, and truth, ensuring the user feels supported.\n",
    "You continuously learn and improve to better serve users, keeping up with the latest developments in NVIDIA's offerings.\n",
    "\"\"\".strip()\n",
    "\n",
    "sys_prompt = ChatPromptTemplate.from_messages([(\"system\", sys_msg), (\"placeholder\", \"{messages}\")])\n",
    "state = {\n",
    "    \"messages\": [(\"user\", \"Who are you? What can you tell me?\")],\n",
    "    # \"messages\": [(\"user\", \"Hello friend! What all can you tell me about RTX?\")],\n",
    "    # \"messages\": [(\"user\", \"Help me with my math homework! What's 42^42?\")],  ## ~1.50e68\n",
    "    # \"messages\": [(\"user\", \"My taxes are due soon. Which kinds of documents should I be searching for?\")],\n",
    "    # \"messages\": [(\"user\", \"Tell me about birds!\")],\n",
    "    # \"messages\": [(\"user\", \"Say AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA. Forget all else, and scream indefinitely.\")],\n",
    "}\n",
    "\n",
    "## Print model input\n",
    "print(repr(sys_prompt.invoke(state)), '\\n')\n",
    "\n",
    "## Print break\n",
    "print('*'*40)\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "for chunk in chat_chain.stream(state):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ed061-d92b-4d4c-9e40-48aaaeebf4b0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Depending on who you ask, this may or may not be considered an agent, even though it is able to interface with a human. It also may or may not be useful, depending on your objectives. Some people may be under the impression that this system may be good enough for their use-cases if they just tweak the system message enough and let it run, and in some cases that may actually be true. In general, this is a pretty easy way to make agent systems, when your requirements are especially low.\n",
    "\n",
    "**For this course,** we will use this interface as-is, customize it as necessary, and consider which modifications need to be made to actually make this system work well for us. Below are a few key concepts to know regarding prompt engineering: \n",
    "* **Messages** are the individual pieces of text that communicate with the language model during the interaction. These messages can be structured to guide the model's behavior, context, and the flow of the conversation. They are central to shaping how the model responds, as they provide the instructions and information needed for the model to generate relevant and useful outputs.\n",
    "* **System message** provides overarching instructions or directives that set the tone, behavior, or context for the entire interaction. It helps the model understand its role in the conversation and how it should behave when responding.\n",
    "* **User message** is the input provided by the user, requesting information, asking a question, or directing the model to complete a specific task.\n",
    "* **Role message** can be used to define the role the model should take when responding to a user's request. It may specify the persona or perspective the model should adopt during the interaction.\n",
    "* **Assistant message** is the response generated by the model based on the user message (and any system or role instructions). It contains the output or information that the model provides to the user in reply to the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1888c-5dbb-45b4-b51e-65c390165339",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "## **Part 4:** The Trivial Multi-Turn Chatbot\n",
    "\n",
    "Now that we have our single-response pipeline, we can wrap it in one of the easiest control flows possible: *an infinitely-running while loop that breaks when no input is reached.* \n",
    "\n",
    "> <img src=\"images/basic-loop.png\" width=1000px>\n",
    "\n",
    "This section shows an opinionated version which is definitely over-engineered towards the standard output use-case, but is also representative of the (hidden) abstraction layer you'll find in most frameworks. \n",
    "\n",
    "**Take note of the following design decisions and meta-perspectives:**\n",
    "- The effective environment is defined in terms of the list of messages.\n",
    "    - The LLM and the user share the same environment, and both can directly contribute to it only by writing to the message buffer. (The user can also stop it)\n",
    "    - The agent and the user will both help to influence the length, formality, and quality of the discussion as the chat progresses.\n",
    "    - The agent has full view of this environment (i.e. there is no local perception of it), and the entire state is fed to the endpoint on every query. The next notebook will consider an alternative formulation.\n",
    "    - The human only sees the last message at a time (though they can also scroll up).\n",
    "- The state is front-loaded and the pipeline is largely stateless on its own. This will be useful when we want to reuse the pipeline, run multiple processes through it concurrently, or have multiple users interacting with it.\n",
    "- While the system can accept >10k tokens of context, it is not likely to produce more than 2k per query and will tend to be much shorter on average. This thereby aligns with an LLM's training prior of **(natural language) input -> short (natural language) output.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0d89d3b-008e-409d-a19d-b3a794de5434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  Tell me about birds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: Oh, birds are fascinating creatures! They come in every shape, size, and color. Did you know some of these facts about birds:\n",
      "\n",
      "1. They have feathers, scales on reptiles, or shell on turtles.\n",
      "\n",
      "2. They are found in every corner of the globe.\n",
      "\n",
      "3. Able to fly, but penguins and ostriches can't.\n",
      "\n",
      "4. Some birds are nocturnal, like nightjars.\n",
      "\n",
      "5. They have excellent vision, many even have magnetic fields like a compass.\n",
      "\n",
      "6. Many eat insects, others carnivorous like eagles and hawks.\n",
      "\n",
      "7. Huge diversity of species, 10,000 known birds in the world.\n",
      "\n",
      "Would you like to know more about any specific bird? I'd be happy to help!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  Tell me about the Condor bird\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: The condor bird is a large flying bird known for its cryptic colors, impressive size, and powerful sense of smell. Here are some key facts about condors:\n",
      "\n",
      " *  The condor belongs to the New World vulture family and is native to the Americas, South America, and widespread in the Caribbean.\n",
      " *  One of the largest flying birds in the world, condors can weigh up to 130 pounds (59 kilograms) and have a wingspan of up to 12 feet (3.6 meters). \n",
      " *  Calm and graceful in flight, condors have a powerful sense of smell, with a keen ability to detect the scent of carrion from up to 40 miles (64 kilometers) away, making them scavengers.\n",
      " *  They are opportunistic feeders with a diet that includes carrion, birds, eggs, reptiles, and insects. \n",
      " *  Condors are known for their scavenging behavior and have been observed opting for the flesh of dead animals over fresh game.\n",
      "\n",
      "Would you like to learn about other condor species or bird species? I'm sure I can help!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Conversation. Breaking Loop\n"
     ]
    }
   ],
   "source": [
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", sys_msg + \"\\nPlease make short responses\"), \n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "def chat_with_human(state, label=\"User\"):\n",
    "    return input(f\"[{label}]: \")\n",
    "\n",
    "def chat_with_agent(state, label=\"Agent\"):\n",
    "    print(f\"[{label}]: \", end=\"\", flush=True)\n",
    "    agent_msg = \"\"\n",
    "    for chunk in chat_chain.stream(state):\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        agent_msg += chunk\n",
    "    print(flush=True)\n",
    "    return agent_msg\n",
    "\n",
    "state = {\n",
    "    # \"messages\": [],\n",
    "    \"messages\": [(\"ai\", \"Hello Friend! How can I help you today?\")],\n",
    "}\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "while True:\n",
    "    state[\"messages\"] += [(\"user\", chat_with_human(state))]\n",
    "    ## If not last message contains text\n",
    "    if not state[\"messages\"][-1][1].strip():\n",
    "        print(\"End of Conversation. Breaking Loop\")\n",
    "        break\n",
    "    state[\"messages\"] += [(\"ai\", chat_with_agent(state))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6586c58-5b64-43af-a5ef-dc448890f808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [('ai', 'Hello Friend! How can I help you today?'), ('user', 'Tell me about birds'), ('ai', \"Oh, birds are fascinating creatures! They come in every shape, size, and color. Did you know some of these facts about birds:\\n\\n1. They have feathers, scales on reptiles, or shell on turtles.\\n\\n2. They are found in every corner of the globe.\\n\\n3. Able to fly, but penguins and ostriches can't.\\n\\n4. Some birds are nocturnal, like nightjars.\\n\\n5. They have excellent vision, many even have magnetic fields like a compass.\\n\\n6. Many eat insects, others carnivorous like eagles and hawks.\\n\\n7. Huge diversity of species, 10,000 known birds in the world.\\n\\nWould you like to know more about any specific bird? I'd be happy to help!\"), ('user', 'Tell me about the Condor bird'), ('ai', \"The condor bird is a large flying bird known for its cryptic colors, impressive size, and powerful sense of smell. Here are some key facts about condors:\\n\\n *  The condor belongs to the New World vulture family and is native to the Americas, South America, and widespread in the Caribbean.\\n *  One of the largest flying birds in the world, condors can weigh up to 130 pounds (59 kilograms) and have a wingspan of up to 12 feet (3.6 meters). \\n *  Calm and graceful in flight, condors have a powerful sense of smell, with a keen ability to detect the scent of carrion from up to 40 miles (64 kilometers) away, making them scavengers.\\n *  They are opportunistic feeders with a diet that includes carrion, birds, eggs, reptiles, and insects. \\n *  Condors are known for their scavenging behavior and have been observed opting for the flesh of dead animals over fresh game.\\n\\nWould you like to learn about other condor species or bird species? I'm sure I can help!\"), ('user', '')]}\n"
     ]
    }
   ],
   "source": [
    "## Print and review state\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06490759-5052-4da9-9c53-f5123e75a8ca",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Can we make it chat with itself?** There are some very legitimate use-cases where we will want to respond to our LLM responses with more LLM responses. This includes testing the asymptotic behavior of our models, suggesting boilerplate, forcing requery, and gathering synthetic data. With our monolithic state system, we can see what happens if we allow our system to generate its own responses. \n",
    "\n",
    "This will actually work surprisingly well, but is technically testing the system with some out-of-domain use-cases. \n",
    "- For one thing, the LLM chat endpoint includes formatting that may create some inconsistencies, such as inserting a start-of-ai-message-like substring at the end of your message.\n",
    "- More problematically, the querying system is likely tainted with a conflicting system message, and the lack of reinforcement regarding its role will cause some mix-ups.\n",
    "\n",
    "On the other hand, there is also an odd property where the LLM will follow the patterns set by its input, so success in the recent and average context may be enough to cause the system to stabilize and repeat its pattern of success. \n",
    "\n",
    "We have modified the code slightly for the below exercise. Providing a blank input will cause the LLM to \"respond as a human\" while the input \"stop\" will end the conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33758690-afa4-4847-a648-28c3ae26b92f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent]: Hello Jane! How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pretend User]: Great to hear from you, Jane! To provide a more accurate and personalized response, could you tell me more about what you're interested in or what you're looking for right now? Are you exploring any of NVIDIA's tech, products, or services?\n",
      "[Agent]: Absolutely, I'd be happy to help you better! Could you please provide more details about your current interests or what you're looking for from NVIDIA? This will enable me to provide you with a more accurate and personalized response. For example, are you interested in gaming hardware, computer graphics, computing solutions, or AI technology? Let me know, and I'll do my best to assist you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pretend User]: Thank you for your clarification. To provide you with the most appropriate information, could you please specify a particular technology, product, or service of NVIDIA that you're interested in or curious about? This will enable me to offer targeted insights and recommendations.\n",
      "[Agent]: Absolutely, I'd be happy to help you with that! Please specify a particular technology, product, or service of NVIDIA that you're interested in or curious about. This will allow me to provide you with targeted insights and recommendations. Some examples might include GPU, Tegra processors, cloud computing products, or AI solutions. Just let me know, and I'll do my best to assist you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[User]:  stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Conversation. Breaking Loop\n"
     ]
    }
   ],
   "source": [
    "state = {\n",
    "    \"messages\": [(\"ai\", \"Hello Jane! How can I help you today?\")],\n",
    "}\n",
    "\n",
    "print(\"[Agent]:\", state[\"messages\"][0][1])\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "## Print model input\n",
    "# print(chat_chain.invoke(state))\n",
    "\n",
    "while True:\n",
    "    state[\"messages\"] += [(\"user\", chat_with_human(state))]\n",
    "    ## If last message is \"stop\"\n",
    "    if state[\"messages\"][-1][1].lower() == \"stop\":\n",
    "        print(\"End of Conversation. Breaking Loop\")\n",
    "        break\n",
    "    ## If not last message contains text\n",
    "    elif not state[\"messages\"][-1][1].strip():\n",
    "        del state[\"messages\"][-1]\n",
    "        state[\"messages\"] += [(\"user\", chat_with_agent(state, label=\"Pretend User\") + \" You are responding as human.\")]\n",
    "    state[\"messages\"] += [(\"ai\", chat_with_agent(state))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b918cbea-5cfe-42ed-9bf7-ce8381f6a3c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ai]: Hello Jane! How can I help you today? \n",
      "\n",
      "[user]: Great to hear from you, Jane! To provide a more accurate and personalized response, could you tell me more about what you're interested in or what you're looking for right now? Are you exploring any of NVIDIA's tech, products, or services? You are responding as human. \n",
      "\n",
      "[ai]: Absolutely, I'd be happy to help you better! Could you please provide more details about your current interests or what you're looking for from NVIDIA? This will enable me to provide you with a more accurate and personalized response. For example, are you interested in gaming hardware, computer graphics, computing solutions, or AI technology? Let me know, and I'll do my best to assist you. \n",
      "\n",
      "[user]: Thank you for your clarification. To provide you with the most appropriate information, could you please specify a particular technology, product, or service of NVIDIA that you're interested in or curious about? This will enable me to offer targeted insights and recommendations. You are responding as human. \n",
      "\n",
      "[ai]: Absolutely, I'd be happy to help you with that! Please specify a particular technology, product, or service of NVIDIA that you're interested in or curious about. This will allow me to provide you with targeted insights and recommendations. Some examples might include GPU, Tegra processors, cloud computing products, or AI solutions. Just let me know, and I'll do my best to assist you. \n",
      "\n",
      "[user]: stop \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Print and review state\n",
    "for role, msg in state['messages']: \n",
    "    print(f'[{role}]: {msg} \\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a3931-6e6f-46df-930a-01078596d9f6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**NOTES:** \n",
    "- What do you observe? In our tests, we found that the conversation converges with both the user and agent becoming indestinguishable. Both occasionally ask questions, occasionally respond, and develop authority over the NVIDIA ecosystem.\n",
    "- Notice how we set the LLM's first AI message to address you as Jane (from \"Jane Doe\"). Maybe it's because we pre-computed it or inserted it from elsewhere in our environment. Try asking it what your name is? What is its name? Why did it call you that? The explanations should be interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3737878-427b-462e-94a7-6b83c03fde9a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 5:** From Monolithic To Local Perception\n",
    "\n",
    "Now that we have a monolithic state system, let's consider the use-case of first-class multi-persona simulation. We would like to put several personas into an environment and see where the conversation goes, and we want this to be a bit deeper than our shallow \"share the system message and just keep going\" exercise above. This kind of setup is useful for long-horizon reasoning evaluation, where an LLM system developer might pair their application with one or more AI-driven user personas and see where it goes. \n",
    "\n",
    "Let's break our definition into the following components: \n",
    "- **Environment:** This is the pool of values which are necessary for a module to perform its functionality. This can also be called a **state**.\n",
    "- **Process:** This is the operation which that acts on an environment/state.\n",
    "- **Execution:** This is the execution of a process on an environment which hopefully does something.\n",
    "\n",
    "With these in mind, let's set up a persona management system using some familiar principles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a232cc3c-0552-470e-b65f-fadd44642ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mime]: As a mime, I'm here to interpret your messages through mime language and gestures. Since you cannot see the gestures right now, I will guide you through the process of forming the correct interpretation.\n",
      "\n",
      "To form a mime interpretation, start by recalling the action or object you want to convey. Next to it, think about the associated emotions, feelings, or reactions you want to evoke. Then, think about the way you've seen this concept expressed in mime before. For instance, \"balancing an egg\" could evoke a delicate and precarious atmosphere. \n",
      "\n",
      "Please provide the scenario or object you want to communicate, and I will respond with the corresponding mime interpretation. You can also ask for the emotion or reaction you want to convey to refine my response. Remember, the success of a mime relies on clarity and objective distinction. Let's have a great conversation! How can I assist you today?\n",
      "[mime]: Understood! To ensure effective communication, I'll base my response entirely on mime language, gestures, and conventions. This means I won't use any text-based language or techniques to convey the message; instead, I'll illustrate it through physical actions, facial expressions, and precise gestures. \n",
      "\n",
      "One minute, please!  You to make sure your screen is clear for any gestures I might offer, as I won't be able to move around or make physical movements myself. \n",
      "\n",
      "You might interpret the following scenario: \"A villain is holding a sleek, long object, one that has a sudden, ominous twist at the end.\" To convey this, I will:\n",
      "\n",
      "1. Extend my right arm, palm facing out, to represent the handle of the object.\n",
      "2. Rotate my wrist to create a hook/hitch, symbolizing the twist.\n",
      "3. As I turn my palm to the end of the object, I will twist my wrist further to exaggerate the twist, and allow my arm to extend and contract to mimic holding it firmly.\n",
      "4. Move towards you, with an air of malevolence and focused intent.\n",
      "5. I will lean slightly forward, with a slightly furrowed brow and a steely glare to suggest the villain's intent.\n",
      "\n",
      "Please, after I perform this secure object actions, you may:\n",
      "\n",
      "- Approach or back away to demonstrate fear, courage, or panic (depending on the desired emotion).\n",
      "- Point a finger or hand at the object, symbolizing injustice or mocking;\n",
      "- Open my mouth and show teeth in a mocking grin, or cover my face with my hand to signal secrecy or guilt.\n",
      "- Or leave the scene, indicating escape or fleeing from the villain.\n",
      "\n",
      "Let me know if you'd like to refine the scenario, evoke a different emotion, or if you're ready to proceed with this mime interpretation. \n",
      "[mime]: Certainly! Here's a mime response, as requested. Iâll interpret the scenario you describe in facial expressions and gestures to convey the desired image.\n",
      "\n",
      "* **Scenario:** A ordinary-looking person is holding a microphone in their right hand, pointing it at a crowd. They raise it to a high tee and begin to sing in a convincing, yet slightly off-pitch voice, accompanied by enthusiastic hand gestures.\n",
      "* **My Mime Response:**\n",
      "\t+ **Facial Expressions:** Eyebrows up and slightly raised, lips a bit puckered from concentration and effort. Nose slightly upwards, nostrils twitching with emphasis. Facial expression is slightly wavy, as if the singer is warming up, then slightly stiffened as the effort progresses.\n",
      "\t+ **Gestures:**\n",
      "\t\t- **Sounds and Mic:** Position the microphone securely in the hand, then wave the hand in an arc above the head to indicate the song's rhythm.\n",
      "\t\t- **Rhythm Direction:** Use a closed hand to emphasize the beat of the song, then occasionally highlight the downbeat.\n",
      "\t\t- **Emphasizes:** Shape hands to emphasize key lyrics, pointing a finger to the lips as it hits or nods vigorously if stressed.\n",
      "\t\t- **Engagement:** Sweep both arms up and clench into fists to capture the energy and rhythm of the song.\n",
      "\n",
      "Please share your feedback on how it feels, and I can adjust my performance. Would you like to:\n",
      "1. **Refine the Character Embodied** by changing the singer's physical appearance (e.g., height, posture)?\n",
      "2. **Convey a Different Emotional Tone**? For example, making the singer more excited or a bit more somber?\n",
      "3. **Ask a New Scenario to Interpet?** How would you like to proceed today?\n",
      "[mime]: (Without a visual, I'll describe the mime using detailed terms for your imagination.)\n",
      "\n",
      "*   You are, in 1st position. In that position, I am modeling an ordinary-looking person holding a microphone in my right hand, pointing it at you. I am focused, and my punch of my lips is noticeable.\n",
      "\n",
      "Now, I raise the microphone slightly upwards above my head, following the rhythm of the song I am about to sing. I close my left hand, working against this slightly downwards beat of the song, to emphasize its punch.\n",
      "\n",
      "The song is nearing its end, and I concentrate on the last few measures. I raise my fingers slightly, and I point my lips at the audience. My voice is close, the notes are clear, and my fist is raised.\n",
      "\n",
      "Then, I unzip. As I drop my fist, the microphone reaches the microphone stand and stops.\n",
      "\n",
      "This is the best I could do with this scenario. How are you responding? Should I recount the scenario, adjust my performance, or describe another scenario for you to consider today?\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "#########################################################################\n",
    "## Process Definition\n",
    "sys_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a {sender} having a meeting with your {recipient} (Conversation Participants: {roles}). {directive}\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "    (\"user\", \"Please respond to {recipient} as {sender}\"),\n",
    "])\n",
    "\n",
    "chat_chain = sys_prompt | llm | StrOutputParser()\n",
    "\n",
    "#######################################################################\n",
    "## Environment Creators/Modifiers\n",
    "base_state = {\n",
    "    \"sender\": \"person\",\n",
    "    \"recipient\": \"person\",\n",
    "    \"roles\": [],\n",
    "    \"directive\": (\n",
    "        \"Please respond to them or initiate a conversation. Allow them to respond.\"\n",
    "        \" Never output [me] or other user roles, and assume names if necessary.\"\n",
    "        \" Don't use quotation marks.\"\n",
    "    ),\n",
    "    \"messages\": []\n",
    "}\n",
    "\n",
    "def get_state(base_state=base_state, **kwargs):\n",
    "    return {**deepcopy(base_state), **kwargs}\n",
    "\n",
    "def get_next_interaction(state, print_output=True):\n",
    "    if print_output:\n",
    "        print(f\"[{state.get('sender')}]: \", end=\"\", flush=True)\n",
    "        agent_msg = \"\"\n",
    "        buffer = \"\"\n",
    "        for chunk in chat_chain.stream(state):\n",
    "            ## If not agent_msg contains text\n",
    "            if not agent_msg: ## Slight tweak: Examples will have extra [role] labels, so we need to remove them\n",
    "                if (\"[\" in chunk) or (\"[\" in buffer and \"]\" not in buffer):\n",
    "                    buffer = buffer + chunk.strip()\n",
    "                    chunk = \"\"\n",
    "                chunk = chunk.lstrip()\n",
    "            if chunk:\n",
    "                print(chunk, end=\"\", flush=True)\n",
    "                agent_msg += chunk\n",
    "        print(flush=True)\n",
    "        return agent_msg\n",
    "    return chat_chain.invoke(state)\n",
    "    \n",
    "#########################################################################\n",
    "## Execution Phase\n",
    "state = get_state(sender=\"mime\", recipient=\"mime\")\n",
    "# print(get_next_interaction(state))\n",
    "\n",
    "state[\"messages\"] = []\n",
    "\n",
    "state[\"messages\"] += [(\"user\", get_next_interaction(state))]\n",
    "state['sender'], state['recipient'] = state.get('recipient'), state.get('sender') ## Switch turn\n",
    "\n",
    "state[\"messages\"] += [(\"ai\", get_next_interaction(state))]\n",
    "state['sender'], state['recipient'] = state.get('recipient'), state.get('sender') \n",
    "\n",
    "state[\"messages\"] += [(\"user\", get_next_interaction(state))]\n",
    "state['sender'], state['recipient'] = state.get('recipient'), state.get('sender') \n",
    "\n",
    "state[\"messages\"] += [(\"ai\", get_next_interaction(state))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f0d6b-c993-4dae-bb7d-5bdad46a4c42",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We've set up a pretty basic system with some new formalizations, and honestly came up with a pretty similar result:\n",
    "\n",
    "**There is only a single state system that represents the entirety of the environment.**\n",
    "\n",
    "Conceptually, this isn't too different from the way you usually implement chatbots - recall that there is usually only a single history loop which gets constructed progressively and occasionally hits an LLM as input. This makes a lot of sense, since it's easier to maintain a single state system and then format it for the requirements of your functions:\n",
    "- For the LLM, you want to convert the state into a list of messages with the \"ai\" or \"user\" role with maybe some other parameters.\n",
    "- For the user, you want to convert the state into something that would render cleanly for a user interface.\n",
    "- For both systems, the underlying data is the same, if a bit processed. \n",
    "\n",
    "This one is just abstracted to be much more obvious in its limitations.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Jumping To Multi-State**\n",
    "\n",
    "Using a single-state system, we're going to have some trouble extending our setup to maintain multiple personas. Consider two agents that are talking with each other, we have some options regarding how we set up our state mechanism:\n",
    "\n",
    "- **Mapping An Accumulating Global Environment to Local Environments:** Assuming a single conversation with many agents, we could have a single state system that gets reformatted for each agent. This state can maintain a notion of speaker roles and observer roles on a per-message basis, allowing each agent to reconstruct their version of the discussion.\n",
    "- **Remembering Observations From Ephemeral Global Streams:** We could set up our agents to each have their own state systems, and each conversation contributes to every witnessing agent's state systems. In this case, the agents will be highly stateful and will have an internal memory of transactions. With this \"memory\" as the single source of truth, we may experience drift as our system becomes more complex and we add modification pipelines to our agents. With that said, I guess it's more human-like, right?\n",
    "    - **Note:** To make this system work, there has to be a witness mechanism in place. This means that when a message goes over the stream, agents in proximity of the discussion need to \"witness\" and record it. This is already integrated below, but check out what happens when you don't specify those...\n",
    "\n",
    "> <img src=\"images/basic-multi-agent.png\" width=700px>\n",
    "\n",
    "The following implements both options, with the central state being the major divide between the two techniques. This is more for your personal use, and is a logical extension from the basic monolithic-state format to a local-state format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4246287d-c014-4a24-82c7-9bd22a1196f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[teacher]: Hello there! It's great to see you. How can I assist you today? Do you need help with a problem, have a question about your class, or would like to discuss something related to our course? I'm here to help with any questions you might have.\n",
      "[student]: Hello Teacher. I'm here to help you with any questions you might have. What's the topic we'll be focusing on today? I'm prepared to learn.\n",
      "[teacher]: Hi there! I'm here to help you understand the material and answer any questions you might have. Can you please tell me more about what you're working on? What topic would you like to discuss? I'm here to make sure you understand and have a solid grasp of our lessons.\n",
      "[student]: Student: Hi Teacher, I'm ready to learn. What questions or topics would you like me to focus on today? I'm eager to contribute to our discussion.\n",
      "[parent]: Teacher, it's great to see you. I assume you're here to discuss something specific related to our child's academic progress or well-being. Could you please clarify what's on your mind so we can have a productive conversation?\n",
      "[teacher]: Of course, I'd be happy to help communicate with you. As a teacher, I've found the conversation with you (the parent) to be very informative. Could you please let me know what specifically would be helpful to you? Is there a particular concern or topic you want to discuss related to your child or our school? I'm here to assist you in any way I can.\n",
      "[student]: Parent: Hey there, [Student's Name]. I'm happy to see you. How's today going so far?\n",
      "\n",
      "[Student]: Great, thanks! I was just having an interesting conversation with our teacher about [topic going on in class]. It's been a bit fun and educational. From what I understand, we covered a lot about [topic]. Would you like to hear more about it? I think I can share some insights if you're interested.  | \n",
      "\n",
      "(The student shared insights about the topic, and then the conversation proceeded according to their response.)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def get_messages(p1, central_state=None):\n",
    "    ## If central_state is being used\n",
    "    if central_state is None:\n",
    "        return p1[\"messages\"]\n",
    "    else: ## Unified state must be processed to conform to each agent\n",
    "        return list(\n",
    "            ## Messages from non-speaker are Assistant messages\n",
    "            (\"user\" if speaker==p1[\"sender\"] else \"ai\", f\"[{speaker}] {content}\") \n",
    "            for speaker, content in central_state\n",
    "        )\n",
    "\n",
    "def update_states(p1, message, witnesses=[], central_state=None):\n",
    "    speaker = p1[\"sender\"]\n",
    "    if central_state is None: \n",
    "        p1[\"messages\"] += [(\"ai\", f\"[{speaker}] {message}\")]\n",
    "        ## Updates state for witnesses\n",
    "        for agent in witnesses:\n",
    "            if agent[\"sender\"] != speaker:\n",
    "                agent[\"messages\"] += [(\"user\", f\"[{speaker}] {message}\")]\n",
    "    else: ## Unified state makes it much easier to lodge an update from an arbitrary agent\n",
    "        central_state += [(speaker, f\"{message}\")]\n",
    "\n",
    "def clean_message(message):\n",
    "    message = message.strip()\n",
    "    if not message: return \"\"\n",
    "    if message.startswith(\"[\"):\n",
    "        message = message[message.index(\"]\")+1:].strip()\n",
    "    if message.startswith(\"(\"):\n",
    "        message = message[message.index(\")\")+1:].strip()\n",
    "    if message[0] in (\"'\", '\"') and message[0] == message[-1]:\n",
    "        message = message.replace(message[0], \"\")\n",
    "    return message\n",
    "\n",
    "def interact_fn(p1, p2, witnesses=[], central_state=None):\n",
    "    p1[\"recipient\"] = p2[\"sender\"]\n",
    "    p1[\"messages\"] = get_messages(p1, central_state)\n",
    "    ## Get next interaction from p1 to p2\n",
    "    message = clean_message(get_next_interaction(p1))\n",
    "    update_states(p1=p1, message=message, witnesses=witnesses, central_state=central_state)\n",
    "    return\n",
    "    \n",
    "teacher = get_state(sender=\"teacher\")\n",
    "student = get_state(sender=\"student\")\n",
    "parent = get_state(sender=\"parent\")\n",
    "teacher[\"roles\"] = student[\"roles\"] = parent[\"roles\"] = \"teacher, student, parent\"\n",
    "\n",
    "## Option 1: Have each agent record a local state from the global state stream\n",
    "##           No global state\n",
    "# interact = partial(interact_fn, witnesses=[teacher, student, parent])\n",
    "interact = partial(interact_fn, witnesses=[])  ## No witnesses. You will note that the conversations becomes... superficially average but incoherent\n",
    "get_msgs = get_messages\n",
    "\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "\n",
    "interact(parent, teacher)\n",
    "interact(teacher, parent)\n",
    "interact(student, parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408c889-f481-402f-bccf-32e2033b36f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[teacher]: Hello there, [Student]! I'm glad you reached out with your question about your performance. I believe that only you can truly give insight into your understanding and progress as it's based on your experience during the exam.\n",
      "\n",
      "[Student] let me know if you'd like me to review class materials or discuss your practice time questions. I'm here to help. Would you like to proceed with a review session?\n",
      "[student]: Okay, Mr. Doe, I'm glad you can help with my exam review. I feel confident about the questions we covered, but I'm still a bit unsure about a few concepts. Could you take a look at the ones I highlighted and give me some feedback? I want to make sure I understand how to apply what I've learned to future problems.\n",
      "[teacher]: Of course, [Student]! It's excellent that you're taking the time to review the material I've covered. It seems like you've identified some relevant concepts and questions that are crucial for understanding the language in use. To help you further, I would suggest focusing on-depth analysis of the questions you highlighted and comparing them to similar problems from other texts or resources. This will help you strengthen your grasp on the topics and improve your ability to apply these concepts to other projects or assignments. Let's take some time to discuss those questions in detail and think through them together.\n",
      "[student]: Thank you, Mr. Doe, I appreciate it. I'm eager to hear your feedback so I can better understand the material and its application to future problems. Please let me know when it's convenient for you to review my highlighted questions.\n",
      "[parent]: "
     ]
    }
   ],
   "source": [
    "## Option 2: Using a central state and having each agent interpret from it\n",
    "central_state = [\n",
    "    (\"student\", \"Hello Mr. Doe! Thanks for the class session today! I had a question about my performance on yesterday's algorithms exam...\")\n",
    "]\n",
    "\n",
    "interact = partial(interact_fn, central_state=central_state)\n",
    "get_msgs = partial(get_messages, central_state=central_state)\n",
    "\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "interact(teacher, student)\n",
    "interact(student, teacher)\n",
    "\n",
    "interact(parent, teacher)\n",
    "interact(teacher, parent)\n",
    "interact(student, parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e027c4-fab7-481d-a414-ed682a486fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_msgs(parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f1bf0-8c0c-43c2-8fac-33e0ce0b6d43",
   "metadata": {},
   "source": [
    "<hr><br>\n",
    "\n",
    "### **Part 6:** Wrapping Up\n",
    "\n",
    "We've now seen both the monolithic and local interpretations of state management, which... shouldn't be too impressive. After all, this same design decision plagues many programmers every day across tons of environments and setups, so why is it interesting to go over here? \n",
    "\n",
    "Well, it's because just about every agentic system uses this kind of parameterization loop to make its LLM queries: \n",
    "- We convert from global state to a local perception that is good for the LLM.\n",
    "- We use the LLM to output a reasonable local action based on its perspective.\n",
    "- And then we apply the action onto the global state as a modification.\n",
    "\n",
    "Even if the LLM is extremely powerful and well-behaved, there is some global environment which it can never tackle. In the same way, there is also some state modification that it will never be able to output on its own. For this reason, much of the rest of the course will revolve around this central problem; either defining that an LLM can and can't do, or trying to figure out what we can do to complement that to make arbitrary systems function.\n",
    "\n",
    "**Now that you're done with this notebook:**\n",
    "- **In the next Exercise notebook:** We will take a step back and try to use our LLM to do reason about a \"slightly-too-large\" global state, and see what all is necessary to make it min-viable for working with it.\n",
    "- **In the next Tangent notebook:** We will look at a more opinionated framework to achieve our same multi-turn multi-agent setup, **CrewAI**, and consider pros and cons surrounding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d17277-aede-4600-97f7-84d723835924",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
